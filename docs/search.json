[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Aj",
    "section": "",
    "text": "Data Science Enthusiast"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rooftop Data",
    "section": "",
    "text": "Linear Regression Guide\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nA county like mine\n\n\n\n\n\n\n\npolitics\n\n\nvisualization\n\n\nstatistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nQuerying conspiracies with Reddit\n\n\n\n\n\n\n\nvisualization\n\n\nstreamlit\n\n\npython\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nRosetta Stone for Data Science\n\n\n\n\n\n\n\nstatistics\n\n\npython\n\n\nr\n\n\nnotation\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Visualization using Anime\n\n\n\n\n\n\n\nanime\n\n\nvisualization\n\n\nstreamlit\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nPandas vs Tidyverse\n\n\n\n\n\n\n\nstatistics\n\n\npython\n\n\nr\n\n\nnotation\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nState of American Christianity\n\n\n\n\n\n\n\nreligion\n\n\nr\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nThe Triangle of Power\n\n\n\n\n\n\n\nnotation\n\n\nlatex\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nThe P-value\n\n\n\n\n\n\n\nstatistics\n\n\nanime\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nA Quick Review\n\n\n\n\n\n\n\nstatistics\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\nAj Averett\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pvalue/index.html",
    "href": "posts/pvalue/index.html",
    "title": "The P-value",
    "section": "",
    "text": "“While the p-value can be a useful statistical measure, it is commonly misused and misinterpreted.” (Wasserstein and Lazar, 2016)\nNote: The contents of this article are not peer-reviewed and may contain errors.\n\nMeet Levi. He is a short person. He is around 5’3” (63 inches). While we can tell he is short, we may wonder how short he is. In order to know this we need something to compare his height to.\n\nAmerican Sample\nThe code below allows us to create a hypothetical (but reasonable) data set of American male heights. I will be creating a sheet of 200 random American men. Below are a few entries…\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport requests\nimport matplotlib.pyplot as plt\n\n###\n\ndef create_random_people_dataframe(n):\n  # Create a Pandas data frame with a single column for heights\n  df = pd.DataFrame({'height': np.random.normal(loc=69, scale=3, size=n)})\n   \n  # Round the heights to the nearest integer\n  df['height'] = df['height'].round(2)\n\n  # Add an 'id' column to the data frame\n  df['id'] = df.index + 1\n\n  # Send a request to the API and retrieve the names\n  response = requests.get(\n    \"https://randommer.io/api/Name\", \n    headers={\n      \"X-Api-Key\":api_key},\n    params={\n      \"nameType\": \"surname\",\n      \"quantity\": n}\n    )\n\n  # Parse the response from the API to extract the names\n  names = response.json()\n\n  # Add names to the data frame\n  df = df.assign(name=names)\n  \n  # Rearrange the columns\n  df = df.reindex(columns=[\"id\", \"name\", \"height\"])\n\n  # Return the data frame\n  return df\n\n# Generate a data frame with random people's heights\ndf = create_random_people_dataframe(200)\n\ndf.drop(columns=['id']).head()\n\n\n\n\n\n\n  \n    \n      \n      name\n      height\n    \n  \n  \n    \n      0\n      Gaines\n      66.38\n    \n    \n      1\n      Hoch\n      71.11\n    \n    \n      2\n      Futrell\n      67.41\n    \n    \n      3\n      Scanlan\n      69.01\n    \n    \n      4\n      Polanco\n      71.97\n    \n  \n\n\n\n\n\n\nQuantifying Extremes!\nA standard score, also known as a z-score, is a way of comparing a single data point to a group of data points. It helps us understand how high or low a number is relative to the group. The z-score measures how many standard deviations away a point is from the group average. (For a review, see here)\nThe average height and standard deviation for the generated American men is calculated below\n\nmean_height = df[\"height\"].mean().round()\nst_dev_height = df[\"height\"].std().round()\nn_height = df[\"height\"].count()\n\nprint(\"Average height is: \"+str(mean_height))\nprint(\"Standard deviation of height is: \"+str(st_dev_height))\nprint(\"Number of people in sample is: \"+str(n_height))\n\nAverage height is: 69.0\nStandard deviation of height is: 3.0\nNumber of people in sample is: 200\n\n\nThe formula for z-score is as so:\n\\[\nz = \\frac{x-\\mu}{\\sigma}\n\\]\nAs the z-score increases/decreases, the father away it gets from the mean. For example, Shaq is 85 inches tall. His z-score would be calculated as so:\n\nz_score_shaq = (85 - mean_height)/st_dev_height\n\nprint(\"Shaq's height is \"+str(z_score_shaq.round(2))+\" standard deviations from the mean\")\n\nShaq's height is 5.33 standard deviations from the mean\n\n\nThis also means that the smaller the z-score is, the closer it is to the mean. Take an average height person:\n\nz_score_avg = (69 - mean_height)/st_dev_height\n\nprint(z_score_avg)\n\n0.0\n\n\n\n\nLevi’s Height\nLevi is 5’3” (63 inches). That’s pretty short. Let’s see how short by calculating a z-score:\n\nz_score_levi = (63 - mean_height)/st_dev_height\n\nprint(z_score_levi)\n\n-2.0\n\n\nThis shows that Levi is a couple standard deviations lower than average. Pretty short!\n\n\nArea means probability\nThe graph below shows the normal distribution of American men with labeled heights for a standard deviation above and below average height (assuming average is 69 inches and standard deviation is 3 inches)\n\n\n\nIf you met a random American, the odds that they are taller than average is 50 percent. (the odds that they are shorter than average is 50 percent) We can see that illustrated below. If the area under the whole curve is equal to 1, that means that half the area is equal to .5.\n\nWe can also imagine Levi compared to the American male population. The red line marks where Levi’s height is and the blue fill is for any person as short as Levi or shorter. This means that the probability of meeting someone as short or shorter than Levi in the American population is 2.28 percent.\n\nShaq is unbelievably tall. Like you will probably not meet anyone as tall (or taller) than him. On the graph below, the red line on the right represents the height of Shaq. The filled in the blue area is so small, you cannot even see it. The number on top tells us that the odds of finding an American man as tall (or taller) than Shaq is 0.000004862 percent. (very unlikely!)\n\n\n\nA whole bunch of guys\nWhat we did above is calculate a z-score for a single piece of data given a normal distribution. The formula is seen below. The curve represents the possible heights that the individual could be.\n\\[\nz = \\frac{x-\\mu}{\\sigma}\n\\]\nWhat if instead of using a single data point, we use a group of data points to compare? The reason for doing a single data point would to see how uncommon a data point would be given a distribution- this means that the reason for doing a group of points would be to see how uncommon a group would be given a distribution.\nAssume we have a group of people from Attack on Titan.\n\n\\(\\bar{x}\\) would represent the average of the Attack on Titan group.\n\\(\\mu\\) would represent the mean of American men\n\\(\\sigma\\) would represent the standard deviation of American men\n\\(n\\) would represent the number of people in the Attack on Titan group\n\n\\[\nz = \\frac{\\bar{x}-\\mu}{\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)}\n\\]\nA cool mathematical trick has just been done. Looking at one person compared to a population can tell us the probability that he came from the population. More interestingly, looking at a group compared to the population can tell us the probability that the group came from the population.\nFor the comparison with the individual, we need\n\nFind the height of the individual.\nFind the distribution of heights from the population (the curve represents potential/possible heights that the person could be)\nUse the height to produce a z-score for the individual. (calculated with the formula above)\nWherever the z-score is, find the area of the tail of the distribution. The area shaded under the curve would represent the probability of finding someone that tall/short or taller/shorter.\n\nSimilarly, For the comparison with the group, we need\n\nFind the height average of the group.\nFind the distribution of heights from the population (the curve represents potential/possible height averages that the person could be)\nUse the height to produce a z-score (test-statistic) for the group. (calculated with the formula above)\nWherever the z-score is, find the area of the tail of the distribution. The area shaded under the curve would represent the probability of finding a group that tall/short or taller/shorter.\n\nBelow, we find the test statistic (z-score) of the Attack on Titan group.\n\ndf_aot = pd.DataFrame(\n      {'name': ['Eren', 'Armin', 'Levi','Connie','Marco'],\n       'height': [67, 64, 65, 62, 70]}\n)\n\nzscore_aot = (df_aot[\"height\"].mean() - mean_height)/(st_dev_height/np.sqrt(df_aot[\"height\"].count()))\n\nprint(\"The average height for the Attack on Titan group is \"+str(df_aot[\"height\"].mean()))\nprint(\"The test-statistic/z-score for the Attack on Titan group is \"+str(zscore_aot.round(2)))\n\nThe average height for the Attack on Titan group is 65.6\nThe test-statistic/z-score for the Attack on Titan group is -2.53\n\n\n\nimport scipy.stats\n#Code to find area under curve on left-side\npval = (scipy.stats.norm.sf(abs(-2.53)))*100\n\nprint(\"The probability of finding American men as short (or shorter) than S1 Attack on Titan characters (assuming American men are truly 69 inches tall with a standard deviation of 3 inches) would be \" + str(pval.round(2)) + \"%\")\n\nThe probability of finding American men as short (or shorter) than S1 Attack on Titan characters (assuming American men are truly 69 inches tall with a standard deviation of 3 inches) would be 0.57%\n\n\nNote: We just generated a distribution from theoretical possible sample means. This is possible due to the Central Limit Theorem- see here and here for some good explanations. Just for simple intuition, you are more likely to get a sample with a mean closer to the population mean than farther. (Also, notice how this assumes that the population standard deviation is known-it usually isn’t)\nOn the graph below, if the population mean is truly 69 inches, then finding samples with the means 66 and 72 would be equally likely.\n\n\n\nIntroducing the P-value\nIn null hypotheses significance testing, we set up two hypotheses, the null hypothesis, and the alternative hypothesis. The process to test a hypothesis is as so:\n\nAssume the null hypothesis is true\nCalculate what we would expect to see if the null hypothesis is true (output is a distribution of potential test statistics/sample means etc.)\nCalculate what we actually observe (output is your sample’s test statistic)\nUsing the distribution of potential test statistics, find the probability of getting your sample’s test statistic\nIf you find that the probability of you getting a test statistic is low enough according to the generated distribution of test statistics from your null hypothesis, (usually below 5 percent) reject the null hypothesis- otherwise fail to reject it.\n\nThat’s it. The p-value is the probability of getting a sample as extreme or more extreme than any other result assuming your null hypothesis. As shown above, the two things you need to calculate the p-value are\n\nthe observed sample value (test statistic)\nthe distribution of theoretical sample values (test statistics) that could occur\n\nGetting these two numbers depends on what type of test you are doing and what type of assumptions you are willing to make.\nThe cut-off\nNow how improbably is improbable? For example, if a coin is fair and you throw it 100 times, we expect to roughly get as many heads as tails- but at what point are we going to say, “Hey this coin doesn’t seem fair?” If we get 40 heads and 60 tails, this is less likely than 50 heads and 50 tails, but we can reasonably see this happening just by chance. However, if we get 75 heads and 25 tails- this would be reallllly unlikely assuming the coin is fair.\nBy tradition, researchers have used the arbitrary “5 percent” cutoff point to determine what is “significant” or not. If the sample’s test statistic is so extreme that we would only expect the given sample to be that (or more) extreme less than 5 percent of the time, we deem it significant. `\n\nExpected Data/The Distribution and Observed Data/The Test statistic\nThis article will not deeply go into the distributions that you may see how extreme your test statistic is, but understanding the general nature will be useful for grasping how the p-value works.\nZ-distribution\nThe z-distribution, also called the normal distribution, assumes that you know the standard deviation of your population. When your sample increases, your sample’s standard deviation will approach the population standard deviation. If this is the case, sufficiently large samples may assume the population standard deviation is ‘known’.\nt-distribution\nAssuming the population standard deviation is not known, we may need to use the t-distribution.\nThe z-distribution has a set spread because we know the spread of the population. If we are unsure of the spread of the population, me may have to accept the possibility that seemingly rare events may just be results of a large spread. Now as our sample increases, we can be more sure of how the spread looks, but otherwise, we just make the distribution of test statistics wider (and therefore less sensitive to extreme findings).\nWhat specifically determines the spread of the t-distribution is the degrees of freedom. (calculated using the sample) Since larger sample sizes approximate the spread of the data better than small ones, the distribution becomes narrower as a result of these large sample sizes.\nF-distribution\nTest statistics and distributions of test statistics come in all varieties. The F-statistic is usually used to measure if groups of variables have some relation.\n\nAnalysis of Variance (ANOVA) uses an F-test to measure if group means are all equal. Assuming that they are all equal, a super abnormal F-statistic would mean that either they aren’t all equal or they are equal and your sample observations were just really unlikely\nLinear Regression may use an F-test to measure if the model or any of its variables is related to the outcome. Assuming no relationship with the model or any of its variables, a super abnormal F-statistic would mean that either the model actually explains some of the model outcome, or its doesn’t and the sample observations were just really unlikely.\n\nSimilar to the t-distribution, the F-distribution will change given the degrees of freedom of the sample.\nChi-Squared (χ2) Distribution\nThis distribution is similar to F-tests since it measures relationships between groups, but it does so with counts/frequencies of categorical variables rather than values or any other measurements from quantitative variables.\nLike the previous distributions, the chi-squared distribution will change given the degrees of freedom of the sample\nParametric and Non-Parametric tests\nYou can calculate a p-value from anything. All you need is the distribution of what you would find theoretically given your null hypothesis, and what you have actually found. Most of the tests above are parametric tests (all but the chi squared) and therefore have various assumptions- though there are more relaxed tests that may be appropriate for various situations. Some of these are as following:\n\nWilcoxon Signed Rank test (One sample/Paired Sample t-test but non-parametric)\nWilcoxon Rank Sum/Mann-Whitney U test (Two Sample t-test but non-parametric)\nKruskal-Wallis test (ANOVA but non-parametric)\n\nThe ultimate non-parametric test is the permutation test which makes a custom distribution of theoretical test statistics. This is only to show that the only thing one needs for a p-value is a theoretical sampling distribution of test statistics (under the null hypothesis) and the observed test statistic.\n\n\n\nWhat the p-value is NOT\nThe p-value is NOT the likelihood that a result is due to chance\n“When \\(p\\) is calculated, it is already assumed that \\(H_0\\) is true, so the probability that sampling error is the only explanation is already taken to be [100 percent]. It is thus illogical to view \\(p\\) as measuring the likelihood of sampling error. thus, \\(p\\) does not apply to a particular result as the probability that sampling error was the sole causal agent. there is no such thing as a statistical technique that determines the probability that various causal factors, including sampling error, acted on a particular result. instead, inference about causation is a rational exercise that considers results within the context of design, measurement, and analysis. Besides, virtually all sample results are affected by error of some type, including measurement error.”\n(Kline 2013)\nThe p-value is NOT the likelihood that a result is due to chance under the null hypothesis.\n“That this is not the case is seen immediately from the P value’s definition, the probability of the observed data, plus more extreme data, under the null hypothesis. The result with the P value of exactly .05 (or any other value) is the most probable of all the other possible results included in the”tail area” that defines the P value. The probability of any individual result is actually quite small, and Fisher said he threw in the rest of the tail area “as an approximation.”\n(Goodman, 2014)\nThe p-value is NOT the likelihood that a Type 1 error has occurred\n“[T]he p-value represents the probability of making a Type I error if the null hypothesis is perfectly true…\nThe p-value does not give you the probability that you have made a Type I error in reality. The phrase “if the null hypothesis were perfectly true” is the key to avoiding this misinterpretation. If H0 is not an absolutely perfect statistical model for reality (perhaps because of tiny differences between group means in the population, or just because the null is completely wrong), then the probability expressed in a p-value may not be accurate (Kirk, 1996). It is still a useful statistic, however, especially because it can help us make rational decisions about how well the null hypothesis matches the data in a study (Wasserstein & Lazar, 2016; Winch & Campbell, 1969).\n(Warne, 2017)\nThe p-value is NOT the likelihood that \\(H_0\\) is true\n“The \\(p\\) value does not say anything about the probability that the null hypothesis is true because the p-value is calculated under the scenario that the null hypothesis is perfectly true (Fidler, 2010). As a result, \\(p\\) cannot tell us whether the null hypothesis is true because we had to assume it was true in the first place to calculate \\(p\\) (Haller & Krauss, 2002). Likewise, a p-value cannot tell us whether the alternative hypothesis is true because \\(p\\) is based on the assumption that the null hypothesis is perfectly true.”\n(Warne, 2017)\n\\(1 -\\) p-value is NOT the likelihood that the result will be replicated\n“p says very little about the replicability or stability of results (Schmidt, 1996). Indeed, p-values fluctuate wildly across studies (Cumming, 2008). It is true, though, that (generally speaking) low p-values indicate that the null hypothesis may possibly be easier to reject in the future (Cumming & Maillardet, 2006). However, this assumes that the replications are conducted under precisely the same conditions as the original study, with every possible relevant (and irrelevant) factor perfectly replicated. In real life, a low p-value does not necessarily mean that you will get similar results if you conducted the same study again. In fact, attempts to replicate studies with low p-values often do not succeed (Open Science Collaboration, 2015). If the sample size is small or if the sample is not representative of the population in some way (as occurs frequently with nonrandom samples), the results may be unstable and not replicable, even if p is very low and the null hypothesis is strongly rejected. The best way to determine whether the results of a study will replicate is to conduct a replication.”\n(Warne 2017)\nThe p-value is NOT an indicator of how important the findings are\nThe p-value says nothing about the importance of findings (Kieffer, Reese, & Thompson, 2001). Remember that importance is an issue of practical significance – not statistical significance. There is nothing magical about an α value less than .05. Decisions about policy, psychological interventions, and other practical implications of research should be based on more than just a p-value. Such important decisions should not be based on a statistic that is vulnerable to changes in sample size and study conditions (Wasserstein & Lazar, 2016).\n(Warne, 2017)\nThe p-value is NOT an indicator about the size of an effect\n“p-values indicate the size of an effect (e.g., the difference between means or the magnitude of the relationship between variables). For example, a researcher finding that p is greater than α (indicating that the null hypothesis should be retained) may decide that the results are”insignificant” or irrelevant. This confuses p-values with effect sizes. Effect sizes quantify the strength of mean group differences or variable relationships; p-values do not (Wasserstein & Lazar, 2016). If the sample size is small or the study has low statistical power, then there could be a large effect size that is worth discussing, even if p is too high to provide evidence to reject the null hypothesis… Understanding the strength of relationships or the magnitude of group differences is important – and that is why it is essential to calculate and report an effect size to accompany a null hypothesis statistical significance test (American Psychological Association, 2010; Wasserstein & Lazar, 2016).”\n(Warne, 2017)\n\nSee more here and here\nThe misinterpretation of p-values and its consequences have been a disaster on academia"
  },
  {
    "objectID": "posts/pvalue/index.html#introducing-the-p-value",
    "href": "posts/pvalue/index.html#introducing-the-p-value",
    "title": "The P-value",
    "section": "Introducing the P-value",
    "text": "Introducing the P-value\nIn null hypotheses significance testing, we set up two hypotheses, the null hypothesis, and the alternative hypothesis. The process to test a hypothesis is as so:\n\nAssume the null hypothesis is true\nCalculate what we would expect to see if the null hypothesis is true (output is a distribution of potential test statistics/sample means etc.)\nCalculate what we actually observe (output is your sample’s test statistic)\nUsing the distribution of potential test statistics, find the probability of getting your sample’s test statistic\nIf you find that the probability of you getting a test statistic is low enough according to the generated distribution of test statistics from your null hypothesis, (usually below 5 percent) reject the null hypothesis- otherwise fail to reject it.\n\nThat’s it. The p-value is the probability of getting a sample as extreme or more extreme than any other result assuming your null hypothesis. As shown above, the two things you need to calculate the p-value are\n\nthe observed sample value (test statistic)\nthe distribution of theoretical sample values (test statistics) that could occur\n\nGetting these two numbers depends on what type of test you are doing and what type of assumptions you are willing to make.\nThe cut-off\nNow how improbably is improbable? For example, if a coin is fair and you throw it 100 times, we expect to roughly get as many heads as tails- but at what point are we going to say, “Hey this coin doesn’t seem fair?” If we get 40 heads and 60 tails, this is less likely than 50 heads and 50 tails, but we can reasonably see this happening just by chance. However, if we get 75 heads and 25 tails- this would be reallllly unlikely assuming the coin is fair.\nBy tradition, researchers have used the arbitrary “5 percent” cutoff point to determine what is “significant” or not. If the sample’s test statistic is so extreme that we would only expect the given sample to be that (or more) extreme less than 5 percent of the time, we deem it significant. `\n\nExpected Data/The Distribution and Observed Data/The Test statistic\nThis article will not deeply go into the distributions that you may see how extreme your test statistic is, but understanding the general nature will be useful for grasping how the p-value works.\nZ-distribution\nThe z-distribution, also called the normal distribution, assumes that you know the standard deviation of your population. When your sample increases, your sample’s standard deviation will approach the population standard deviation. If this is the case, sufficiently large samples may assume the population standard deviation is ‘known’.\nt-distribution\nAssuming the population standard deviation is not known, we may need to use the t-distribution.\nThe z-distribution has a set spread because we know the spread of the population. If we are unsure of the spread of the population, me may have to accept the possibility that seemingly rare events may just be results of a large spread. Now as our sample increases, we can be more sure of how the spread looks, but otherwise, we just make the distribution of test statistics wider (and therefore less sensitive to extreme findings).\nWhat specifically determines the spread of the t-distribution is the degrees of freedom. (calculated using the sample) Since larger sample sizes approximate the spread of the data better than small ones, the distribution becomes narrower as a result of these large sample sizes.\nF-distribution\nTest statistics and distributions of test statistics come in all varieties. The F-statistic is usually used to measure if groups of variables have some relation.\n\nAnalysis of Variance (ANOVA) uses an F-test to measure if group means are all equal. Assuming that they are all equal, a super abnormal F-statistic would mean that either they aren’t all equal or they are equal and your sample observations were just really unlikely\nLinear Regression may use an F-test to measure if the model or any of its variables is related to the outcome. Assuming no relationship with the model or any of its variables, a super abnormal F-statistic would mean that either the model actually explains some of the model outcome, or its doesn’t and the sample observations were just really unlikely.\n\nSimilar to the t-distribution, the F-distribution will change given the degrees of freedom of the sample.\nChi-Squared (χ2) Distribution\nThis distribution is similar to F-tests since it measures relationships between groups, but it does so with counts/frequencies of categorical variables rather than values or any other measurements from quantitative variables.\nLike the previous distributions, the chi-squared distribution will change given the degrees of freedom of the sample\nParametric and Non-Parametric tests\nYou can calculate a p-value from anything. All you need is the distribution of what you would find theoretically given your null hypothesis, and what you have actually found. Most of the tests above are parametric tests (all but the chi squared) and therefore have various assumptions- though there are more relaxed tests that may be appropriate for various situations. Some of these are as following:\n\nWilcoxon Signed Rank test (One sample/Paired Sample t-test but non-parametric)\nWilcoxon Rank Sum/Mann-Whitney U test (Two Sample t-test but non-parametric)\nKruskal-Wallis test (ANOVA but non-parametric)\n\nThe ultimate non-parametric test is the permutation test which makes a custom distribution of theoretical test statistics. This is only to show that the only thing one needs for a p-value is a theoretical sampling distribution of test statistics (under the null hypothesis) and the observed test statistic."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "A Quick Review",
    "section": "",
    "text": "Central Tendency\nThe mean and the median are two different ways of describing the “middle” of a group of numbers.\nMean (Average)\nThe mean (also known as average) is found by adding up all of the numbers in the group and then dividing that number by the total number of numbers in the group. For example, if a group of kids has the following test scores: 82, 85, 90, 91, and 92, the mean (or average) score would be calculated like this:\n\n# The code below is just printing out the class average\n\nclass_average <- (82 + 85 + 90 + 91 + 92) / 5\n\nprint(class_average)\n\n[1] 88\n\n\nThe class mean or class average is 88\nMedian\nThe median is the number that is in the middle of the group when the numbers are listed in order from least to greatest. In the same group of scores above, the numbers would be listed like this when they are in order: 82, 85, 90, 91, and 92. The number in the middle is 90.\nThe class mean is 90\nBasically, the mean is found by adding up all of the numbers and dividing by the total number of numbers, while the median is the number in the middle when the numbers are listed in order. Both the mean and the median can be used to describe the “middle” of a group of numbers, but they can sometimes give different results.\n\n\nNormal Distributions\nFor people that go to the gym, you may see wear and tear on some of the equipment. On the image below, a curve shape appears with the deterioration of the metal finish. The center of this curve represents the average weightlifting ability of the population that uses it. There would be fewer people who would lift significantly more or less weight than the average. This pattern is called a normal distribution.\n\n\n\n\n\nIf we were to graph height, most people tend to be around the average since there are natural limits to how tall or short people can be.\n\n\n\n\n\n\n\nStandard Deviation\n“A standard deviation is a measure of how dispersed the data is in relation to the mean. Low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out. A standard deviation close to zero indicates that data points are close to the mean, whereas a high or low standard deviation indicates data points are respectively above or below the mean.” [1]\n\n\\(\\mu\\) refers to the mean or average\n\\(\\sigma\\) refers to a standard deviation\n\n\n\n\n\n\nThe standard deviation, as explained above, measures the spread of the data. To see this with an example, take two cities, Kansas City, MO, and San Diego, CA. In 2019, both of these cities probably had an average temperature in the 60’s. The green line in the picture above represents the average temperature.\nthough the average temperatures were comparable, the spread is much bigger for one of these cities. [2][3]\n\nKansas City, MO- low/high: 22°F-91°F\nSan Diego, CA- low/high: 50°F-77°F\n\nWe can expect the standard deviation for temperature to be far greater in Kansas City than San Diego."
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html",
    "href": "posts/power_triangle/power_triangle.html",
    "title": "The Triangle of Power",
    "section": "",
    "text": "A post on Math Overflow about the relationship between powers, roots, and logs sparked the birth of a notation to harmonize these seemingly unrelated concepts. Grant Sanderson from 3Blue1Brown further popularized and named this notation as, “The Triangle of Power”"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#a-remains-constant",
    "href": "posts/power_triangle/power_triangle.html#a-remains-constant",
    "title": "The Triangle of Power",
    "section": "\\(a\\) remains constant",
    "text": "\\(a\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Exponents}\n\\\\\\\\\n&\n\\log_a(x\\times y) = \\log_a(x) + \\log_b(x)\n&\na^{x + y} = a^x \\times a ^y\n\\\\\\\\\n&\n\\log_a(\\frac{x}{y}) = \\log_a(x) - \\log_b(x)\n&\na^{x - y} = \\frac{a^x}{a^y}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#b-remains-constant",
    "href": "posts/power_triangle/power_triangle.html#b-remains-constant",
    "title": "The Triangle of Power",
    "section": "\\(b\\) remains constant",
    "text": "\\(b\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Exponents}\n&\\text{Properties of Roots}\n\\\\\\\\\n&   xy^b = x^b \\times y^b\n&   \\sqrt[b]{x\\times y} = \\sqrt[b]{x} \\times \\sqrt[b]{y}\n\\\\\\\\\n&   \\left(\\frac{x}{y}\\right)^b = \\frac{x^b}{y^b}\n&   \\sqrt[b]{\\frac{x}{y}} = \\frac{\\sqrt[b]{x}}{\\sqrt[b]{y}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#c-remains-constant",
    "href": "posts/power_triangle/power_triangle.html#c-remains-constant",
    "title": "The Triangle of Power",
    "section": "\\(c\\) remains constant",
    "text": "\\(c\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Roots}\n\\\\\\\\\n&   \\log_{x\\times y}(c) = \\left(\\left(\\log_xc\\right)^{-1} + \\left(\\log_yc\\right)^{-1}\\right)^{-1}\n&   \\sqrt[(x^{-1}+ y^{-1})^{-1}]{c} = \\sqrt[x]{c} \\times \\sqrt[y]{c}\n\\\\\\\\\n&   \\log_{\\frac{x}{y}}(c) = \\left(\\left(\\log_xc\\right)^{-1} - \\left(\\log_yc\\right)^{-1}\\right)^{-1}\n&   \\sqrt[(x^{-1}- y^{-1})^{-1}]{c} = \\frac{\\sqrt[x]{c}}{\\sqrt[y]{c}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#a-remains-constant-1",
    "href": "posts/power_triangle/power_triangle.html#a-remains-constant-1",
    "title": "The Triangle of Power",
    "section": "\\(a\\) remains constant",
    "text": "\\(a\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Exponents}\n\\\\\\\\\n&\n_{a}\n\\stackrel{\\phantom{b}}\n\\triangle _{xy}\n={}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{x} +{}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{y}\n&\n_{a}\n\\stackrel{x+y}\n\\triangle _{\\phantom{c}}\n={}\n_{a}\\stackrel{x}\\triangle _{\\phantom{c}} \\times{}\n_{a}\\stackrel{y}\\triangle _{\\phantom{c}}\n\\\\\\\\\n&_{a}\n\\stackrel{\\phantom{b}}\n\\triangle _{\\frac{x}{y}}\n={}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{x} -{}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{y}\n&\n_{a}\n\\stackrel{x-y}\n\\triangle _{\\phantom{c}}\n={}\n\\frac\n{_{a}\\stackrel{x}\\triangle _{\\phantom{c}}}\n{_{a}\\stackrel{y}\\triangle _{\\phantom{c}}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#b-remains-constant-1",
    "href": "posts/power_triangle/power_triangle.html#b-remains-constant-1",
    "title": "The Triangle of Power",
    "section": "\\(b\\) remains constant",
    "text": "\\(b\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Exponents}\n&\\text{Properties of Roots}\n\\\\\\\\\n&\n_{x\\times y}\n\\stackrel{b}\n\\triangle _{\\phantom{c}}\n={}\n_{x}\\stackrel{b}\\triangle _{} \\times{}\n_{y}\\stackrel{b}\\triangle _{}\n&\n_{\\phantom{a}}\n\\stackrel{b}\n\\triangle _{x\\times y}\n={}\n_{\\phantom{x}}\\stackrel{b}\\triangle _{x} \\times{}\n_{\\phantom{x}}\\stackrel{b}\\triangle _{y}\n\\\\\\\\\n&_{\\frac{x}{y}}\n\\stackrel{b}\n\\triangle _{\\phantom{c}}\n={}\n\\frac\n{_{x}\\stackrel{b}\\triangle _{\\phantom{c}} }\n{_{y}\\stackrel{b}\\triangle _{\\phantom{c}} }\n&\n_{\\phantom{a}}\n\\stackrel{b}\n\\triangle _{\\frac{x}{y}}\n=\n\\frac\n{_{\\phantom{a}}\\stackrel{b}\\triangle _{x}}\n{_{\\phantom{a}}\\stackrel{b}\\triangle _{y}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#c-remains-constant-1",
    "href": "posts/power_triangle/power_triangle.html#c-remains-constant-1",
    "title": "The Triangle of Power",
    "section": "\\(c\\) remains constant",
    "text": "\\(c\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Roots}\n\\\\\\\\\n&\n_{x\\times y}\n\\stackrel{\\phantom{b}}\n\\triangle _{c}\n={}\n\\left(\n\\left( _{x}\\stackrel{}\\triangle _{c}\\right)^{-1} +{}\n\\left(_{y}\\stackrel{}\\triangle _{c}\\right)^{-1}\n\\right)^{-1}\n&\n_{\\phantom{a}}\n\\stackrel{\n  \\left(x^{-1}+y^{-1}\\right)^{-1}\n}\n\\triangle _{c}\n={}\n_{\\phantom{x}}\\stackrel{x}\\triangle _{c} \\times{}\n_{\\phantom{x}}\\stackrel{y}\\triangle _{c}\n\\\\\\\\\n&\n_{\\frac{x}{y}}\n\\stackrel{\\phantom{b}}\n\\triangle _{c}\n={}\n\\left(\n\\left( _{x}\\stackrel{}\\triangle _{c}\\right)^{-1} -{}\n\\left(_{y}\\stackrel{}\\triangle _{c}\\right)^{-1}\n\\right)^{-1}\n&\n_{\\phantom{a}}\n\\stackrel{\n  \\left(x^{-1}-y^{-1}\\right)^{-1}\n}\n\\triangle _{c}\n={}\n\\frac\n{_{\\phantom{x}}\\stackrel{x}\\triangle _{c} }\n{_{\\phantom{x}}\\stackrel{y}\\triangle _{c}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/linear_regression/linear_regression.html",
    "href": "posts/linear_regression/linear_regression.html",
    "title": "Linear Regression (by hand)",
    "section": "",
    "text": "We have three functions above that we can fit points to. \\(f_1\\) describes a straight line while \\(f_2\\) describes a parabola.\nFor our points, we can represent them as \\((x_i, y_i)\\) for \\(i = 1, 2, 3, ... , n\\). The given function models. Both function 1 and 2 are used to predict a \\(y\\) value from an \\(x\\) value. Additionally, the domain of \\(f\\) is \\(x >= 0\\). To calculate probability and likelihood, the density function, \\(f(r) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(r-\\mu)^2}{2\\sigma^2}}\\) will be used. For this function, \\(\\mu\\) refers to the mean and will be set at 0 and \\(\\sigma\\) refers to the standard deviation and will be set to 1.\nFinally, for any model \\(f_j\\), we can obtain\n\na derived set of residuals or errors, \\(r_j\\),\na respective probability model, \\(p_j\\),\na joint probability model, \\(J_j\\),\na likelihood function, \\(L_j\\), and\na log-likelihood function, \\(\\ell_j\\).\n\n\n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_1(t_i) = 100 + a_1t_i\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{1i} = y_i -(100 + a_1t_i) \\Leftrightarrow r_{1i} = y_i -100 - a_1t_i \\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_1(t_i, y_i;a_1) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_1(\\overrightarrow{t}, \\overrightarrow{y}; a_1) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_1\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_1(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_1(a_1;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\) \\[\n\\begin{align*}\n\\mathscr{}\n&= \\ln(L_1(a_1;\\overrightarrow{t}, \\overrightarrow{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i - 100 - a_1t_i\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_1(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i - 100 - a_1t_i\\right)^2\\)\n\n\n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_2(t_i; a_1,a_2) = 100 + a_1t_i + a_2t_i^2\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{2i} = y_i -(100 + a_1t_i + a_2t_i^2) \\Leftrightarrow r_{2i} = y_i -100 - a_1t_i - a_2t_i^2 \\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_2(t_i, y_i;a_1,a_2) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_2(\\overrightarrow{t}, \\overrightarrow{y}; a_1, a_2) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_2\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_2(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_2(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\) \\[\n\\begin{align*}\n&= \\ln(L_2(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_2(a_1,a_2;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2\\) \n\n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_4(t; a_1,a_2) = 100 + a_1t + a_2\\ln(0.005t+1)\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{4i} = y_i -(100 + a_1t_i + a_2\\ln(0.005t_i+1)) \\Leftrightarrow r_{4i} = y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_4(t_i, y_i;a_1,a_2) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_4(\\overrightarrow{t}, \\overrightarrow{y}; a_1, a_2) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_2\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_4(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_4(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\)\n\n\\[\n\\begin{align*}\n&= \\ln(L_4(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_4(a_1,a_2;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2\\) \n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_5(t; a_1) = 100e^{-0.00005t} + a_1te^{-0.00005t}\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{5i} = y_i -(100e^{-0.00005t_i} + a_1t_ie^{-0.00005t_i}) \\Leftrightarrow r_{5i} = y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_5(t_i, y_i;a_1) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_5(\\overrightarrow{t}, \\overrightarrow{y}; a_1) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_2\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_5(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_5(a_1;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\)\n\n\\[\n\\begin{align*}\n&= \\ln(L_5(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_5(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2\\) \n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_6(t; a_1,a_2) = 100 + a_1t + a_2(1-e^{-0.0003t})\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{6i} = y_i -(100 + a_1t_i + a_2(1-e^{-0.0003t_i})) \\Leftrightarrow r_{6i} = y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\\\\\).\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_6(t_i, y_i;a_1,a_2) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_6(\\overrightarrow{t}, \\overrightarrow{y}; a_1, a_2) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_6\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_6(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_6(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\) \\[\n\\begin{align*}\n&= \\ln(L_6(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_6(a_1,a_2;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2\\)"
  },
  {
    "objectID": "posts/usa_christian/index.html",
    "href": "posts/usa_christian/index.html",
    "title": "American Christianity",
    "section": "",
    "text": "The General Social Survey\nThe General Social Survey (GSS) is a nationally representative survey of American adults that has been conducted since 1972. The GSS is conducted by the National Opinion Research Center (NORC) at the University of Chicago, and it is widely considered to be one of the most important sources of data on trends in American attitudes, beliefs, and behaviors.\n\n\nShow the code\ndf_raw <- readxl::read_excel(\"GSS.xlsx\")\n\n\nOne of the key findings of the GSS is that Christianity in the United States is in decline. In the early 1970s, around 90% of Americans identified as Christian, but by 2020, that number had dropped to around 70%. This decline is particularly pronounced among young adults, with only around half of adults under the age of 30 identifying as Christian.\nOne of the main reasons for this decline is the rise of the “Nones” - a term used to describe people who do not identify with any particular religion. The number of Nones in the United States has been steadily increasing over the past few decades, and by 2020, around one in four Americans identified as Nones.\n\n\nShow the code\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(across(everything(), ~replace(., . %in% c(\"Protestant\",\"Catholic\",\"Orthodox-christian\") , \"Christian\"))) %>%\n  mutate(across(everything(), ~replace(., . %in% c(\"Buddhism\",\"Hinduism\",\"Other eastern religions\",\"Inter-nondenominational\",\"Native american\",\"Other\",\"Muslim/islam\") , \"Other\"))) %>%\n  group_by(year, relig) %>% \n  count() %>%\n  filter(!relig %in% c(\".n:  No answer\", \".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Religion\",\n    title = \"Percent Religon Over Time\"\n  ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw()\n\n\n\n\n\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Catholicism has remained relatively stable, with the proportion of Catholics in the U.S. population staying around 30% since the 1970s. On the other hand, the proportion of Protestants has seen a slight decline, dropping from around 55% in the 1970s to around 45% in 2020.\nOther branches of Christianity, such as Mormonism, Jehovah’s Witnesses, and Eastern Orthodoxy, have seen an increase in recent years. By 2020, these denominations made up around 5% of the U.S. population, a significant increase from their representation in the 1970s.\n\n\nShow the code\nchristian_vector <- c('PROTESTANT','CATHOLIC','CHRISTIAN','ORTHODOX-CHRISTIAN') %>% tolower()\n\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  filter(tolower(relig) %in% christian_vector) %>% \n  mutate(across(everything(), ~replace(., . ==  \"Orthodox-christian\" , \"Christian\"))) %>% \n  mutate(across(everything(), ~replace(., . ==  \"Christian\" , \"Other\"))) %>% \n  group_by(year, relig) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Denomination\",\n    title = \"Percent Christian Denomination Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw()"
  },
  {
    "objectID": "posts/usa_christian/index.html#rise-of-none",
    "href": "posts/usa_christian/index.html#rise-of-none",
    "title": "American Christianity",
    "section": "Rise of ‘None’",
    "text": "Rise of ‘None’\nOne of the key findings of the GSS is that Christianity in the United States is in decline. In the early 1970s, around 90% of Americans identified as Christian, but by 2020, that number had dropped to around 70%. This decline is particularly pronounced among young adults, with only around half of adults under the age of 30 identifying as Christian.\nOne of the main reasons for this decline is the rise of the “Nones” - a term used to describe people who do not identify with any particular religion. The number of Nones in the United States has been steadily increasing over the past few decades, and by 2020, around one in four Americans identified as Nones.\n\n\nShow the code\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(across(everything(), ~replace(., . %in% c(\"Protestant\",\"Catholic\",\"Orthodox-christian\") , \"Christian\"))) %>%\n  mutate(across(everything(), ~replace(., . %in% c(\"Buddhism\",\"Hinduism\",\"Other eastern religions\",\"Inter-nondenominational\",\"Native american\",\"Other\",\"Muslim/islam\") , \"Other\"))) %>%\n  group_by(year, relig) %>% \n  count() %>%\n  filter(!relig %in% c(\".n:  No answer\", \".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Religion\",\n    title = \"Percent Religon Over Time\"\n  ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_bw()"
  },
  {
    "objectID": "posts/usa_christian/index.html#new-christians",
    "href": "posts/usa_christian/index.html#new-christians",
    "title": "American Christianity",
    "section": "New Christians",
    "text": "New Christians\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Catholicism has remained relatively stable, with the proportion of Catholics in the U.S. population staying around 30% since the 1970s. On the other hand, the proportion of Protestants has seen a slight decline, dropping from around 55% in the 1970s to around 45% in 2020.\nOther branches of Christianity, such as Mormonism, Jehovah’s Witnesses, and Eastern Orthodoxy, have seen an increase in recent years. By 2020, these denominations made up around 5% of the U.S. population, a significant increase from their representation in the 1970s.\n\n\nShow the code\nchristian_vector <- c('PROTESTANT','CATHOLIC','CHRISTIAN','ORTHODOX-CHRISTIAN') %>% tolower()\n\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  filter(tolower(relig) %in% christian_vector) %>% \n  mutate(across(everything(), ~replace(., . ==  \"Orthodox-christian\" , \"Christian\"))) %>%\n  mutate(across(everything(), ~replace(., . ==  \"Christian\" , \"Other\"))) %>% \n  group_by(year, relig) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Denomination\",\n    title = \"Percent Christian Denomination Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_bw()"
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-christians",
    "href": "posts/usa_christian/index.html#strength-of-christians",
    "title": "State of American Christianity",
    "section": "Strength of Christians",
    "text": "Strength of Christians\nAnother aspect of the decline of Christianity in the United States is the weakening of its strength within the population. This can be seen in the decreasing participation in religious practices such as church attendance.\nAccording to the GSS, the proportion of general Christians who do not attend church has increased from 15% in 1970 to 23% in 2020. Additionally, the proportion of Christians who attend church weekly has declined from 41% in 1970 to 36% in 2020. However, the proportion of Christians who attend church either monthly or less frequently has remained relatively stable.\nThis decline in church attendance is particularly pronounced among young Christians. The proportion of young Christians who attend church weekly or more frequently has decreased in all categories, except for the “seldom” category in which young Christians increased from 23% in 1970 to 45% in 2020.\nTechnical Note: This code is using the tidyverse and patchwork library to analyze and create a plot of church attendance among Christians over time. The code first filters the dataframe to only include rows where the “age” column contains numeric values, and then converts the “age” and “year” columns to numeric format. Next, it combines all Christian denominations as a single variable called “Christian”, and then filters to only Christians. The code replaces the several different categories of church attendance to Weekly, Monthly, Yearly, and Seldom. The code then groups and counts how many people in each category and uses “ggplot” to make a visualization.\n\n\nShow the code\nchristian_attend <- df_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(age = as.numeric(age)) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(age = plyr::round_any(as.numeric(age), 10)) %>% \n  mutate(across(everything(), ~replace(., . %in% c(\"Protestant\",\"Catholic\",\"Orthodox-christian\") , \"Christian\"))) %>% \n  filter(relig == \"Christian\") %>% \n  filter(!attend %in% c(\".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>%\n  mutate(attend = ifelse(attend %in% c(\"Every week\",\"Nearly every week\",\"Several times a week\"), \"Weekly\", attend),\n         attend = ifelse(attend %in% c(\"2-3 times a month\",\"About once a month\",\"At least monthly\"), \"Monthly\", attend),\n         attend = ifelse(attend %in% c(\"About once or twice a year\",\"Several times a year\"), \"Yearly\", attend),\n         attend = ifelse(attend %in% c(\"Less than once a year\",\"Never\"), \"Seldom\", attend)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(attend = factor(attend, levels = c(\"Weekly\", \"Monthly\", \"Yearly\",\"Seldom\"))) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nChristians Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n  \n\ndf_christian <- df_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(age = as.numeric(age)) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(age = plyr::round_any(as.numeric(age), 10)) %>% \n  filter(!attend %in% c(\".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>%\n  mutate(attend = ifelse(attend %in% c(\"Every week\",\"Nearly every week\",\"Several times a week\"), \"Weekly\", attend),\n         attend = ifelse(attend %in% c(\"2-3 times a month\",\"About once a month\",\"At least monthly\"), \"Monthly\", attend),\n         attend = ifelse(attend %in% c(\"About once or twice a year\",\"Several times a year\"), \"Yearly\", attend),\n         attend = ifelse(attend %in% c(\"Less than once a year\",\"Never\"), \"Seldom\", attend)) %>% \n  filter(attend != \".n:  No answer\") %>% \n  mutate(attend = factor(attend, levels = c(\"Weekly\", \"Monthly\", \"Yearly\",\"Seldom\"))) \n\nchristian_attend_youth <- df_christian %>%\n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nChristian Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n\n\nchristian_attend + christian_attend_youth & scale_y_continuous(limits = c(.1, .46),labels = scales::percent)\n\n\n\n\n\nStrength of Protestants\nWhile overall Christianity in the United States is in decline, the decline has not been evenly distributed across different denominations. In particular, the decline of Protestant Christianity appears to be less pronounced than that of Christianity as a whole.\nWhen it comes to church attendance, the data shows that weekly attendance among Protestants has remained relatively stable. According to the GSS, the proportion of Protestants who attend church weekly has remained around 35% since 1970. Furthermore, the weekly attendance for both general Protestants and young Protestants actually increased slightly.\n\n\nShow the code\nprot_attend <- df_christian %>% \n  filter(relig == \"Protestant\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nProtestants Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() +  theme(legend.position = 'none')\n\nprot_attend_youth <- df_christian %>%\n  filter(relig == \"Protestant\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"\",\n    title = \"Church Attendance Among\\nProtestant Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n\n\n\nprot_attend + prot_attend_youth & scale_y_continuous(limits = c(.1, .42),labels = scales::percent)\n\n\n\n\n\nStrength of Catholics\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Among all Christian denominations, the Catholic Church appears to be the one that has been most affected by the decline in attendance.\nAccording to the GSS, weekly attendance among Catholics has decreased significantly since 1970. Among general Catholics, weekly attendance dropped from 53% to 27% during this time period. Similarly, weekly attendance among Catholic youth has also decreased, declining from 40% in 1970 to 15% in 2020.\nHowever, the decline in attendance has not been limited to weekly attendance. The proportion of Catholics who attend church seldom or less frequently has also increased. Seldom attendance increased from 11% in 1970 to 26% in 2020. Furthermore, the youth seldom attendance doubled from 12% to 24% in 2020.\n\n\nShow the code\ncath_attend <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholics Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n\ncath_attend_youth <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1) +\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholic Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n  \n\ncath_attend + cath_attend_youth & scale_y_continuous(limits = c(.1, .54),labels = scales::percent)\n\n\n\n\n\nCharting the decline\nIn conclusion, this blog post has discussed the decline of Christianity in the United States over the past few decades, as reported by the GSS. The data shows that the proportion of Christians in the U.S. population has dropped from around 90% in the early 1970s to around 70% in 2020.\nThe decline of Christianity is also reflected in the weakening of its strength within the population, as seen in the decreasing participation in religious practices such as church attendance. The proportion of general Christians who do not attend church has increased from 15% in 1970 to 23% in 2020 and the proportion of Christians who attend church weekly has declined from 41% in 1970 to 36% in 2020. However, the proportion of Christians who attend church either monthly or less frequently has remained relatively stable.\nWhen looking at different Christian denominations, the decline of Protestant Christianity appears to be less pronounced than that of Christianity as a whole. The proportion of Protestants who attend church weekly has remained around 35% since 1970, and weekly attendance for both general Protestants and young Protestants actually increased slightly. On the other hand, the Catholic Church appears to be the one that has been most affected by the decline in attendance. Among general Catholics, weekly attendance dropped from 53% to 27% during this time period, and the youth seldom attendance doubled from 12% to 24% in 2020.\nThe term “Chreaster” is defined as such: “A Christian who does not frequently attend church, attending only on the major holidays of Christmas and Easter.” (Wiktionary) Chreasters have been on the increase especially among younger Christians and Catholics.\nWhile this blog post does not try to explain the ‘why’ in this phenomenon, there are several factors that readers can probably think of that have led to this decline. Some of the possible factors include the increasing secularization of American society, the rise of social media and the internet, the growing diversity of American society, the sexual abuse scandals that have plagued the Catholic Church, and the loss of trust and credibility among Catholics."
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-protestants",
    "href": "posts/usa_christian/index.html#strength-of-protestants",
    "title": "American Christianity",
    "section": "Strength of Protestants",
    "text": "Strength of Protestants\nWhile overall Christianity in the United States is in decline, the decline has not been evenly distributed across different denominations. In particular, the decline of Protestant Christianity appears to be less pronounced than that of Christianity as a whole.\nWhen it comes to church attendance, the data shows that weekly attendance among Protestants has remained relatively stable. According to the General Social Survey (GSS), the proportion of Protestants who attend church weekly has remained around 35% since 1970. Furthermore, the weekly attendance for both general Protestants and young Protestants actually increased slightly.\n\n\nShow the code\nprot_attend <- df_christian %>% \n  filter(relig == \"Protestant\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nProtestants Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() +  theme(legend.position = 'none')\n\nprot_attend_youth <- df_christian %>%\n  filter(relig == \"Protestant\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"\",\n    title = \"Church Attendance Among\\nProtestant Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n\n\n\nprot_attend + prot_attend_youth & scale_y_continuous(limits = c(.1, .42),labels = scales::percent)"
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-protestants-1",
    "href": "posts/usa_christian/index.html#strength-of-protestants-1",
    "title": "American Christianity",
    "section": "Strength of Protestants",
    "text": "Strength of Protestants\n\n\nShow the code\ncath_attend <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among Catholics Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n\ncath_attend_youth <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among Protestant Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw()\n\ncath_attend + cath_attend_youth & scale_y_continuous(limits = c(.1, .5),labels = scales::percent)\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\nWarning: Removed 1 rows containing missing values (geom_text_repel)."
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-catholics",
    "href": "posts/usa_christian/index.html#strength-of-catholics",
    "title": "American Christianity",
    "section": "Strength of Catholics",
    "text": "Strength of Catholics\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Among all Christian denominations, the Catholic Church appears to be the one that has been most affected by the decline in attendance.\nAccording to the General Social Survey (GSS), weekly attendance among Catholics has decreased significantly since 1970. Among general Catholics, weekly attendance dropped from 53% to 27% during this time period. Similarly, weekly attendance among Catholic youth has also decreased, declining from 40% in 1970 to 15% in 2020.\nHowever, the decline in attendance has not been limited to weekly attendance. The proportion of Catholics who attend church seldom or less frequently has also increased. Seldom attendance increased from 11% in 1970 to 26% in 2020. Furthermore, the youth seldom attendance doubled from 12% to 24% in 2020.\n\n\nShow the code\ncath_attend <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholics Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n\ncath_attend_youth <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1) +\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholic Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n  \n\ncath_attend + cath_attend_youth & scale_y_continuous(limits = c(.1, .54),labels = scales::percent)"
  },
  {
    "objectID": "posts/lin_reg/index.html",
    "href": "posts/lin_reg/index.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "You have a dataset where each row represents an individual sample and each column represents a variable about each sample.\n\n\\(n\\) refers to the total number of samples you have\n\\(\\vec{x}\\) refers to the independent variable column in your data\n\\(\\vec{y}\\) refers to the dependent variable column in your data\nWe will use the notation \\(x_i\\) and \\(y_i\\) to denote the\\(i\\)th observations of \\(x\\) and \\(y\\), respectively.This would mean, for example: \\(x_1\\) would refer to the first x value in your data while \\(y_n\\) would refer to the last y value in your data\n\n\n\n\nThe goal of simple linear regression is to find the line of best fit, which is represented by the equation \\(f(x)=\\beta_0+\\beta_1x\\) , where \\(\\beta_1\\) is the slope of the line and \\(\\beta_0\\) is the y-intercept. The function \\(f(x)\\) will find any \\(\\hat{y_i}\\) which is the predicted y value of \\(x_i\\) with a best fit line.\n\n\n\nGiven any line, we can calculate how different our predicted value, \\(\\hat{y_i}\\) is from our actual data point, \\(y_i\\). This vertical distance from the real y output and the predicted y output is called the residual. The residual for a given point is calculated as \\(e_i = y_i - \\hat{y_i}\\).\nThe goal of simple linear regression is to minimize the sum of the squared residuals, also known as the residual sum of squares (\\(RSS\\)). This is represented by the equations:\n\\[\\sum_{i=1}^n \\left(e_i\\right)^2 \\text{ or } \\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2 \\text{ or } \\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2\\].\nThe closer any given line is to the best fit line, the lower the \\(RSS\\) will be. In fact, the best fit line represents the line with the lowest possible \\(RSS\\). This line is found by manipulating the values for \\(\\beta_0\\) and \\(\\beta_1\\) since the x values must stay the same.\n\n\n\nThere exists a combination of \\(\\beta_0\\) and \\(\\beta_1\\) that is the lowest. Geometrically, we can imagine a shape in three dimensions where two inputs (\\(\\beta_0\\) and \\(\\beta_1\\)) produce an output (\\(RSS\\)). Roughly speaking, where the three dimensional slope is equal to 0 is where the combination of \\(\\beta_0\\) and \\(\\beta_1\\) will produce the lowest possible \\(RSS\\). Since the derivative of an equation can tell us the slope at any given point, we can set both equations’ derivatives to zero to find where slope is zero for both equations.\nStep One: Take the partial derivatives with respect to \\(\\beta_0\\) and \\(\\beta_1\\)\nThe partial derivative with respect to \\(\\beta_0\\) is calculated as so:\n\\[\\frac{\\partial}{\\partial\\beta_0}\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2\\] \\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial\\beta_0}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\frac{\\partial}{\\partial\\beta_0}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)(-1) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nThe partial derivative with respect to \\(\\beta_1\\) is calculated as so:\n\\[\\frac{\\partial}{\\partial\\beta_1}\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2\\] \\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial\\beta_1}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\frac{\\partial}{\\partial\\beta_1}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)(-x_i) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^nx_i\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nStep Two: Set the simplified partial derivatives to zero and solve for \\(\\beta_0\\) and \\(\\beta_1\\) respectively\nSolving for \\(\\beta_0\\): \\[0 = -2\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^n\\left(y_i - \\beta_0-\\beta_1x_i\\right) &\\text{Simplify}\\\\\n&0 = \\sum_{i=1}^ny_i - \\sum_{i=1}^n\\beta_0-\\sum_{i=1}^n\\beta_1x_i &\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^ny_i - n\\beta_0-\\beta_1\\sum_{i=1}^nx_i &\\text{Constant Multiple Rule for Sums}\\\\\n& n\\beta_0 = \\sum_{i=1}^ny_i -\\beta_1\\sum_{i=1}^nx_i &\\text{Simplify}\\\\\n& \\beta_0 = \\frac{\\sum_{i=1}^ny_i}{n} -\\frac{\\beta_1\\sum_{i=1}^nx_i}{n} &\\text{Simplify}\\\\\n& \\beta_0 = \\bar{y} - \\beta_1\\bar{x} &\\text{Simplify to 'Average'}\\\\\n\\end{align*}\n\\]\nSolving for \\(\\beta_1\\) while substituting \\(\\beta_0\\):\n\\[0 = -2\\sum_{i=1}^nx_i\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\] \\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^nx_i\\left(y_i - ((\\bar{y} - \\beta_1\\bar{x})+\\beta_1x_i)\\right) &\\text{Substitute }\\beta_0\\\\\n&0 = \\sum_{i=1}^nx_i(y_i-\\bar{y}) - \\sum_{i=1}^n\\beta_1x_i(x_i-\\bar{x})&\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nx_i(y_i-\\bar{y}) - \\beta_1\\sum_{i=1}^nx_i(x_i-\\bar{x})&\\text{Constant Multiple Rule for Sums}\\\\\n& \\beta_1= \\frac{\\sum_{i=1}^nx_i(y_i-\\bar{y})}{\\sum_{i=1}^nx_i(x_i-\\bar{x})} &\\text{Simplify}\\\\\n& \\beta_1= \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} &\\text{Simplify (steps skipped)}\\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/pandasvtidy/index.html",
    "href": "posts/pandasvtidy/index.html",
    "title": "Pandas vs Tidyverse",
    "section": "",
    "text": "Python’s pandas and R’s tidyverse are incredible tools to manipulate tabular data. Below is a data science “Rosetta Stone” to compare the different but similar commands in both languages.\n\nCreate a dataframe\ntidyverse\ndf <- data.frame(\n      col_one = c('A','B','C','D'),\n      col_two = c(1,2,3,4)\n)\npandas\ndf = pd.DataFrame(\n      {'col_one': ['A', 'B', 'C','D'],\n       'col_two': [1, 2, 3, 4]}\n)\n\n\nInspect a dataframe\ntidyverse\n\n    # Output column names\n    df %>% colnames\n\n    # Output column dimensions (row and column length)\n    df %>% dim\n    df %>% nrow\n    df %>% ncol\n\n    # Output first/last n rows\n    df %>% head(n)\n    df %>% tail(n)\n\n    #Get information about a df\n    df %>% summary\n    df %>% glimpse\n\n    #Clean column names\n    df %>% janitor::clean_names()\npandas\n\n    # Output column names\n    df.columns\n\n    # Output column dimensions (row and column length)\n    df.shape\n    df.shape[0]\n    df.shape[1]\n\n    # Output first/last n rows\n    df.head(n)\n    df.tail(n)\n\n    #Get information about a df\n    df.describe()\n    df.info()\n\n    #Clean column names\n    from skimpy import clean_columns\n    clean_columns(df)\n\n\nImport data\ntidyverse\ndf <- read_csv('data.csv')\ndf <- read_csv('data.csv', col_names = F)\npandas\ndf = pd.read_csv('data.csv')\ndf = pd.read_csv('data.csv', header=None)\n\n\nCount frequency of vaules in a column\ntidyverse\ndf %>% count(col_one)\ndf %>% count(col_one, sort = T)\ndf$col_one %>% janitor::tabyl()\npandas\ndf['col_one'].value_counts()\ndf['col_one'].value_counts(ascending = False)\ndf['col_one'].value_counts(dropna=False)\ndf['col_one'].value_counts(normalize=True)\ndf['col_one'].value_counts(bin = n)\n\n\nCalculate summary statistics\ntidyverse\ndf$col_one %>% mean\ndf$col_one %>% median\ndf$col_one %>% sd\ndf$col_one %>% min\ndf$col_one %>% max\npandas\ndf['col_one'].mean()\ndf['col_one'].median()\ndf['col_one'].std()\ndf['col_one'].min()\ndf['col_one'].max()\n\n\nKeep columns\ntidyverse\ndf %>% select(col_one)\ndf %>% select(col_one,col_two)\ndf %>%select(matches(\"[pt]al\"))\ndf %>% select(starts_with(\"col\"))\n...\ndf %>% select(ends_with(\"two\"))\n...\ndf %>% select(contains(\"col\"))\n...\npandas\ndf.filter(items=['col_one'])\ndf.filter(items=['col_one','col_two'])\ndf.filter(regex='[pt]al')\ndf.loc[:,df.columns.str.startswith(\"col\")]\ndf.filter(regex='^col')\ndf.loc[:,df.columns.str.endswith(\"two\")]\ndf.filter(regex='two$')\ndf.loc[:,df.columns.str.contains(\"col\")]\ndf.filter(like='col')\n\n\nDrop columns\ntidyverse\ndf %>% select(!col_one)\ndf %>% select(!c(col_one,col_two))\npandas\ndf.drop(columns=['col_one'])\ndf.drop(columns=['col_one','col_two'])\n\n\nRename a column\ntidyverse\ndf %>% rename(col_1 = column_one)\ndf %>% rename(col_1 = column_one, \n              col_2 = column_two\n)\npandas\ndf.rename(columns={\"column_one\": \"col_1\"})\ndf.rename(columns={\"column_one\": \"col_1\", \n                   \"column_two\": \"col_2\"}\n)\n\n\nChange datatype of a column\ntidyverse\ndf %>% mutate(Race = as.character(Race), Age = as.numeric(Age))\npandas\ndf.astype({\"Race\":'category', \"Age\":'int64'})\n\n\nLocate values\ntidyverse\ndf[,]\ndf[1,]\ndf[c(1,6),]\ndf[c(1:6),]\ndf[,'col_one']\ndf[,c('col_one','col_three')]\ndf %>% select(col_one:col_three)\ndf[,c(1,3)]\ndf[,c('1:3')]\npandas\ndf.loc[:,:]\ndf.loc[1,:] \ndf.loc[[1,6],:] \ndf.loc[[1:6],:] \ndf.loc[:,['col_one']]\ndf.loc[:,['col_one','col_three']] \ndf.loc[:,'col_one':'col_three'] \ndf.iloc[:,[1,3]]\ndf.iloc[:,1:3]\n\n\nQuery values in a column\ntidyverse\ndf %>% filter(col_one >= 100)\ndf %>% filter(col_one != \"Blue\")\ndf %>% filter(col_one %in% c('A','B'))\ndf %>% filter(!(Race == \"White\" & Gender == \"Male\"))\npandas\ndf.query(\"col_one >= 100\")\ndf.query(\"col_one != 'Blue'\")\ndf.query(\"col_one in ['A', 'B']\")\ndf.query(\"not (Race == 'White' and Gender == 'Male')\")\n\n\nQuery by string\ntidyverse\n    df %>% filter(str_detect(col1, \"string\"))\n    df %>% filter(str_detect(col1, c(\"string1\", \"string2\")))\n    df %>% filter(str_starts(col1, \"string\"))\n    df %>% filter(str_ends(col1, \"string\"))\n    df %>% filter(str_match(col1, \"regex_pattern\"))\n    ```\n    \n**pandas**\n``` python\n    df.query('col1.str.contains(\"string\").values')\n    df.query('col1.str.contains([\"string1\", \"string2\"]).values')\n    df.query('col1.str.startswith(\"string\").values')\n    df.query('col1.str.endswith(\"string\").values')\n    df.query('col1.str.match(\"regex_pattern\").values')\n\n\nSort a dataframe by a column’s value\ntidyverse\ndf %>% arrange('col_one')\ndf %>% arrange(col_one %>% desc())\npandas\ndf.sort_values('col_one')\ndf.sort_values('col_one', ascending=False)\n\n\nKeep distinct values in a column\ntidyverse\ndf %>% distinct(col_one, .keep_all = T)\ndf %>% distinct()\npandas\ndf.drop_duplicates(subset = [\"col_one\"])\ndf.drop_duplicates()\n\n\nReplace values in a column\ntidyverse\ndf %>% mutate(across(everything(), ~replace(., . ==  2 , \"foo\")))\ndf %>% mutate(across(c(col_one,col_two), ~replace(., . ==  2 , \"foo\")))\ndf %>% mutate(col_one = ifelse(col_one == 2, \"foo\", col_one))\npandas\ndf.replace(2,\"foo\")\ndf[['col_one','col_two']].replace(2,\"foo\")\ndf['col_one'].replace(2,\"foo\")\n\n\nDrop NA’s in a column\ntidyverse\ndf %>% drop_na\ndf %>% drop_na(c(\"col_one\", \"col_two\"))\ndf %>% select(where(~mean(is.na(.)) < n)) #percent threshold\npandas\ndf.dropna()\ndf.dropna(subset=['col_one', 'col_two'])\ndf.dropna(thresh=n) #integer threshold\n\n\nFill NA’s in a column\ntidyverse\ndf %>% replace(is.na(.), x)\ndf %>% mutate(col_one = ifelse(is.na(col_one),x,col_one))\ndf %>% fill(col_one, .direction = \"up\")\ndf %>% mutate(col_one= ifelse(is.na(col_one), mean(df$col_one, na.rm = T), col_one))\npandas\ndf.fillna(x)\ndf['col_one'].fillna(x)\ndf['col_one'].fillna(method = 'ffill')\ndf['col_two'].fillna(df['col_two'].mean())\n\n\nGroup and summarize columns\ntidyverse\ndf %>% group_by(Race) %>% count()\ndf %>% group_by(Race) %>% summarize(new_col = median(Income))\ndf %>% group_by(Race, Sex) %>%\n     summarize(\n       new_col1 = median(Income),\n       new_col2 = n(),\n       new_col3 = mean(age)\n)\npandas\ndf.groupby('Race', as_index=False).count()\ndf.groupby('Race', as_index=False)['Income'].median()\n(df.groupby(['Race', 'Sex'], as_index=False)\n   .agg(\n      new_col1=pd.NamedAgg(column = 'Income', aggfunc = np.median),\n      new_col1=pd.NamedAgg(column = 'Age', aggfunc = np.mean)\n))\n\n\nPivot longer\ntidyverse\ndf %>% \n  pivot_longer(\n     cols = Belgium:`United Kingdom`,\n     names_to = \"Country\",\n     values_to = \"Fatalities\"\n)\npandas\n(df\n  .melt(\n    id_vars=['iyear'],\n    var_name='Country', \n    value_name='Fatalities')\n)\n\n\nPivot wider\ntidyverse\ndf %>% pivot_wider(\n      names_from = state, \n      values_from = number\n)\npandas\ndf.pivot_table(index=['year','name','sex'],\n      columns='state',\n      values='number'\n)\n\n\nBind two dataframes\ntidyverse\ndf1 %>% bind_rows(df2)\ndf1 %>% bind_cols(df2)\npandas\npd.concat([df1,df2])\npd.concat([df1,df2], axis=1)\n\n\nInner join two dataframes\ntidyverse\ndf1 %>% inner_join(\n      df2, by = c(col_one = \"first_col\")\n)\npandas\npd.merge(df1, df2, \n     left_on='col_one', right_on='first_col'\n)\n\n\nLeft join dataframes\ntidyverse\ndf1 %>% left_join(df2, \n      by = c(col_one = \"first_col\")\n)\npandas\npd.merge(df1, df2, how = 'left',\n     left_on='col_one', right_on='first_col'\n)\n\n\nAdd a new column\ntidyverse\ndf %>% mutate(\n      twomore = x + 2,\n      twoless    = x - 2\n)\npandas\ndf.assign(\n  twomore = lambda df: df.x + 2,\n  twoless    = lambda df: df.x - 2\n)"
  },
  {
    "objectID": "posts/lin_alg_rev/index.html",
    "href": "posts/lin_alg_rev/index.html",
    "title": "Linear Algebra Exam 1 Review",
    "section": "",
    "text": "System of Equations\n\\[\n\\begin{cases}\na_{11}x_1 & + &a_{12}x_2 &+ \\dots+ & a_{1n}x_n &= b_1 \\\\\na_{21}x_1 & + &a_{22}x_2 &+ \\dots+ & a_{2n}x_n &= b_2 \\\\\n\\vdots && \\vdots & \\ddots & \\vdots & \\vdots \\\\\na_{m1}x_1 & + &a_{m2}x_2 &+ \\dots+ & a_{mn}x_n &= b_m\n\\end{cases}\n\\]\nAugmented matrix: \\(\\begin{bmatrix}A|\\vec{b}\\end{bmatrix}\\)\n\\[\n\\left[\\begin{array}{@{}cccc|c@{}}\na_{11} & a_{12} & \\dots & a_{1n} & b_1 \\\\\na_{21} & a_{22} & \\dots & a_{2n} & b_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn} & b_m\n\\end{array}\\right]\n\\]\nVector Equation: \\(x_1\\vec{a_1}+x_2\\vec{a_2}+...+x_n\\vec{a_n}=\\vec{b}\\)\n\\[\nx_1\n\\begin{bmatrix}\na_{11}\\\\\na_{21}\\\\\n\\vdots\\\\\na_{m1}\n\\end{bmatrix} +\nx_2\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\n\\vdots\\\\\na_{m2}\n\\end{bmatrix}+\n\\dots +\nx_n\n\\begin{bmatrix}\na_{1n}\\\\\na_{2n}\\\\\n\\vdots\\\\\na_{mn}\n\\end{bmatrix} =\n\\begin{bmatrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_m\n\\end{bmatrix}\n\\]\nMatrix Equation: \\(A\\vec{x}=\\vec{b}\\)\n\\[\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}=\n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m\n\\end{bmatrix}\n\\]\n\n\n\nA system of linear equations has either:\n\nno solution (inconsistent)\nexactly one solution (consistent not-unique)\ninfinitely many solutions. (consistent unique)\n\nThese are two fundamental questions (consistency, uniqueness)\nIf two linear systems are equivalent:\n\nthe augmented matrices of these two linear systems are row equivalent\nthey both have the same solution set\n\nFor row reduction:\n\nEvery elementary row operation is reversible.\nElementary row operations on an augmented matrix never change the solution set of the associated linear system. TRUE(solution set never changes)\n\nMiscellaneous:\n\nThe solution set of a linear system involving variables \\(x_1,...,x_n\\) is a list of numbers \\((s_1,...,s_n)\\) that makes each equation in the system a true statement when the values \\(s_1,...,s_n\\) are substituted for \\(x_1,...,x_n\\) respectively. FALSE (this may just be a single solution, not the solution set)\n\n\n\n\nFor augmented matrices:\n\nDefinition- Nonzero row or column in a matrix means a row or column that contains at least one non-zero leading entry\nDefinition- Leading entry of a row refers to the leftmost nonzero entry (in a nonzero row).\n\nDefinition- Row echelon form (REF):\n\nAll nonzero rows are above any rows of all zeros.\nEach leading entry of a row is in a column to the right of the leading entry of the row above it.\nAll entries in a column below a leading entry are zeros.\n\nREF- not unique, RREF- unique\nDefinition- Reduced row echelon form (RREF):\n\nMeets requirements for row echelon form\nThe leading entry in each nonzero row is 1.\nEach leading 1 is the only nonzero entry in its column.\n\nThe pivot positions in a matrix do NOT depend on whether row interchanges are used in the row reduction process because pivot positions are derived from RREF which are unique\nTheorem 1- Uniqueness of the Reduced Echelon Form:\n\nEach matrix is row equivalent to one and only one reduced echelon matrix\n\nFor pivots:\n\nDefinition- Pivot position in a matrix A is a location in A that corresponds to a leading 1 in the reduced echelon form of A.\nDefinition- Pivot column is a column of A that contains a pivot position.\n\nFor variables:\n\nDefinition- basic variables are the variables corresponding to pivot columns in a RREF matrix. (A basic variable in a linear system is a variable that corresponds to a pivot column in the coefficient matrix.)\nDefinition- free variables are the other variables not corresponding to a pivot column (zeroed out)\n\nWhenever a system has free variables, the solution set contains many solutions FALSE(since [[00|1],[00|0]] has a free variable but is inconsistent)\nTheorem 2- Existence and Uniqueness Theorem:\nA linear system is consistent if and only if the rightmost column of the augmented matrix is not a pivot column—that is, if and only if an echelon form of the augmented matrix has no row of the form\n[ 0 … 0 | b ] with b nonzero\nIf a linear system is consistent, then the solution set contains either\n\na unique solution, when there are no free variables, or\ninfinitely many solutions, when there is at least one free variable.\n\nIf one row in an echelon form of an augmented matrix is [ … a_i … | 0 ] , then the associated linear system is inconsistent. FALSE\nSuppose a m x n coefficient matrix for a system has m pivot columns. This is consistent by theorem 2.\nSuppose a system of linear equations has a m x n augmented matrix whose nth column is a pivot column. This is inconsistent by theorem 2\nFor row reduction:\n\nRow equivalent matrices will always row reduce to one matrix in RREF form\nThe row reduction algorithm applies to all nonzero matrices\nReducing a matrix to echelon form is called the forward phase of the row reduction process.\n\nMiscellaneous:\n\nFinding a parametric description of the solution set of a linear system is the same as solving the system. A general solution of a system is an explicit description of all solutions of the system.\n\n\n\n\nDefinition- Vector\n\nA matrix with only one column is called a column vector, or simply a vector\nThe length n of a vector indicates indicates that is in the set: \\(\\mathbb{R}^n\\)\n\\(\\begin{bmatrix}u_1\\\\u_2\\\\\\vdots\\\\u_n\\end{bmatrix}\\in\\mathbb{R}^n\\)\n\\(\\begin{bmatrix}a\\\\b\\end{bmatrix} \\neq \\begin{bmatrix}a &b\\end{bmatrix}\\)\nAny list of n real numbers is a vector in \\(\\mathbb{R}^n\\)\n\nFor linear combinations and span:\n\nDefinition- Linear combination: Given vectors \\(v_1, v_2, ..., v_p\\) in \\(\\mathbb{R}^n\\) and given scalars \\(c_1, c_2, ..., c_p\\), the vector y defined by \\(y = c_1v_1 + ... + c_pv_p\\) in which y is a linear combination of \\(v_1, v_2, ..., v_p\\) with weights: \\(c_1, c_2, ..., c_p\\),\nIf \\(\\vec{v_1},...,\\vec{v_p}\\) are in \\(\\mathbb{R^n}\\), then the set of all linear combinations of \\(\\vec{v_1},...,\\vec{v_p}\\) is denoted by Span \\(\\{\\vec{v_1},...,\\vec{v_p}\\}\\) and is called the subset of of \\(\\mathbb{R}^n\\) spanned (or generated) by \\(\\vec{v_1},...,\\vec{v_p}\\). That is, Span\\(\\{\\vec{v_1},...,\\vec{v_p}\\}\\) is the collection of all vectors that can be written in the form \\(c_1\\vec{v_1}+c_2\\vec{v_2}+…+c_p\\vec{v_p}\\) with \\(c_1,…,c_p\\) scalars.\nThe set Span \\(\\{\\vec{u}, \\vec{v}\\}\\) is visualized as a plane in \\(\\mathbb{R}^2\\) if the vectors in that set are linearly independent\nAsking whether the linear system corresponding to an augmented matrix \\([\\vec{a_1}\\vec{a_2}|\\text{ }\\vec{b}]\\) has a solution amounts to asking whether \\(\\vec{b}\\) is in Span \\(\\{\\vec{a_1},\\vec{a_2},\\vec{a_3}\\}\\)\n\nVector equations:\n\nFor \\(x_1\\vec{a_1}+x_2\\vec{a_2}+...+x_n\\vec{a_n}=\\vec{b}\\) , this means that \\(b\\) can be generated by a linear combination of \\(\\vec{a_1},...,\\vec{a_n}\\) if and only if there exists a solution to the linear system corresponding to the matrix\n\n\n\n\nTheorem 3: The equations below have the same solution sets\n\n[\\(\\vec{a_1},…,\\vec{a_n}|\\vec{b}\\)] (Augmented matrix)\n\\(x_1\\vec{a_1}+x_2\\vec{a_2}+...+x_n\\vec{a_n}=\\vec{b}\\), (Vector Equation)\n\\([\\vec{a_1},…,\\vec{a_n}]\\vec{x}=\\vec{b}\\) or \\(A\\vec{x}=\\vec{b}\\) (Matrix Equation)\n\nFor the existence of solutions and consistency\n\nThe equation \\(A\\vec{x}=\\vec{b}\\) has a solution if and only if \\(b\\) is a linear combination of the columns of \\(A\\).\nThe equation \\(A\\vec{x}=\\vec{b}\\) is consistent if the augmented matrix [\\(A\\text{ }|\\text{ }\\vec{b}\\)] has a pivot position in every row. FALSE\nIf the columns of an m x n matrix \\(A\\) span \\(\\mathbb{R}^m\\), then the equation \\(A\\vec{x}=\\vec{b}\\) is consistent for each \\(b\\) in \\(\\mathbb{R}^m\\).\nIf the equation \\(A\\vec{x}=\\vec{b}\\) is inconsistent, then \\(b\\) is not in the set spanned by the columns of \\(A\\).\n\nTheorem 4: The statements below are either all true or all false for A, an m x n coefficient matrix\n\nFor each \\(b\\) in \\(\\mathbb{R}^m\\), the equation \\(A\\vec{x}=\\vec{b}\\) has a solution.\nEach \\(b\\) in \\(\\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\).\nThe columns of A span \\(\\mathbb{R}^m\\).\n\\(A\\) has a pivot position in every row\n\nRow-Vector Rule for Computing \\(A\\vec{x}\\)\n\nIf the product \\(A\\vec{x}\\) is defined, then the ith entry in \\(A\\vec{x}\\) is the sum of the products of corresponding entries from row \\(i\\) of \\(A\\) and from the vector \\(\\vec{x}\\)\n\nTheorem 5: If \\(A\\) is an m x n matrix, \\(\\vec{u}\\) and \\(\\vec{v}\\) are vectors in \\(\\mathbb{R}^n\\) , and \\(c\\) is a scalar, then:\n\n\\(A(\\vec{u}+\\vec{v}) = A\\vec{u}+A\\vec{v}\\)\n\\(A(c\\vec{u})=c(A\\vec{u})\\)\n\nFor linear combinations:\n\nA vector \\(b\\) is a linear combination of the columns of a matrix \\(A\\) if and only if the equation \\(A\\vec{x}=\\vec{b}\\) has at least one solution.\n\nMiscellaneous:\n\nThe first entry in the product \\(A\\vec{x}\\) is a sum of products\nIf \\(A\\) is an m x n matrix and if the equation \\(A\\vec{x}=\\vec{b}\\) is inconsistent for some \\(b\\) in \\(\\mathbb{R}^m\\), then \\(A\\) cannot have a pivot position in every row.\n\n???\n\nIf \\(A\\) is an m x n matrix whose columns do not span \\(\\mathbb{R}^m\\), then the equation \\(A\\vec{x}=\\vec{b}\\) is inconsistent for some \\(b\\) in \\(\\mathbb{R}^m\\). ???\n\n\n\n\nDefinition- Homogeneous:\n\nA system of linear equations is said to be homogeneous if it can be written in the form \\(A\\vec{x}=\\vec{0}\\), where A is an m x n matrix and 0 is the zero vector in \\(\\mathbb{R}^m\\)\nThe equation \\(A\\vec{x}=\\vec{b}\\) is homogeneous if the zero vector is a solution\nThe homogeneous equation \\(A\\vec{x}=\\vec{0}\\) has a nontrivial solution if and only if the equation has at least one free variable. A homogeneous equation is always consistent, however, because there is always the trivial solution.\nThe homogeneous equation \\(A\\vec{x}=\\vec{0}\\) has the nontrivial solution if and only if the equation has at least one free variable.\nThe equation \\(A\\vec{x} = \\vec{0}\\) gives an implicit (not explicit) description of its solution set.\n\nDefinition- Parametric vector form\n\nParametric Vector Form is the explicit description of the plane as the set spanned by u and v. Sometimes such an equation is written as \\(x = s\\vec{u} +t\\vec{v} (s, t \\in \\mathbb{R})\\). Whenever a solution set is described explicitly with vectors, we say that the solution is in parametric vector form.\n\nThe equation \\(\\vec{x} = \\vec{p} + t\\vec{v}\\), \\(t \\in \\mathbb{R}\\) describes the solution set of \\(A\\vec{x} = \\vec{b}\\) in parametric vector form. Thus the solutions of \\(A\\vec{x} = \\vec{b}\\) are obtained by adding the vector \\(\\vec{p}\\) to the solutions of \\(A\\vec{x} = \\vec{0}\\). The vector \\(\\vec{p}\\) is just one particular solution of \\(A\\vec{x} = \\vec{b}\\). Geometrically, given \\(\\vec{v}\\) and \\(\\vec{p}\\), the effect of adding \\(\\vec{p}\\) to \\(\\vec{v}\\) is to move \\(\\vec{v}\\) in a direction parallel to the line through \\(\\vec{p}\\) and \\(\\vec{0}\\). Thus \\(\\vec{x} = \\vec{p} + t\\vec{v}\\) is the equation of the line through \\(\\vec{p}\\) parallel to \\(\\vec{v}\\).\nThe effect of adding \\(\\vec{p}\\) to a vector is to move the vector in a direction parallel to \\(\\vec{p}\\)\nThe relation between the solution sets of \\(A\\vec{x} = \\vec{b}\\) and \\(\\vec{A}\\vec{x} = \\vec{0}\\) generalizes to any consistent equation \\(\\vec{A}\\vec{x} = \\vec{b}\\), although the solution set will be larger than a line when there are several free variables.\nTheorem 6:\n\nSuppose the equation \\(A\\vec{x} = \\vec{b}\\) is consistent for some given \\(\\vec{b}\\), and let \\(\\vec{p}\\) be a solution. Then the solution set of \\(A\\vec{x} = \\vec{b}\\) is the set of all vectors of the form \\(\\vec{w} = \\vec{p} + \\vec{v_h}\\), where \\(\\vec{v_h}\\) is any solution of the homogeneous equation \\(A\\vec{x} = \\vec{0}\\). Theorem 6 says that if \\(A\\vec{x} = \\vec{b}\\) has a solution, then the solution set is obtained by translating the solution set of \\(A\\vec{x} = \\vec{0}\\), using any particular solution \\(\\vec{p}\\) of \\(A\\vec{x} = \\vec{b}\\) for the translation.\nTheorem 6 apply only to an equation \\(A\\vec{x}=\\vec{b}\\) that has at least one nonzero solution \\(\\vec{b}\\) . When \\(A\\vec{x}=\\vec{b}\\) has no solution, the solution set is empty\n\nMiscellaneous:\n\nThe solution set of \\(A\\vec{x} = \\vec{b}\\) is the set of all vectors of the form \\(\\vec{w} = \\vec{p} + \\vec{v}_h\\), where \\(\\vec{v}_h\\) is any solution of the equation \\(A\\vec{x} = \\vec{0}\\). This is only true when there exists some vector \\(\\vec{p}\\) such that \\(A\\vec{p}=\\vec{b}\\).\n\n???\n\nThe equation \\(\\vec{x} = x_2\\vec{u} + x_3\\vec{v}\\), with \\(x_2\\) and \\(x_3\\) free (and neither \\(\\vec{u}\\) nor \\(\\vec{v}\\) a multiple of the other), describes a plane through the origin. ???\n\n\n\n\nDefinition- Linear independence\n\nAn indexed set of vectors \\({\\vec{v_1}, \\vec{v_2}, . . . , \\vec{v_p}}\\) in \\(\\mathbb{R}^n\\) is said to be linearly independent if the vector equation: \\(x_1\\vec{v_1} + x_2\\vec{v_2} + \\dots + x_p\\vec{v_p} = \\vec{0}\\) has only the trivial solution. The set \\({\\vec{v_1}, \\vec{v_2}, . . . , \\vec{v_p}}\\) is said to be linearly dependent if there exist weights \\(c_1, . . . , c_p\\), not all zero, such that: \\(c_1\\vec{v_1} + c_2\\vec{v_2} + \\dots + c_p\\vec{v_p} = \\vec{0}\\)\n\\(c_1\\vec{v_1} + c_2\\vec{v_2} + \\dots + c_p\\vec{v_p} = \\vec{0}\\) is called a linear dependence relation among \\(\\vec{v_1}, \\vec{v_2}, . . . , \\vec{v_p}\\) when the weights are not all zero. An indexed set is linearly dependent if and only if it is not linearly independent.\nWe may write the matrix equation \\(A\\vec{x} = \\vec{0}\\) as: \\(x_1\\vec{a_1} + x_2\\vec{a_2} + \\dots + x_p\\vec{a_p} = \\vec{0}\\) Each linear dependence relation among the columns of \\({A}\\) corresponds to a nontrivial solution of \\(A\\vec{x} = \\vec{0}\\). The columns of matrix \\({A}\\) are linearly independent if and only if the equation \\(A\\vec{x} = \\vec{0}\\) has only the trivial solution.\nA set containing only one vector, say \\(\\vec{v}\\), is linearly independent if and only if \\(\\vec{v}\\neq\\vec{0}\\). This is because the vector equation \\(x_1\\vec{v}=\\vec{0}\\) has only the trivial solution when \\(\\vec{v}\\neq\\vec{0}\\). The zero vector is linearly dependent because \\(x_1\\vec{0}=\\vec{0}\\) has many nontrivial solutions.\n\nTheorem 7- Characterization of Linearly Dependent Sets:\n\nAn indexed set \\(S = \\{\\vec{v_1}, . . . , \\vec{v_p}\\}\\) of two or more vectors is linearly dependent if and only if at least one of the vectors in \\(S\\) is a linear combination of the others. In fact, if \\(S\\) is linearly dependent and \\(\\vec{v_1} \\neq \\vec{0}\\), then some \\(\\vec{v_j}\\) , with \\(j > 1\\), is a linear combination of the preceding vectors \\(\\vec{v_1}, . . . , \\vec{v_{j-1}}\\).\n\nNote that Theorem 7 does not say that every vector in a linearly dependent set is a linear combination of the preceding vectors. A vector in a linearly dependent set may fail to be a linear combination of the other vectors.\nTheorem 8: If a set contains more vectors than there are entries in each vector\n\nthen the set is linearly dependent. That is, any set \\({\\vec{v_1}, . . . , \\vec{v_p}}\\) in \\(\\mathbb{R}^n\\) is linearly dependent if \\(p > n\\).\n\nProof. Let \\(\\mathbf{A} = [\\vec{v_1} \\ \\dots \\ \\vec{v_p}]\\). Then \\(\\mathbf{A}\\) is \\(n \\times p\\) and the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) corresponds to a system of \\(n\\) equations in \\(p\\) unknowns. If \\(p > n\\), there are more variables than equations, so there must be a free variable. Hence \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has a nontrivial solution and the columns of \\(\\mathbf{A}\\) are linearly dependent. Note that Theorem 8 says nothing about the case in which the number of vectors in the set does not exceed the number of entries in each vector.\nTheorem 9: If a set \\(S = {\\vec{v_1}, . . . , \\vec{v_p}}\\) in \\(\\mathbb{R}^n\\) contains the zero vector\n\nthen the set is linearly dependent.\n\nProof. By renumbering the vectors, we may suppose \\(\\vec{v_1} = \\vec{0}\\). Then the equation \\(1\\vec{v_1} + 0\\vec{v_2} + \\dots + 0\\vec{v_p} = \\vec{0}\\) shows that \\(S\\) is linearly dependent.\nFor linear independence:\n\nIf \\(\\vec{v_1}, \\dots, \\vec{v_4}\\) are in \\(\\mathbb{R}^4\\) and \\(\\vec{v_3} = 2\\vec{v_1} + \\vec{v_2}\\), then \\({\\vec{v_1}, \\vec{v_2}, \\vec{v_3}, \\vec{v_4}}\\) is linearly dependent.\nIf \\(\\vec{v_1}, \\dots, \\vec{v_4}\\) are in \\(\\mathbb{R}^4\\) and \\(\\vec{v_3} = \\vec{0}\\), then \\({\\vec{v_1}, \\vec{v_2}, \\vec{v_3}, \\vec{v_4}}\\) is linearly dependent.\nIf \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\) are in \\(\\mathbb{R}^4\\) and \\(\\vec{v_2}\\) is not a scalar multiple of \\(\\vec{v_1}\\), then \\({\\vec{v_1}, \\vec{v_2}}\\) is linearly independent. FALSE(\\(\\vec{v_1}\\) could be the zero vector)\nIf \\(\\vec{v_1}, \\dots, \\vec{v_4}\\) are in \\(\\mathbb{R}^4\\) and \\(\\vec{v_3}\\) is not a linear combination of \\(\\vec{v_1}, \\vec{v_2}, \\vec{v_4}\\), then \\({\\vec{v_1}, \\vec{v_2}, \\vec{v_3}, \\vec{v_4}}\\) is linearly independent. FALSE(since \\(\\vec{v_1}=[1,1,1,1]; \\vec{v_2}=[2,2,2,2]; \\vec{v_3}=[1,2,3,4]; \\vec{v_4}=[3,3,3,3]\\))\nIf \\(\\vec{v_1}, \\dots, \\vec{v_4}\\) are in \\(\\mathbb{R}^4\\) and \\({\\vec{v_1}, \\vec{v_2}, \\vec{v_3}}\\) is linearly dependent, then \\(\\{\\vec{v_1}, \\dots, \\vec{v_4}\\}\\) is also linearly independent TRUE\nIf \\(\\vec{v_1}, \\dots, \\vec{v_4}\\) are linearly independent vectors in \\(\\mathbb{R}^4\\), then \\(\\{{\\vec{v_1}, \\vec{v_2}, \\vec{v_3}}\\}\\) is also linearly independent. [Hint: Think about \\(x_1\\vec{v_1} + x_2\\vec{v_2} + x_3\\vec{v_3} + 0\\vec{v_4} = \\vec{0}\\).] TRUE\n\n\n\n\nDefinition- Transformation:\n\nA transformation (or function or mapping) \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) is a rule that assigns to each vector \\(\\vec{x}\\) in \\(\\mathbb{R}^n\\) a vector \\(T(\\vec{x})\\) in \\(\\mathbb{R}^m\\). The set \\(\\mathbb{R}^n\\) is called the domain of \\(T\\), and \\(\\mathbb{R}^m\\) is called the codomain of \\(T\\). The notation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) indicates that the domain of \\(T\\) is \\(\\mathbb{R}^n\\) and the codomain is \\(\\mathbb{R}^m\\). For \\(\\vec{x}\\) in \\(\\mathbb{R}^n\\), the vector \\(T(\\vec{x})\\) in \\(\\mathbb{R}^m\\) is called the image of \\(\\vec{x}\\) (under the action of \\(T\\)). The set of all images \\(T(\\vec{x})\\) is called the range of \\(T\\). For each \\(\\vec{x}\\) in \\(\\mathbb{R}^n\\), \\(T(\\vec{x})\\) is computed as \\(\\mathbf{A}\\vec{x}\\), where \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix. For simplicity, we denote such a matrix transformation by \\(\\vec{x} \\rightarrow \\mathbf{A}\\vec{x}\\). Observe that the domain of \\(T\\) is \\(\\mathbb{R}\\)\n\nDefinition- Linear transformation:\nA transformation (or mapping) \\(T\\) is linear if:\n\n\\(T(\\vec{u} + \\vec{v}) = T(\\vec{u}) + T(\\vec{v})\\)\n\\(T(c\\vec{v}) = cT(\\vec{v})\\)\n\nfor all \\(\\vec{v}, \\vec{c}\\) in the domain of \\(T\\) and for all scalars \\(c\\).\n\nA linear transformation is a special type of function.\nEvery matrix transformation is a linear transformation. BUT not every linear transformation is a matrix transformation\nA linear transformation preserves the operations of vector addition and scalar multiplication.\nThe superposition principle is a physical description of a linear transformation.\n\nMiscellaneous:\n\nIf \\(\\mathbf{A}\\) is a \\(3 \\times 5\\) matrix and \\(T\\) is a transformation defined by \\(T(\\vec{x}) = \\mathbf{A}\\vec{x}\\), then the domain of \\(T\\) is \\(\\mathbb{R}^3\\). FALSE(The domain is \\(\\mathbb{R}^5\\) since domain is based on column not row.)\nIf \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, then the range of the transformation \\(\\vec{x} \\rightarrow \\mathbf{A}\\vec{x}\\) is \\(\\mathbb{R}^m\\). FALSE(The range is the set of all linear combinations of the columns of A)\nA transformation \\(T\\) is linear if and only if \\(T(c_1\\vec{v_1} + c_2\\vec{v_2}) = c_1T(\\vec{v_1}) + c_2T(\\vec{v_2})\\) for all \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\) in the domain of \\(T\\) and for all scalars \\(c_1\\) and \\(c_2\\). TRUE(Since it’s linear, the factor can be factor out to form a linear equation)\nThe codomain of the transformation \\(\\vec{x} \\rightarrow A\\vec{x}\\) is the set of all linear combinations of the columns of \\(A\\). FALSE(If \\(A\\) is an m x n matrix, the codomain is \\(\\mathbb{R}^m\\))\nIf \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) is a linear transformation and if \\(\\vec{c}\\) is in \\(\\mathbb{R}^m\\), then a uniqueness question is “Is \\(\\vec{c}\\) in the range of \\(T\\)?” FALSE(existence question)"
  },
  {
    "objectID": "posts/usa_christian/readme.html",
    "href": "posts/usa_christian/readme.html",
    "title": "Aj's Blog",
    "section": "",
    "text": "Overview\nThis Quarto Markdown file uses R to analyze the GSS to find insights on Christianity in the United States.\nSoftware Demo Video\n\n\nDevelopment Environment\nUsed Python 3.10.7, Quarto Markdown, and R version 4.2.1.\n\n\nUseful Websites\n\nDplyr Documentation\nGgplot2 Documentation\nGSS Documentation\n\n\n\nFuture Work\n\nMight need to check out more data from the GSS"
  },
  {
    "objectID": "posts/hxh/power_triangle.html",
    "href": "posts/hxh/power_triangle.html",
    "title": "Dynamic Visualization using Anime",
    "section": "",
    "text": "Hunter X Hunter\nHunter x Hunter is a popular TV show from Japan. On MyAnimeList, this is the second most favorited anime with roughly 200,000 people.\nThis show follows the adventures of a young boy named who sets out to become a skilled and licensed professional who is tasked with hunting down treasure, rare creatures, called “hunters”. The anime has become a fan favorite for its well-developed characters, intricate plotlines, and stunning visuals.\nIn this blog post, I will introduce a web app that allows users to search through all the subtitles of Hunter x Hunter and see how many times a certain term appears in each episode. This tool can be useful for fans of the show who want to study the dialogue and the development of the storylines, as well as for researchers who are interested in analyzing the language used in the anime.\nCheck out the website here.\n\n\nProcess\nThe first step in building this web app is to gather the data. I used web scraping techniques to obtain the list of episodes from Wikipedia and then converted it into a Pandas dataframe. I then downloaded a set of CSV files that contain the subtitles for each episode, and I looped through them to add them to the dataframe.\nOnce I had the data, I built a Streamlit app that allows users to input a term and get a count of how many times that term appears in each episode. I used string expressions and Pandas methods to search through the subtitles and extract the relevant information.\nFinally, I used Plotly to create a dynamic graph that shows the frequency of the searched term throughout the series. This plot can be updated in real-time as the user changes the search term, allowing for a flexible and interactive experience.\n\n\nSome Results\nHere, we have the results from the term Nen.\n\nResults from the term Kurapika"
  },
  {
    "objectID": "posts/rosetta/index.html",
    "href": "posts/rosetta/index.html",
    "title": "Rosetta Stone for Data Science",
    "section": "",
    "text": "A zoo of tools\nThere are countless data science languages and tools for data processing and analysis. Since the various tools used by data analysts each have their own unique syntax and approach to data manipulation, it may seem challenging for users to switch between them. To address this challenge, I made a “Rosetta Stone” to provide users with the equivalent functions and phrases for analysis, allowing users to easily compare and switch between them. Since functionality (by design) is not supposed to be the same between these libraries and languages, the primary goal of this tool is just to allow people to get a better grasp on syntax.\nIncluded in the Rosetta Stone:\n\nPandas: Pandas is a Python library that provides data structures for efficient data manipulation and analysis. It is widely used for data cleaning, exploration, and visualization.\nTidyverse: The tidyverse is a collection of R packages designed for data science, which includes tools for data wrangling, visualization, and modeling.\nPolars: Polars is a newer library designed for fast and efficient data manipulation and analysis\nSQL: SQL is most different from the other libraries on this list. While it works with tables of data, the setting is designed for a lower level approach.\n\nPlay with the tool here:"
  },
  {
    "objectID": "posts/conspiracy/conspiracy.html",
    "href": "posts/conspiracy/conspiracy.html",
    "title": "Querying conspiracies with Reddit",
    "section": "",
    "text": "That’s just a theory…\nAre you tired of scrolling through endless conspiracy theories on Reddit without any sense of direction or purpose? Do you want to know what topics are actually trending over time? Look no further, because this Streamlit app has got you covered! With just a few clicks, you can dive into the world of conspiracies and gain valuable insights into the Reddit community’s most popular theories. So put on your tin foil hat and join us on this exciting adventure!\nThis Streamlit page is a data analysis tool that allows users to explore and visualize public forum data related to conspiracy theories. The tool loads in a dataset of Reddit posts that mention a specific query, which can be selected based on different genres such as governmental, corporate, scientific, and other.\n\n\nHow to Use the App\n\nSelect the topic you would like to see trends on. Select the query from the selected topic that you would like to see trends on.\nSelect the subreddits you want to include using the checkboxes for r/Politics, r/PoliticalDiscussion, r/conspiracy, and r/Freethought.\nInput a secondary query to see how often it appears in the posts related to the initial query.\nAdjust the chart’s width to your liking. Hover over the chart to see the number of posts for each month.\nUse the legend to toggle between mentions of the initial query and mentions of both the initial and secondary queries.\nPlay around with the various graph settings, including downloading the graph as a PNG, zooming in and out, panning across the graph, autoscaling the graph, resetting the axes, and viewing the graph in fullscreen mode.\n\nWhat is Reddit?\nReddit is a social media site with various forums called subreddits. Our site used PRAW to collect data from four subreddits specifically: Conspiracy, Politics, PoliticalDiscussion, and Freethought. To collect this data, we use Reddit’s builtin API. The information is up to date to February 2023. While this tool may not query every post on reddit with their respective search term, this tool is for general trends.\nCheck out the website here."
  },
  {
    "objectID": "posts/hxh/hxh.html",
    "href": "posts/hxh/hxh.html",
    "title": "Dynamic Visualization using Anime",
    "section": "",
    "text": "Hunter X Hunter\nHunter x Hunter is a popular TV show from Japan. On MyAnimeList, this is the second most favorited anime with roughly 200,000 people.\nThis show follows the adventures of a young boy named who sets out to become a skilled and licensed professional who is tasked with hunting down treasure, rare creatures, called “hunters”. The anime has become a fan favorite for its well-developed characters, intricate plotlines, and stunning visuals.\nIn this blog post, I will introduce a web app that allows users to search through all the subtitles of Hunter x Hunter and see how many times a certain term appears in each episode. This tool can be useful for fans of the show who want to study the dialogue and the development of the storylines, as well as for researchers who are interested in analyzing the language used in the anime.\nCheck out the website here.\n\n\nProcess\nThe first step in building this web app is to gather the data. I used web scraping techniques to obtain the list of episodes from Wikipedia and then converted it into a Pandas dataframe. I then downloaded a set of CSV files that contain the subtitles for each episode, and I looped through them to add them to the dataframe.\nOnce I had the data, I built a Streamlit app that allows users to input a term and get a count of how many times that term appears in each episode. I used string expressions and Pandas methods to search through the subtitles and extract the relevant information.\nFinally, I used Plotly to create a dynamic graph that shows the frequency of the searched term throughout the series. This plot can be updated in real-time as the user changes the search term, allowing for a flexible and interactive experience.\n\n\nSome Results\nHere, we have the results from the term Nen.\n\nResults from the term Kurapika"
  },
  {
    "objectID": "posts/pca/pca.html",
    "href": "posts/pca/pca.html",
    "title": "A county like mine",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a statistical technique used to simplify complex data by reducing the number of variables while retaining most of the original information. In other words, it helps to identify the most important patterns or relationships in large datasets by transforming them into a set of linearly uncorrelated variables, known as principal components.\nPCA works by identifying the underlying structure of the data and extracting the directions of maximum variance, which are the principal components. Each principal component is a linear combination of the original variables, and they are orthogonal to each other, meaning that they are uncorrelated. The first principal component captures the most significant variation in the data, and subsequent components capture decreasing amounts of variation. By selecting the appropriate number of principal components, we can reduce the dimensionality of the dataset while retaining the essential information.\nIn this data science project, I used Principal Component Analysis (PCA) to visualize and explore the similarity of various counties in the United States based on a set of demographic metrics including th following:\nThe primary utility of using PCA in this context is to reduce the dimensionality of the dataset while retaining as much information as possible. By doing so, we can efficiently examine patterns and relationships among counties in a lower-dimensional space.\nFor each of these data, I applied PCA to transform the original high-dimensional data into a lower-dimensional space that still captures most of the variation present in the original dataset.\nBy plotting the first two or three principal components on a 2D scatter plot, we can visualize the relationships among counties based on their transformed coordinates. This allows us to identify clusters of similar counties, outliers, or any other interesting patterns. In the code, I also highlighted a specific county (represented by highlighted_area), making it easier to identify its position relative to other counties in the reduced-dimensional space.\nAdditionally, I created a bar chart displaying the proportion of variance explained by each principal component, which helps to determine the optimal number of components to retain for further analysis or modeling.\nThe use of PCA in this project allows us to effectively summarize the complex, high-dimensional relationships between counties, making it easier to identify patterns and interpret the data."
  },
  {
    "objectID": "posts/pca/Untitled-1.html",
    "href": "posts/pca/Untitled-1.html",
    "title": "Untitled",
    "section": "",
    "text": "import plotly.express as px\nimport pandas as pd\nimport ipywidgets as widgets\nfrom IPython.display import display\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 4, 5],\n    'y': [10, 20, 30, 40, 50]\n})\n \nnumber_input = widgets.IntText(\n    value=1,\n    description='Enter a number:',\n    disabled=False\n)\n\ndef update_plot(number):\n    # Generate a new dataset based on the user input\n    filtered_df = df[df['x'] <= number]\n    \n    # Generate a plot using Plotly\n    fig = px.line(filtered_df, x='x', y='y')\n    \n    # Display the plot\n    fig.show()\n\nwidgets.interact(update_plot, number=number_input)"
  },
  {
    "objectID": "posts/pca/pca.html#pc1",
    "href": "posts/pca/pca.html#pc1",
    "title": "A county like mine",
    "section": "PC1",
    "text": "PC1\nThe first principal component seems to be some measure of traditional metrics of the society including how White the population is, both religious and nonreligious organizations, low criminality, and percent married.\n\n\nCode\nPC1\n\n\nwhite                                      0.184024\nnon_religious_and_religious_orgs_per_1k    0.183871\ncrime_rate_per_100k                       -0.180371\npct_women_married                          0.179891\nnon_religious_non_profit_orgs_per_1k       0.176724\nName: PC1, dtype: float64"
  },
  {
    "objectID": "posts/pca/pca.html#pc2",
    "href": "posts/pca/pca.html#pc2",
    "title": "A county like mine",
    "section": "PC2",
    "text": "PC2\nThe second principal component seems to be some measure of how economically disadvantaged the population is, with measures like poverty rate highly correlated and median household income negatively correlated. Interestingly, religious congregations and teen births per 1k is also highly correlated with this principal component. It seems like this principal component has some aspects of traditionality of the society, but the negative aspects of it.\n\n\nCode\nPC2\n\n\npct_pov017_2020                   0.263468\npct_povall_2020                   0.248170\nmedhhinc_2020                    -0.244717\nreligious_congregations_per_1k    0.225318\npct_bachelors_plus_2017_21       -0.219958\nName: PC2, dtype: float64"
  },
  {
    "objectID": "posts/pca/pca.html#pc3",
    "href": "posts/pca/pca.html#pc3",
    "title": "A county like mine",
    "section": "PC3",
    "text": "PC3\nThe third principal component seems to be some measure of how busy the area is. It is highly correlated with the percent of the population that is employed in the management, production, and construction industry.\n\n\nCode\nPC3\n\n\nciv_labor_force_2021    0.292500\nemployed_2021           0.286014\nmgmt                    0.273708\nproduction              0.263062\nconstruction            0.256393\nName: PC3, dtype: float64"
  },
  {
    "objectID": "posts/pca/pca.html#pc4",
    "href": "posts/pca/pca.html#pc4",
    "title": "A county like mine",
    "section": "PC4",
    "text": "PC4\nThe fourth principal component seems to be some negative measure of civic engagement and social capital.\n\n\nCode\nPC4\n\n\nnon_religious_non_profit_orgs_per_1k      -0.296876\nnon_religious_and_religious_orgs_per_1k   -0.272233\nrep16_frac                                 0.237853\nrecreation_leisure_est_per_1k             -0.234750\nmembership_orgs_per_1k                    -0.227421\nName: PC4, dtype: float64"
  },
  {
    "objectID": "posts/pca/pca.html#pc5",
    "href": "posts/pca/pca.html#pc5",
    "title": "A county like mine",
    "section": "PC5",
    "text": "PC5\nNot exactly sure what this principal component is measuring.\n\n\nCode\nPC5\n\n\nhispanic                                                       -0.302891\ntwo_or_more                                                    -0.263537\nother_race                                                     -0.250530\ncharitable_contributions_share_of_agi_middle_class_itemizers    0.214137\nassociations_per_1k_penn_state_method                           0.207575\nName: PC5, dtype: float64"
  },
  {
    "objectID": "posts/pca/pca.html#pc6",
    "href": "posts/pca/pca.html#pc6",
    "title": "A county like mine",
    "section": "PC6",
    "text": "PC6\nProbably measures a variable related to something with race and crime\n\n\nCode\nPC6\n\n\ncrime_rate_per_100k    0.253964\nhispanic               0.253551\nag_assault             0.252535\nrape                   0.248669\nblack                 -0.237223\nName: PC6, dtype: float64"
  },
  {
    "objectID": "posts/linalg/manim1.html",
    "href": "posts/linalg/manim1.html",
    "title": "manim",
    "section": "",
    "text": "Manimce test\n\nfrom manim import *\n\nconfig.media_width = \"75%\"\nconfig.verbosity = \"WARNING\"\n\nManim Community v0.17.2\n\n\n\n\n\n%%manim -qm CircleToSquare\n\nclass CircleToSquare(Scene):\n    def construct(self):\n        blue_circle = Circle(color=BLUE, fill_opacity=0.5)\n        green_square = Square(color=GREEN, fill_opacity=0.8)\n        self.play(Create(blue_circle))\n        self.wait()\n        \n        self.play(Transform(blue_circle, green_square))\n        self.wait()\n\nAnimation 0: Create(Circle):   0%|          | 0/30 [00:00<?, ?it/s]\n\n\nAnimation 0: Create(Circle):   3%|3         | 1/30 [00:00<00:23,  1.23it/s]\n\n\nAnimation 0: Create(Circle):  80%|########  | 24/30 [00:00<00:00, 35.22it/s]\n\n\n                                                                            \n\n\n\n\n\nAnimation 2: Transform(Circle):   0%|          | 0/30 [00:00<?, ?it/s]\n\n\nAnimation 2: Transform(Circle):   3%|3         | 1/30 [00:00<00:14,  2.02it/s]\n\n\nAnimation 2: Transform(Circle):  47%|####6     | 14/30 [00:00<00:00, 30.34it/s]\n\n\n                                                                               \n\n\n\n\n\n\n      Your browser does not support the video element."
  },
  {
    "objectID": "posts/linalg/tempw1.html",
    "href": "posts/linalg/tempw1.html",
    "title": "Aj's Blog",
    "section": "",
    "text": "from manim import *\n\nconfig.media_width = \"75%\"\nconfig.verbosity = \"WARNING\"\n\nManim Community v0.17.2\n\n\n\n\n\n%%manim -qm CircleToSquare\n\nclass CircleToSquare(Scene):\n    def construct(self):\n        blue_circle = Circle(color=BLUE, fill_opacity=0.5)\n        green_square = Square(color=GREEN, fill_opacity=0.8)\n        self.play(Create(blue_circle))\n        self.wait()\n        \n        self.play(Transform(blue_circle, green_square))\n        self.wait()\n\n                                                                                \n\n\n\n      Your browser does not support the video element."
  },
  {
    "objectID": "posts/linalg/eigenvals.html",
    "href": "posts/linalg/eigenvals.html",
    "title": "Untitled",
    "section": "",
    "text": "Introduction to Eigenvalues\nThe eigenvalue-eigenvector equation is the equation \\(A\\mathbf{x} = \\lambda \\mathbf{x}\\). Any non-zero vector that satisfies the equation for scalar \\(\\lambda\\) is an eigenvalue \\(\\lambda\\)."
  },
  {
    "objectID": "posts/lin_reg2/index.html",
    "href": "posts/lin_reg2/index.html",
    "title": "Linear Regression Guide",
    "section": "",
    "text": "Simple linear regression means that there is only a single variable (not multiple \\(X\\)’s)\n\n\\(X\\) - A single quantitative explanatory variable (independent)\n\\(Y\\) - A single quantitative response variable (dependent)\n\nThe True Regression Line\n\\[\nE\\{Y\\} = \\beta_0 + \\beta_1 X_i\n\\]\n\n\\(E\\{Y\\}\\) - True mean y-value, also \\(\\mu_{Y|X}\\) or \\(E\\{Y|X\\}\\)\n\\(\\beta_0\\) - True \\(y\\)-intercept\n\\(\\beta_1\\) - True slope\n\nThe Observed Points\n\\[\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i = b_0 + b_1X_i + r_i\n\\]\n\n\\(Y_i\\) - Response or dependent variable for the \\(i^{\\text{th}}\\) observation.\n\\(\\epsilon_i\\) - Error, distance of dot to true line. \\(\\epsilon_i = Y_i - E\\{Y\\}\\)\n\\(r_i\\) - Residual, distance of dot to estimated line. \\(r_i = Y_i - \\hat{Y}_i\\)\n\nThe Estimated Regression Line obtained from a regression analysis from the observed points\n\\[\n\\hat{Y}_i = b_0 + b_1 X_i\n\\]\n\n\\(\\hat{Y}_i\\) - The fitted line\n\\(b_0\\) - Estimated \\(y\\)-intercept, also \\(\\hat{\\beta}_0\\)\n\\(b_1\\) - Estimated slope, also \\(\\hat{\\beta}_1\\)\n\n\n\n\n\n\n\nR code for calling your data\n\n\n\n\n\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", \n       y=\"Miles per Gallon\") +\n  theme_classic()  \n\n\n\n\n\n\n\n\nY <- mtcars$mpg \nX <- mtcars$wt\n\ni <- 3 #random number\n\n\nX[i]\n\n[1] 2.32\n\n\n\nY[i]\n\n[1] 22.8\n\n\n\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", y=\"Miles per Gallon\") +\n  theme_classic() +\n  geom_vline(aes(xintercept = wt[i] ), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = mpg[i]), color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nThe estimated regression line or line of best fit from the observed points is represented by the equation \\(\\hat{Y}_i=b_0+b_1X_i\\) , where \\(b_1\\) is the estimated slope of the line and \\(b_0\\) is the estimated y-intercept.\n\n\nA single residual, \\(r_i\\), is the distance between an observed point and the estimated regression line. Given any line, we can calculate how different our predicted value, \\(\\hat{Y_i}\\) is from our actual data point, \\(Y_i\\). This vertical distance from the real y output and the predicted y output is called the residual. The formula for any given residual is represented as such\n\\[\n\\begin{align*}\nr_i &= Y_i-\\hat{Y}_i\\\\\nr_i &= Y_i - (b_0+b_1X_i)\n\\end{align*}\n\\]\nThe goal of simple linear regression is to minimize the sum of the squared errors (\\(\\text{SSE}\\)). This is represented by the equations:\n\\[\n\\begin{align*}\n\\text{SSE} &= \\sum_{i=1}^n \\left(r_i\\right)^2 \\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2\\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\end{align*}\n\\]\nThe closer any given line is to the best fit line, the lower the \\(\\text{SSE}\\) will be. In fact, the best fit line represents the line with the lowest possible \\(\\text{SSE}\\). This line is found by manipulating the values for \\(b_0\\) and \\(b_1\\) since the \\(X\\) values must stay the same.\n\n\n\n\nThere are multiple ways to calculate the estimated regression line like with calculus or linear algebra.\n\n\n\n\n\n\nThe Calculus Behind Best Fit\n\n\n\n\n\n\nThere exists a combination of \\(b_0\\) and \\(b_1\\) that is the lowest. Geometrically, we can imagine a shape in three dimensions where two inputs (\\(b_0\\) and \\(b_1\\)) produce an output (\\(\\text{SSE}\\)). Roughly speaking, where the three dimensional slope is equal to 0 is where the combination of \\(b_0\\) and \\(b_1\\) will produce the lowest possible \\(\\text{SSE}\\). Since the derivative of an equation can tell us the slope at any given point, we can set both equations’ derivatives to zero to find where slope is zero for both equations.\nStep One: Take the partial derivatives with respect to \\(b_0\\) and \\(b_1\\)\nThe partial derivative with respect to \\(b_0\\) is calculated as so:\n\\[\\frac{\\partial}{\\partial b_0}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\\] \\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-1) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nThe partial derivative with respect to \\(b_1\\) is calculated as so:\n\\[\\frac{\\partial}{\\partial b_1}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\\] \\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-X_i) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nStep Two: Set the simplified partial derivatives to zero and solve for \\(b_0\\) and \\(b_1\\) respectively\nSolving for \\(b_0\\): \\[0 = -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^n\\left(Y_i - b_0-b_1x_i\\right) &\\text{Simplify}\\\\\n&0 = \\sum_{i=1}^nY_i - \\sum_{i=1}^nb_0-\\sum_{i=1}^nb_1X_i &\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nY_i - nb_0-b_1\\sum_{i=1}^nX_i &\\text{Constant Multiple Rule for Sums}\\\\\n& nb_0 = \\sum_{i=1}^nY_i -b_1\\sum_{i=1}^nX_i &\\text{Simplify}\\\\\n& b_0 = \\frac{\\sum_{i=1}^nY_i}{n} -\\frac{b_1\\sum_{i=1}^nX_i}{n} &\\text{Simplify}\\\\\n& b_0 = \\bar{Y} - b_1\\bar{X} &\\text{Simplify to average}\\\\\n\\end{align*}\n\\]\nSolving for \\(b_1\\) while substituting \\(b_0\\):\n\\[\n0 = -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right)\n\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^nX_i\\left(Y_i - ((\\bar{Y} - b_1\\bar{X})+b_1X_i)\\right) &\\text{Substitute }b_0\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - \\sum_{i=1}^nb_1X_i(X_i-\\bar{X})&\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - b_1\\sum_{i=1}^nX_i(X_i-\\bar{X})&\\text{Constant Multiple Rule for Sums}\\\\\n& b_1= \\frac{\\sum_{i=1}^nX_i(Y_i-\\bar{Y})}{\\sum_{i=1}^nX_i(X_i-\\bar{X})} &\\text{Simplify}\\\\\n& b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2} &\\text{Simplify (steps skipped)}\\\\\n\\end{align*}\n\\]\nStep Three: Construct the formula\nOnce you have found \\(b_0\\) and \\(b_1\\) as so:\n\n\\(b_0 = \\bar{Y} - b_1\\bar{X}\\)\n\\(b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}\\)\n\n(\\(\\bar{Y}\\) represents the mean of the \\(Y\\) values and \\(\\bar{X}\\) represents the mean of the \\(X\\) values)\nWe can construct the formula for \\(\\hat{Y}_i\\) as the following:\n\\[\n\\hat{Y}_i = b_0 + b_1X_i\n\\]\nWhich means \\(b_0\\) is the change in the mean of \\(Y\\) for a \\(1\\) unit increase in \\(X\\).\n\n\n\n\n\n\n\n\n\nLeast Squares in Linear Algebra\n\n\n\n\n\n\nStep One: Organize the data into vectors Let’s assume you have \\(n\\) observations of the independent variable \\(\\mathbf{X}\\) and dependent variable \\(\\mathbf{Y}\\). Organize the data into two vectors:\n\\[\n\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\quad \\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}\n\\]\nStep Two: Create a design matrix\nCreate a design matrix \\(\\mathbf{A}\\) by adding a column of ones to represent the intercept term\n\\[\n\\mathbf{A} = \\begin{bmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{bmatrix}\n\\]\nStep Three: Make a Projection:\nThe goal of simple linear regression is to find the best-fitting line, which can be viewed as projecting the dependent variable \\(\\mathbf{Y}\\) onto the column space of the design matrix \\(\\mathbf{A}\\). This projection, denoted by \\(\\mathbf{\\hat{Y}}\\), can be computed as:\n\\[\n\\mathbf{\\hat{Y}} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{Y}\n\\]\nThis formula comes from the orthogonal projection of \\(\\mathbf{Y}\\) onto the column space of \\(\\mathbf{A}\\), which minimizes the squared residuals.\nStep Four: Compute the coefficients:\nThe coefficients of the best-fitting line can be found by solving the following linear system:\n\\[\n\\mathbf{A}^T\\mathbf{A}\\mathbf{\\beta} = \\mathbf{A}^T\\mathbf{Y}\n\\]\nHere, \\(\\mathbf{\\beta}\\) is a vector containing the coefficients \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n\\mathbf{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\n\\]\nTo solve for \\(\\mathbf{\\beta}\\), you can use the following formula:\n\\[\n\\mathbf{\\beta} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{Y}\n\\]\nStep Five: Construct the formula:\nOnce you have computed the coefficients \\(\\beta_0\\) and \\(\\beta_1\\), you can write the estimated regression line as:\n\\[\n\\hat{Y}_i = \\beta_0 + \\beta_1 X_i\n\\]\nThe estimated slope, \\(\\beta_1\\), indicates the average change in the dependent variable (\\(Y\\)) for a one-unit increase in the independent variable (\\(X\\)). The estimated intercept, \\(\\beta_0\\), represents the predicted value of \\(Y\\) when \\(X = 0\\).\n\n\n\nThe formula from calculus to find the coefficients for the estimated regression line is as so:\n\n\\(b_0 = \\bar{Y} - b_1\\bar{X}\\)\n\\(b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}\\)\n\n(\\(\\bar{Y}\\) represents the mean of the \\(Y\\) values and \\(\\bar{X}\\) represents the mean of the \\(X\\) values)\n\n\n\n\n\n\nR code for calculating the coefficients for the estimated regression line\n\n\n\n\n\n\nmylm <- lm(mpg ~ wt, data = mtcars)\npander::pander(summary(mylm))\n\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n37.29\n1.878\n19.86\n8.242e-19\n\n\nwt\n-5.344\n0.5591\n-9.559\n1.294e-10\n\n\n\n\nFitting linear model: mpg ~ wt\n\n\n\n\n\n\n\n\nObservations\nResidual Std. Error\n\\(R^2\\)\nAdjusted \\(R^2\\)\n\n\n\n\n32\n3.046\n0.7528\n0.7446\n\n\n\n\n\n\nb_0 <- mylm$coefficients[1]\nb_1 <- mylm$coefficients[2]\n\n\nb_0\n\n(Intercept) \n   37.28513 \n\n\n\nb_1\n\n       wt \n-5.344472 \n\n\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\ngeom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\nstat_function(fun = function(x) b_0 + b_1 * x) + \nlabs(title=\"Miles per Gallon vs. Weight of Car\", \n     x=\"Weight of Car\", \n     y=\"Miles per Gallon\") +\ntheme_classic() \n\n\n\n\n\n\n\n\n\n\nIf we are to make a model off the points displayed as such:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i\n\\]\nWe need to confirm the following assumptions\n\n1) Linear relation: Linear relationship between \\(X\\) and \\(Y\\)\n2) Normal errors: \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\), the error terms are normally distributed with a mean of 0\n3) Constant variance: The variance \\(σ^2\\) of the error terms is constant (the same) across all values of \\(X_i\\)\n4) Fixed \\(X\\): the \\(X\\) values can be considered fixed and measured without error\n5) Independent errors: \\(\\epsilon\\) is independent\n\n\\(Y_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i :\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\)\nGiven \\(X\\), \\(Y \\sim \\mathcal{N}(\\beta_0 +\\beta_1X,\\sigma^2)\\)\n\n\n\n\n\n\nR code for testing the assumptions for simple linear regression\n\n\n\n\n\nThree plots in R can be used to test the assumptions for simple linear regression:\nResiduals versus Fitted-values Plot The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(Y^i\\)\n\nThe residuals are the \\(r_i\\)\nThis plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable.\n\nQ-Q Plot of the Residuals\nThe normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption.\nResiduals versus Order Plot\nWhen the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated.\n\npar(mfrow=c(1,3)) #Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3\n\nplot(mylm,which=1:2) #Q-Q Plot of the Residuals: Checks Assumption #2\n\nplot(mylm$residuals) #Residuals versus Order Plot: Checks Assumption #5\n\n\n\n\n\n\n\n\n\n\n\n\nSum of Squared Errors\nMeasures how much the residuals deviate from the line.\n\\(\\text{SSE} = \\sum_{i=1}^n \\left(r_i\\right)^2 = \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2\\)\nSum of Squares Regression\nMeasures how much the regression line deviates from the average y-value.\n\\(\\text{SSR} = \\sum_{i=1}^n \\left(r_i\\right)^2 = \\sum_{i=1}^n\\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\)\nTotal Sum of Squares\nMeasures how much the y-values deviate from the average y-value.\n\\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\)\n\n\\(\\text{SSTO}=\\text{SSE}+\\text{SSR}\\)\n\n\n\n\n\n\nR code for calculating the above terms\n\n\n\n\n\n\nresiduals <- mylm$residuals\nfitted_values <- mylm$fitted.values\nmean_mpg <- mean(mtcars$mpg)\n\nSSE <- sum(residuals^2)\nSSR <- sum((fitted_values - mean_mpg)^2)\nTSS <- sum((mtcars$mpg - mean_mpg)^2)\n\npander::pander(cat(\"Sum of Squared Errors (SSE):\", SSE, \"\\n\"))\n\nSum of Squared Errors (SSE): 278.3219\n\npander::pander(cat(\"Sum of Squares Regression (SSR):\", SSR, \"\\n\"))\n\nSum of Squares Regression (SSR): 847.7252\n\npander::pander(cat(\"Total Sum of Squares (TSS):\", TSS, \"\\n\"))\n\nTotal Sum of Squares (TSS): 1126.047\n\n\n\n\n\n\n\n\n\nUsing the terms above, we can calculate a ratio of the variance of \\(Y\\) explained by the estimated regression line (\\(\\text{SSE}\\)) and the total variance of \\(Y\\) from the average \\(Y\\) value (\\(\\text{SSTO}\\).\n\\[\nR^2 = 1-\\frac{\\text{SSE}}{\\text{SSTO}}\n\\]\n\n\n\n\n\n\nR code for calculating the coefficient of determination\n\n\n\n\n\n\nlm_summary <- summary(mylm)\n\npander::pander(lm_summary$r.squared)\n\n0.7528"
  },
  {
    "objectID": "posts/lin_reg2/index.html#the-data",
    "href": "posts/lin_reg2/index.html#the-data",
    "title": "Linear Regression Guide",
    "section": "The Data",
    "text": "The Data\nYou have a dataset where each row represents an individual sample and each column represents a variable about each sample.\n\n\\(n\\) refers to the total number of samples you have\n\\(\\vec{x}\\) refers to the independent variable column in your data\n\\(\\vec{y}\\) refers to the dependent variable column in your data\nWe will use the notation \\(x_i\\) and \\(y_i\\) to denote the\\(i\\)th observations of \\(x\\) and \\(y\\), respectively.This would mean, for example: \\(x_1\\) would refer to the first x value in your data while \\(y_n\\) would refer to the last y value in your data"
  },
  {
    "objectID": "posts/lin_reg2/index.html#the-line",
    "href": "posts/lin_reg2/index.html#the-line",
    "title": "Linear Regression Guide",
    "section": "The Line",
    "text": "The Line\nThe goal of simple linear regression is to find the line of best fit, which is represented by the equation \\(f(x)=\\beta_0+\\beta_1x\\) , where \\(\\beta_1\\) is the slope of the line and \\(\\beta_0\\) is the y-intercept. The function \\(f(x)\\) will find any \\(\\hat{y_i}\\) which is the predicted y value of \\(x_i\\) with a best fit line."
  },
  {
    "objectID": "posts/lin_reg2/index.html#minimizing-residuals",
    "href": "posts/lin_reg2/index.html#minimizing-residuals",
    "title": "Linear Regression Guide",
    "section": "Minimizing Residuals",
    "text": "Minimizing Residuals\nGiven any line, we can calculate how different our predicted value, \\(\\hat{y_i}\\) is from our actual data point, \\(y_i\\). This vertical distance from the real y output and the predicted y output is called the residual. The residual for a given point is calculated as \\(e_i = y_i - \\hat{y_i}\\).\nThe goal of simple linear regression is to minimize the sum of the squared residuals, also known as the residual sum of squares (\\(RSS\\)). This is represented by the equations:\n\\[\\sum_{i=1}^n \\left(e_i\\right)^2 \\text{ or } \\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2 \\text{ or } \\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2\\].\nThe closer any given line is to the best fit line, the lower the \\(RSS\\) will be. In fact, the best fit line represents the line with the lowest possible \\(RSS\\). This line is found by manipulating the values for \\(\\beta_0\\) and \\(\\beta_1\\) since the x values must stay the same."
  },
  {
    "objectID": "posts/lin_reg2/index.html#the-calculus-behind-best-fit",
    "href": "posts/lin_reg2/index.html#the-calculus-behind-best-fit",
    "title": "Linear Regression Guide",
    "section": "The Calculus Behind Best Fit",
    "text": "The Calculus Behind Best Fit\nThere exists a combination of \\(\\beta_0\\) and \\(\\beta_1\\) that is the lowest. Geometrically, we can imagine a shape in three dimensions where two inputs (\\(\\beta_0\\) and \\(\\beta_1\\)) produce an output (\\(RSS\\)). Roughly speaking, where the three dimensional slope is equal to 0 is where the combination of \\(\\beta_0\\) and \\(\\beta_1\\) will produce the lowest possible \\(RSS\\). Since the derivative of an equation can tell us the slope at any given point, we can set both equations’ derivatives to zero to find where slope is zero for both equations.\nStep One: Take the partial derivatives with respect to \\(\\beta_0\\) and \\(\\beta_1\\)\nThe partial derivative with respect to \\(\\beta_0\\) is calculated as so:\n\\[\\frac{\\partial}{\\partial\\beta_0}\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2\\] \\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial\\beta_0}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\frac{\\partial}{\\partial\\beta_0}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)(-1) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nThe partial derivative with respect to \\(\\beta_1\\) is calculated as so:\n\\[\\frac{\\partial}{\\partial\\beta_1}\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2\\] \\[\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial\\beta_1}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\frac{\\partial}{\\partial\\beta_1}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)(-x_i) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^nx_i\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n\\]\nStep Two: Set the simplified partial derivatives to zero and solve for \\(\\beta_0\\) and \\(\\beta_1\\) respectively\nSolving for \\(\\beta_0\\): \\[0 = -2\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\]\n\\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^n\\left(y_i - \\beta_0-\\beta_1x_i\\right) &\\text{Simplify}\\\\\n&0 = \\sum_{i=1}^ny_i - \\sum_{i=1}^n\\beta_0-\\sum_{i=1}^n\\beta_1x_i &\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^ny_i - n\\beta_0-\\beta_1\\sum_{i=1}^nx_i &\\text{Constant Multiple Rule for Sums}\\\\\n& n\\beta_0 = \\sum_{i=1}^ny_i -\\beta_1\\sum_{i=1}^nx_i &\\text{Simplify}\\\\\n& \\beta_0 = \\frac{\\sum_{i=1}^ny_i}{n} -\\frac{\\beta_1\\sum_{i=1}^nx_i}{n} &\\text{Simplify}\\\\\n& \\beta_0 = \\bar{y} - \\beta_1\\bar{x} &\\text{Simplify to 'Average'}\\\\\n\\end{align*}\n\\]\nSolving for \\(\\beta_1\\) while substituting \\(\\beta_0\\):\n\\[0 = -2\\sum_{i=1}^nx_i\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\] \\[\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^nx_i\\left(y_i - ((\\bar{y} - \\beta_1\\bar{x})+\\beta_1x_i)\\right) &\\text{Substitute }\\beta_0\\\\\n&0 = \\sum_{i=1}^nx_i(y_i-\\bar{y}) - \\sum_{i=1}^n\\beta_1x_i(x_i-\\bar{x})&\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nx_i(y_i-\\bar{y}) - \\beta_1\\sum_{i=1}^nx_i(x_i-\\bar{x})&\\text{Constant Multiple Rule for Sums}\\\\\n& \\beta_1= \\frac{\\sum_{i=1}^nx_i(y_i-\\bar{y})}{\\sum_{i=1}^nx_i(x_i-\\bar{x})} &\\text{Simplify}\\\\\n& \\beta_1= \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} &\\text{Simplify (steps skipped)}\\\\\n\\end{align*}\n\\]"
  }
]