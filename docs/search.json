[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Aj",
    "section": "",
    "text": "Data Science Enthusiast"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rooftop Data",
    "section": "",
    "text": "Regression (by hand)\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nState of American Christianity\n\n\n\n\n\n\n\nstatistics\n\n\nanime\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nAj Averett\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Triangle of Power\n\n\n\n\n\n\n\nnotation\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nThe P-value\n\n\n\n\n\n\n\nstatistics\n\n\nanime\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nAj Averett\n\n\n\n\n\n\n  \n\n\n\n\nA Quick Review\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\nAj Averett\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pvalue/index.html",
    "href": "posts/pvalue/index.html",
    "title": "The P-value",
    "section": "",
    "text": "“While the p-value can be a useful statistical measure, it is commonly misused and misinterpreted.” (Wasserstein and Lazar, 2016)\nNote: The contents of this article are not peer-reviewed and may contain errors.\n\nMeet Levi. He is a short person. He is around 5’3” (63 inches). While we can tell he is short, we may wonder how short he is. In order to know this we need something to compare his height to.\n\nAmerican Sample\nThe code below allows us to create a hypothetical (but reasonable) data set of American male heights. I will be creating a sheet of 200 random American men. Below are a few entries…\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport requests\nimport matplotlib.pyplot as plt\n\n###\n\ndef create_random_people_dataframe(n):\n  # Create a Pandas data frame with a single column for heights\n  df = pd.DataFrame({'height': np.random.normal(loc=69, scale=3, size=n)})\n   \n  # Round the heights to the nearest integer\n  df['height'] = df['height'].round(2)\n\n  # Add an 'id' column to the data frame\n  df['id'] = df.index + 1\n\n  # Send a request to the API and retrieve the names\n  response = requests.get(\n    \"https://randommer.io/api/Name\", \n    headers={\n      \"X-Api-Key\":api_key},\n    params={\n      \"nameType\": \"surname\",\n      \"quantity\": n}\n    )\n\n  # Parse the response from the API to extract the names\n  names = response.json()\n\n  # Add names to the data frame\n  df = df.assign(name=names)\n  \n  # Rearrange the columns\n  df = df.reindex(columns=[\"id\", \"name\", \"height\"])\n\n  # Return the data frame\n  return df\n\n# Generate a data frame with random people's heights\ndf = create_random_people_dataframe(200)\n\ndf.drop(columns=['id']).head()\n\n\n\n\n\n\n  \n    \n      \n      name\n      height\n    \n  \n  \n    \n      0\n      Gaines\n      66.38\n    \n    \n      1\n      Hoch\n      71.11\n    \n    \n      2\n      Futrell\n      67.41\n    \n    \n      3\n      Scanlan\n      69.01\n    \n    \n      4\n      Polanco\n      71.97\n    \n  \n\n\n\n\n\n\nQuantifying Extremes!\nA standard score, also known as a z-score, is a way of comparing a single data point to a group of data points. It helps us understand how high or low a number is relative to the group. The z-score measures how many standard deviations away a point is from the group average. (For a review, see here)\nThe average height and standard deviation for the generated American men is calculated below\n\nmean_height = df[\"height\"].mean().round()\nst_dev_height = df[\"height\"].std().round()\nn_height = df[\"height\"].count()\n\nprint(\"Average height is: \"+str(mean_height))\nprint(\"Standard deviation of height is: \"+str(st_dev_height))\nprint(\"Number of people in sample is: \"+str(n_height))\n\nAverage height is: 69.0\nStandard deviation of height is: 3.0\nNumber of people in sample is: 200\n\n\nThe formula for z-score is as so:\n\\[\nz = \\frac{x-\\mu}{\\sigma}\n\\]\nAs the z-score increases/decreases, the father away it gets from the mean. For example, Shaq is 85 inches tall. His z-score would be calculated as so:\n\nz_score_shaq = (85 - mean_height)/st_dev_height\n\nprint(\"Shaq's height is \"+str(z_score_shaq.round(2))+\" standard deviations from the mean\")\n\nShaq's height is 5.33 standard deviations from the mean\n\n\nThis also means that the smaller the z-score is, the closer it is to the mean. Take an average height person:\n\nz_score_avg = (69 - mean_height)/st_dev_height\n\nprint(z_score_avg)\n\n0.0\n\n\n\n\nLevi’s Height\nLevi is 5’3” (63 inches). That’s pretty short. Let’s see how short by calculating a z-score:\n\nz_score_levi = (63 - mean_height)/st_dev_height\n\nprint(z_score_levi)\n\n-2.0\n\n\nThis shows that Levi is a couple standard deviations lower than average. Pretty short!\n\n\nArea means probability\nThe graph below shows the normal distribution of American men with labeled heights for a standard deviation above and below average height (assuming average is 69 inches and standard deviation is 3 inches)\n\n\n\nIf you met a random American, the odds that they are taller than average is 50 percent. (the odds that they are shorter than average is 50 percent) We can see that illustrated below. If the area under the whole curve is equal to 1, that means that half the area is equal to .5.\n\nWe can also imagine Levi compared to the American male population. The red line marks where Levi’s height is and the blue fill is for any person as short as Levi or shorter. This means that the probability of meeting someone as short or shorter than Levi in the American population is 2.28 percent.\n\nShaq is unbelievably tall. Like you will probably not meet anyone as tall (or taller) than him. On the graph below, the red line on the right represents the height of Shaq. The filled in the blue area is so small, you cannot even see it. The number on top tells us that the odds of finding an American man as tall (or taller) than Shaq is 0.000004862 percent. (very unlikely!)\n\n\n\nA whole bunch of guys\nWhat we did above is calculate a z-score for a single piece of data given a normal distribution. The formula is seen below. The curve represents the possible heights that the individual could be.\n\\[\nz = \\frac{x-\\mu}{\\sigma}\n\\]\nWhat if instead of using a single data point, we use a group of data points to compare? The reason for doing a single data point would to see how uncommon a data point would be given a distribution- this means that the reason for doing a group of points would be to see how uncommon a group would be given a distribution.\nAssume we have a group of people from Attack on Titan.\n\n\\(\\bar{x}\\) would represent the average of the Attack on Titan group.\n\\(\\mu\\) would represent the mean of American men\n\\(\\sigma\\) would represent the standard deviation of American men\n\\(n\\) would represent the number of people in the Attack on Titan group\n\n\\[\nz = \\frac{\\bar{x}-\\mu}{\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)}\n\\]\nA cool mathematical trick has just been done. Looking at one person compared to a population can tell us the probability that he came from the population. More interestingly, looking at a group compared to the population can tell us the probability that the group came from the population.\nFor the comparison with the individual, we need\n\nFind the height of the individual.\nFind the distribution of heights from the population (the curve represents potential/possible heights that the person could be)\nUse the height to produce a z-score for the individual. (calculated with the formula above)\nWherever the z-score is, find the area of the tail of the distribution. The area shaded under the curve would represent the probability of finding someone that tall/short or taller/shorter.\n\nSimilarly, For the comparison with the group, we need\n\nFind the height average of the group.\nFind the distribution of heights from the population (the curve represents potential/possible height averages that the person could be)\nUse the height to produce a z-score (test-statistic) for the group. (calculated with the formula above)\nWherever the z-score is, find the area of the tail of the distribution. The area shaded under the curve would represent the probability of finding a group that tall/short or taller/shorter.\n\nBelow, we find the test statistic (z-score) of the Attack on Titan group.\n\ndf_aot = pd.DataFrame(\n      {'name': ['Eren', 'Armin', 'Levi','Connie','Marco'],\n       'height': [67, 64, 65, 62, 70]}\n)\n\nzscore_aot = (df_aot[\"height\"].mean() - mean_height)/(st_dev_height/np.sqrt(df_aot[\"height\"].count()))\n\nprint(\"The average height for the Attack on Titan group is \"+str(df_aot[\"height\"].mean()))\nprint(\"The test-statistic/z-score for the Attack on Titan group is \"+str(zscore_aot.round(2)))\n\nThe average height for the Attack on Titan group is 65.6\nThe test-statistic/z-score for the Attack on Titan group is -2.53\n\n\n\nimport scipy.stats\n#Code to find area under curve on left-side\npval = (scipy.stats.norm.sf(abs(-2.53)))*100\n\nprint(\"The probability of finding American men as short (or shorter) than S1 Attack on Titan characters (assuming American men are truly 69 inches tall with a standard deviation of 3 inches) would be \" + str(pval.round(2)) + \"%\")\n\nThe probability of finding American men as short (or shorter) than S1 Attack on Titan characters (assuming American men are truly 69 inches tall with a standard deviation of 3 inches) would be 0.57%\n\n\nNote: We just generated a distribution from theoretical possible sample means. This is possible due to the Central Limit Theorem- see here and here for some good explanations. Just for simple intuition, you are more likely to get a sample with a mean closer to the population mean than farther. (Also, notice how this assumes that the population standard deviation is known-it usually isn’t)\nOn the graph below, if the population mean is truly 69 inches, then finding samples with the means 66 and 72 would be equally likely.\n\n\n\nIntroducing the P-value\nIn null hypotheses significance testing, we set up two hypotheses, the null hypothesis, and the alternative hypothesis. The process to test a hypothesis is as so:\n\nAssume the null hypothesis is true\nCalculate what we would expect to see if the null hypothesis is true (output is a distribution of potential test statistics/sample means etc.)\nCalculate what we actually observe (output is your sample’s test statistic)\nUsing the distribution of potential test statistics, find the probability of getting your sample’s test statistic\nIf you find that the probability of you getting a test statistic is low enough according to the generated distribution of test statistics from your null hypothesis, (usually below 5 percent) reject the null hypothesis- otherwise fail to reject it.\n\nThat’s it. The p-value is the probability of getting a sample as extreme or more extreme than any other result assuming your null hypothesis. As shown above, the two things you need to calculate the p-value are\n\nthe observed sample value (test statistic)\nthe distribution of theoretical sample values (test statistics) that could occur\n\nGetting these two numbers depends on what type of test you are doing and what type of assumptions you are willing to make.\nThe cut-off\nNow how improbably is improbable? For example, if a coin is fair and you throw it 100 times, we expect to roughly get as many heads as tails- but at what point are we going to say, “Hey this coin doesn’t seem fair?” If we get 40 heads and 60 tails, this is less likely than 50 heads and 50 tails, but we can reasonably see this happening just by chance. However, if we get 75 heads and 25 tails- this would be reallllly unlikely assuming the coin is fair.\nBy tradition, researchers have used the arbitrary “5 percent” cutoff point to determine what is “significant” or not. If the sample’s test statistic is so extreme that we would only expect the given sample to be that (or more) extreme less than 5 percent of the time, we deem it significant. `\n\nExpected Data/The Distribution and Observed Data/The Test statistic\nThis article will not deeply go into the distributions that you may see how extreme your test statistic is, but understanding the general nature will be useful for grasping how the p-value works.\nZ-distribution\nThe z-distribution, also called the normal distribution, assumes that you know the standard deviation of your population. When your sample increases, your sample’s standard deviation will approach the population standard deviation. If this is the case, sufficiently large samples may assume the population standard deviation is ‘known’.\nt-distribution\nAssuming the population standard deviation is not known, we may need to use the t-distribution.\nThe z-distribution has a set spread because we know the spread of the population. If we are unsure of the spread of the population, me may have to accept the possibility that seemingly rare events may just be results of a large spread. Now as our sample increases, we can be more sure of how the spread looks, but otherwise, we just make the distribution of test statistics wider (and therefore less sensitive to extreme findings).\nWhat specifically determines the spread of the t-distribution is the degrees of freedom. (calculated using the sample) Since larger sample sizes approximate the spread of the data better than small ones, the distribution becomes narrower as a result of these large sample sizes.\nF-distribution\nTest statistics and distributions of test statistics come in all varieties. The F-statistic is usually used to measure if groups of variables have some relation.\n\nAnalysis of Variance (ANOVA) uses an F-test to measure if group means are all equal. Assuming that they are all equal, a super abnormal F-statistic would mean that either they aren’t all equal or they are equal and your sample observations were just really unlikely\nLinear Regression may use an F-test to measure if the model or any of its variables is related to the outcome. Assuming no relationship with the model or any of its variables, a super abnormal F-statistic would mean that either the model actually explains some of the model outcome, or its doesn’t and the sample observations were just really unlikely.\n\nSimilar to the t-distribution, the F-distribution will change given the degrees of freedom of the sample.\nChi-Squared (χ2) Distribution\nThis distribution is similar to F-tests since it measures relationships between groups, but it does so with counts/frequencies of categorical variables rather than values or any other measurements from quantitative variables.\nLike the previous distributions, the chi-squared distribution will change given the degrees of freedom of the sample\nParametric and Non-Parametric tests\nYou can calculate a p-value from anything. All you need is the distribution of what you would find theoretically given your null hypothesis, and what you have actually found. Most of the tests above are parametric tests (all but the chi squared) and therefore have various assumptions- though there are more relaxed tests that may be appropriate for various situations. Some of these are as following:\n\nWilcoxon Signed Rank test (One sample/Paired Sample t-test but non-parametric)\nWilcoxon Rank Sum/Mann-Whitney U test (Two Sample t-test but non-parametric)\nKruskal-Wallis test (ANOVA but non-parametric)\n\nThe ultimate non-parametric test is the permutation test which makes a custom distribution of theoretical test statistics. This is only to show that the only thing one needs for a p-value is a theoretical sampling distribution of test statistics (under the null hypothesis) and the observed test statistic.\n\n\n\nWhat the p-value is NOT\nThe p-value is NOT the likelihood that a result is due to chance\n“When \\(p\\) is calculated, it is already assumed that \\(H_0\\) is true, so the probability that sampling error is the only explanation is already taken to be [100 percent]. It is thus illogical to view \\(p\\) as measuring the likelihood of sampling error. thus, \\(p\\) does not apply to a particular result as the probability that sampling error was the sole causal agent. there is no such thing as a statistical technique that determines the probability that various causal factors, including sampling error, acted on a particular result. instead, inference about causation is a rational exercise that considers results within the context of design, measurement, and analysis. Besides, virtually all sample results are affected by error of some type, including measurement error.”\n(Kline 2013)\nThe p-value is NOT the likelihood that a result is due to chance under the null hypothesis.\n“That this is not the case is seen immediately from the P value’s definition, the probability of the observed data, plus more extreme data, under the null hypothesis. The result with the P value of exactly .05 (or any other value) is the most probable of all the other possible results included in the”tail area” that defines the P value. The probability of any individual result is actually quite small, and Fisher said he threw in the rest of the tail area “as an approximation.”\n(Goodman, 2014)\nThe p-value is NOT the likelihood that a Type 1 error has occurred\n“[T]he p-value represents the probability of making a Type I error if the null hypothesis is perfectly true…\nThe p-value does not give you the probability that you have made a Type I error in reality. The phrase “if the null hypothesis were perfectly true” is the key to avoiding this misinterpretation. If H0 is not an absolutely perfect statistical model for reality (perhaps because of tiny differences between group means in the population, or just because the null is completely wrong), then the probability expressed in a p-value may not be accurate (Kirk, 1996). It is still a useful statistic, however, especially because it can help us make rational decisions about how well the null hypothesis matches the data in a study (Wasserstein & Lazar, 2016; Winch & Campbell, 1969).\n(Warne, 2017)\nThe p-value is NOT the likelihood that \\(H_0\\) is true\n“The \\(p\\) value does not say anything about the probability that the null hypothesis is true because the p-value is calculated under the scenario that the null hypothesis is perfectly true (Fidler, 2010). As a result, \\(p\\) cannot tell us whether the null hypothesis is true because we had to assume it was true in the first place to calculate \\(p\\) (Haller & Krauss, 2002). Likewise, a p-value cannot tell us whether the alternative hypothesis is true because \\(p\\) is based on the assumption that the null hypothesis is perfectly true.”\n(Warne, 2017)\n\\(1 -\\) p-value is NOT the likelihood that the result will be replicated\n“p says very little about the replicability or stability of results (Schmidt, 1996). Indeed, p-values fluctuate wildly across studies (Cumming, 2008). It is true, though, that (generally speaking) low p-values indicate that the null hypothesis may possibly be easier to reject in the future (Cumming & Maillardet, 2006). However, this assumes that the replications are conducted under precisely the same conditions as the original study, with every possible relevant (and irrelevant) factor perfectly replicated. In real life, a low p-value does not necessarily mean that you will get similar results if you conducted the same study again. In fact, attempts to replicate studies with low p-values often do not succeed (Open Science Collaboration, 2015). If the sample size is small or if the sample is not representative of the population in some way (as occurs frequently with nonrandom samples), the results may be unstable and not replicable, even if p is very low and the null hypothesis is strongly rejected. The best way to determine whether the results of a study will replicate is to conduct a replication.”\n(Warne 2017)\nThe p-value is NOT an indicator of how important the findings are\nThe p-value says nothing about the importance of findings (Kieffer, Reese, & Thompson, 2001). Remember that importance is an issue of practical significance – not statistical significance. There is nothing magical about an α value less than .05. Decisions about policy, psychological interventions, and other practical implications of research should be based on more than just a p-value. Such important decisions should not be based on a statistic that is vulnerable to changes in sample size and study conditions (Wasserstein & Lazar, 2016).\n(Warne, 2017)\nThe p-value is NOT an indicator about the size of an effect\n“p-values indicate the size of an effect (e.g., the difference between means or the magnitude of the relationship between variables). For example, a researcher finding that p is greater than α (indicating that the null hypothesis should be retained) may decide that the results are”insignificant” or irrelevant. This confuses p-values with effect sizes. Effect sizes quantify the strength of mean group differences or variable relationships; p-values do not (Wasserstein & Lazar, 2016). If the sample size is small or the study has low statistical power, then there could be a large effect size that is worth discussing, even if p is too high to provide evidence to reject the null hypothesis… Understanding the strength of relationships or the magnitude of group differences is important – and that is why it is essential to calculate and report an effect size to accompany a null hypothesis statistical significance test (American Psychological Association, 2010; Wasserstein & Lazar, 2016).”\n(Warne, 2017)\n\nSee more here and here\nThe misinterpretation of p-values and its consequences have been a disaster on academia"
  },
  {
    "objectID": "posts/pvalue/index.html#introducing-the-p-value",
    "href": "posts/pvalue/index.html#introducing-the-p-value",
    "title": "The P-value",
    "section": "Introducing the P-value",
    "text": "Introducing the P-value\nIn null hypotheses significance testing, we set up two hypotheses, the null hypothesis, and the alternative hypothesis. The process to test a hypothesis is as so:\n\nAssume the null hypothesis is true\nCalculate what we would expect to see if the null hypothesis is true (output is a distribution of potential test statistics/sample means etc.)\nCalculate what we actually observe (output is your sample’s test statistic)\nUsing the distribution of potential test statistics, find the probability of getting your sample’s test statistic\nIf you find that the probability of you getting a test statistic is low enough according to the generated distribution of test statistics from your null hypothesis, (usually below 5 percent) reject the null hypothesis- otherwise fail to reject it.\n\nThat’s it. The p-value is the probability of getting a sample as extreme or more extreme than any other result assuming your null hypothesis. As shown above, the two things you need to calculate the p-value are\n\nthe observed sample value (test statistic)\nthe distribution of theoretical sample values (test statistics) that could occur\n\nGetting these two numbers depends on what type of test you are doing and what type of assumptions you are willing to make.\nThe cut-off\nNow how improbably is improbable? For example, if a coin is fair and you throw it 100 times, we expect to roughly get as many heads as tails- but at what point are we going to say, “Hey this coin doesn’t seem fair?” If we get 40 heads and 60 tails, this is less likely than 50 heads and 50 tails, but we can reasonably see this happening just by chance. However, if we get 75 heads and 25 tails- this would be reallllly unlikely assuming the coin is fair.\nBy tradition, researchers have used the arbitrary “5 percent” cutoff point to determine what is “significant” or not. If the sample’s test statistic is so extreme that we would only expect the given sample to be that (or more) extreme less than 5 percent of the time, we deem it significant. `\n\nExpected Data/The Distribution and Observed Data/The Test statistic\nThis article will not deeply go into the distributions that you may see how extreme your test statistic is, but understanding the general nature will be useful for grasping how the p-value works.\nZ-distribution\nThe z-distribution, also called the normal distribution, assumes that you know the standard deviation of your population. When your sample increases, your sample’s standard deviation will approach the population standard deviation. If this is the case, sufficiently large samples may assume the population standard deviation is ‘known’.\nt-distribution\nAssuming the population standard deviation is not known, we may need to use the t-distribution.\nThe z-distribution has a set spread because we know the spread of the population. If we are unsure of the spread of the population, me may have to accept the possibility that seemingly rare events may just be results of a large spread. Now as our sample increases, we can be more sure of how the spread looks, but otherwise, we just make the distribution of test statistics wider (and therefore less sensitive to extreme findings).\nWhat specifically determines the spread of the t-distribution is the degrees of freedom. (calculated using the sample) Since larger sample sizes approximate the spread of the data better than small ones, the distribution becomes narrower as a result of these large sample sizes.\nF-distribution\nTest statistics and distributions of test statistics come in all varieties. The F-statistic is usually used to measure if groups of variables have some relation.\n\nAnalysis of Variance (ANOVA) uses an F-test to measure if group means are all equal. Assuming that they are all equal, a super abnormal F-statistic would mean that either they aren’t all equal or they are equal and your sample observations were just really unlikely\nLinear Regression may use an F-test to measure if the model or any of its variables is related to the outcome. Assuming no relationship with the model or any of its variables, a super abnormal F-statistic would mean that either the model actually explains some of the model outcome, or its doesn’t and the sample observations were just really unlikely.\n\nSimilar to the t-distribution, the F-distribution will change given the degrees of freedom of the sample.\nChi-Squared (χ2) Distribution\nThis distribution is similar to F-tests since it measures relationships between groups, but it does so with counts/frequencies of categorical variables rather than values or any other measurements from quantitative variables.\nLike the previous distributions, the chi-squared distribution will change given the degrees of freedom of the sample\nParametric and Non-Parametric tests\nYou can calculate a p-value from anything. All you need is the distribution of what you would find theoretically given your null hypothesis, and what you have actually found. Most of the tests above are parametric tests (all but the chi squared) and therefore have various assumptions- though there are more relaxed tests that may be appropriate for various situations. Some of these are as following:\n\nWilcoxon Signed Rank test (One sample/Paired Sample t-test but non-parametric)\nWilcoxon Rank Sum/Mann-Whitney U test (Two Sample t-test but non-parametric)\nKruskal-Wallis test (ANOVA but non-parametric)\n\nThe ultimate non-parametric test is the permutation test which makes a custom distribution of theoretical test statistics. This is only to show that the only thing one needs for a p-value is a theoretical sampling distribution of test statistics (under the null hypothesis) and the observed test statistic."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "A Quick Review",
    "section": "",
    "text": "Central Tendency\nThe mean and the median are two different ways of describing the “middle” of a group of numbers.\nMean (Average)\nThe mean (also known as average) is found by adding up all of the numbers in the group and then dividing that number by the total number of numbers in the group. For example, if a group of kids has the following test scores: 82, 85, 90, 91, and 92, the mean (or average) score would be calculated like this:\n\n# The code below is just printing out the class average\n\nclass_average <- (82 + 85 + 90 + 91 + 92) / 5\n\nprint(class_average)\n\n[1] 88\n\n\nThe class mean or class average is 88\nMedian\nThe median is the number that is in the middle of the group when the numbers are listed in order from least to greatest. In the same group of scores above, the numbers would be listed like this when they are in order: 82, 85, 90, 91, and 92. The number in the middle is 90.\nThe class mean is 90\nBasically, the mean is found by adding up all of the numbers and dividing by the total number of numbers, while the median is the number in the middle when the numbers are listed in order. Both the mean and the median can be used to describe the “middle” of a group of numbers, but they can sometimes give different results.\n\n\nNormal Distributions\nFor people that go to the gym, you may see wear and tear on some of the equipment. On the image below, a curve shape appears with the deterioration of the metal finish. The center of this curve represents the average weightlifting ability of the population that uses it. There would be fewer people who would lift significantly more or less weight than the average. This pattern is called a normal distribution.\n\n\n\n\n\nIf we were to graph height, most people tend to be around the average since there are natural limits to how tall or short people can be.\n\n\n\n\n\n\n\nStandard Deviation\n“A standard deviation is a measure of how dispersed the data is in relation to the mean. Low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out. A standard deviation close to zero indicates that data points are close to the mean, whereas a high or low standard deviation indicates data points are respectively above or below the mean.” [1]\n\n\\(\\mu\\) refers to the mean or average\n\\(\\sigma\\) refers to a standard deviation\n\n\n\n\n\n\nThe standard deviation, as explained above, measures the spread of the data. To see this with an example, take two cities, Kansas City, MO, and San Diego, CA. In 2019, both of these cities probably had an average temperature in the 60’s. The green line in the picture above represents the average temperature.\nthough the average temperatures were comparable, the spread is much bigger for one of these cities. [2][3]\n\nKansas City, MO- low/high: 22°F-91°F\nSan Diego, CA- low/high: 50°F-77°F\n\nWe can expect the standard deviation for temperature to be far greater in Kansas City than San Diego."
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html",
    "href": "posts/power_triangle/power_triangle.html",
    "title": "The Triangle of Power",
    "section": "",
    "text": "A post on Math Overflow about the relationship between powers, roots, and logs sparked the birth of a notation to harmonize these seemingly unrelated concepts. Grant Sanderson from 3Blue1Brown further popularized and named this notation as, “The Triangle of Power”"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#a-remains-constant",
    "href": "posts/power_triangle/power_triangle.html#a-remains-constant",
    "title": "The Triangle of Power",
    "section": "\\(a\\) remains constant",
    "text": "\\(a\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Exponents}\n\\\\\\\\\n&\n\\log_a(x\\times y) = \\log_a(x) + \\log_b(x)\n&\na^{x + y} = a^x \\times a ^y\n\\\\\\\\\n&\n\\log_a(\\frac{x}{y}) = \\log_a(x) - \\log_b(x)\n&\na^{x - y} = \\frac{a^x}{a^y}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#b-remains-constant",
    "href": "posts/power_triangle/power_triangle.html#b-remains-constant",
    "title": "The Triangle of Power",
    "section": "\\(b\\) remains constant",
    "text": "\\(b\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Exponents}\n&\\text{Properties of Roots}\n\\\\\\\\\n&   xy^b = x^b \\times y^b\n&   \\sqrt[b]{x\\times y} = \\sqrt[b]{x} \\times \\sqrt[b]{y}\n\\\\\\\\\n&   \\left(\\frac{x}{y}\\right)^b = \\frac{x^b}{y^b}\n&   \\sqrt[b]{\\frac{x}{y}} = \\frac{\\sqrt[b]{x}}{\\sqrt[b]{y}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#c-remains-constant",
    "href": "posts/power_triangle/power_triangle.html#c-remains-constant",
    "title": "The Triangle of Power",
    "section": "\\(c\\) remains constant",
    "text": "\\(c\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Roots}\n\\\\\\\\\n&   \\log_{x\\times y}(c) = \\left(\\left(\\log_xc\\right)^{-1} + \\left(\\log_yc\\right)^{-1}\\right)^{-1}\n&   \\sqrt[(x^{-1}+ y^{-1})^{-1}]{c} = \\sqrt[x]{c} \\times \\sqrt[y]{c}\n\\\\\\\\\n&   \\log_{\\frac{x}{y}}(c) = \\left(\\left(\\log_xc\\right)^{-1} - \\left(\\log_yc\\right)^{-1}\\right)^{-1}\n&   \\sqrt[(x^{-1}- y^{-1})^{-1}]{c} = \\frac{\\sqrt[x]{c}}{\\sqrt[y]{c}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#a-remains-constant-1",
    "href": "posts/power_triangle/power_triangle.html#a-remains-constant-1",
    "title": "The Triangle of Power",
    "section": "\\(a\\) remains constant",
    "text": "\\(a\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Exponents}\n\\\\\\\\\n&\n_{a}\n\\stackrel{\\phantom{b}}\n\\triangle _{xy}\n={}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{x} +{}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{y}\n&\n_{a}\n\\stackrel{x+y}\n\\triangle _{\\phantom{c}}\n={}\n_{a}\\stackrel{x}\\triangle _{\\phantom{c}} \\times{}\n_{a}\\stackrel{y}\\triangle _{\\phantom{c}}\n\\\\\\\\\n&_{a}\n\\stackrel{\\phantom{b}}\n\\triangle _{\\frac{x}{y}}\n={}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{x} -{}\n_{a}\\stackrel{\\phantom{b}}\\triangle _{y}\n&\n_{a}\n\\stackrel{x-y}\n\\triangle _{\\phantom{c}}\n={}\n\\frac\n{_{a}\\stackrel{x}\\triangle _{\\phantom{c}}}\n{_{a}\\stackrel{y}\\triangle _{\\phantom{c}}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#b-remains-constant-1",
    "href": "posts/power_triangle/power_triangle.html#b-remains-constant-1",
    "title": "The Triangle of Power",
    "section": "\\(b\\) remains constant",
    "text": "\\(b\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Exponents}\n&\\text{Properties of Roots}\n\\\\\\\\\n&\n_{x\\times y}\n\\stackrel{b}\n\\triangle _{\\phantom{c}}\n={}\n_{x}\\stackrel{b}\\triangle _{} \\times{}\n_{y}\\stackrel{b}\\triangle _{}\n&\n_{\\phantom{a}}\n\\stackrel{b}\n\\triangle _{x\\times y}\n={}\n_{\\phantom{x}}\\stackrel{b}\\triangle _{x} \\times{}\n_{\\phantom{x}}\\stackrel{b}\\triangle _{y}\n\\\\\\\\\n&_{\\frac{x}{y}}\n\\stackrel{b}\n\\triangle _{\\phantom{c}}\n={}\n\\frac\n{_{x}\\stackrel{b}\\triangle _{\\phantom{c}} }\n{_{y}\\stackrel{b}\\triangle _{\\phantom{c}} }\n&\n_{\\phantom{a}}\n\\stackrel{b}\n\\triangle _{\\frac{x}{y}}\n=\n\\frac\n{_{\\phantom{a}}\\stackrel{b}\\triangle _{x}}\n{_{\\phantom{a}}\\stackrel{b}\\triangle _{y}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/power_triangle/power_triangle.html#c-remains-constant-1",
    "href": "posts/power_triangle/power_triangle.html#c-remains-constant-1",
    "title": "The Triangle of Power",
    "section": "\\(c\\) remains constant",
    "text": "\\(c\\) remains constant\n\\[\n\\begin{align*}\n&\\text{Properties of Logarithms}\n&\\text{Properties of Roots}\n\\\\\\\\\n&\n_{x\\times y}\n\\stackrel{\\phantom{b}}\n\\triangle _{c}\n={}\n\\left(\n\\left( _{x}\\stackrel{}\\triangle _{c}\\right)^{-1} +{}\n\\left(_{y}\\stackrel{}\\triangle _{c}\\right)^{-1}\n\\right)^{-1}\n&\n_{\\phantom{a}}\n\\stackrel{\n  \\left(x^{-1}+y^{-1}\\right)^{-1}\n}\n\\triangle _{c}\n={}\n_{\\phantom{x}}\\stackrel{x}\\triangle _{c} \\times{}\n_{\\phantom{x}}\\stackrel{y}\\triangle _{c}\n\\\\\\\\\n&\n_{\\frac{x}{y}}\n\\stackrel{\\phantom{b}}\n\\triangle _{c}\n={}\n\\left(\n\\left( _{x}\\stackrel{}\\triangle _{c}\\right)^{-1} -{}\n\\left(_{y}\\stackrel{}\\triangle _{c}\\right)^{-1}\n\\right)^{-1}\n&\n_{\\phantom{a}}\n\\stackrel{\n  \\left(x^{-1}-y^{-1}\\right)^{-1}\n}\n\\triangle _{c}\n={}\n\\frac\n{_{\\phantom{x}}\\stackrel{x}\\triangle _{c} }\n{_{\\phantom{x}}\\stackrel{y}\\triangle _{c}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/linear_regression/linear_regression.html",
    "href": "posts/linear_regression/linear_regression.html",
    "title": "Linear Regression (by hand)",
    "section": "",
    "text": "We have three functions above that we can fit points to. \\(f_1\\) describes a straight line while \\(f_2\\) describes a parabola.\nFor our points, we can represent them as \\((x_i, y_i)\\) for \\(i = 1, 2, 3, ... , n\\). The given function models. Both function 1 and 2 are used to predict a \\(y\\) value from an \\(x\\) value. Additionally, the domain of \\(f\\) is \\(x >= 0\\). To calculate probability and likelihood, the density function, \\(f(r) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(r-\\mu)^2}{2\\sigma^2}}\\) will be used. For this function, \\(\\mu\\) refers to the mean and will be set at 0 and \\(\\sigma\\) refers to the standard deviation and will be set to 1.\nFinally, for any model \\(f_j\\), we can obtain\n\na derived set of residuals or errors, \\(r_j\\),\na respective probability model, \\(p_j\\),\na joint probability model, \\(J_j\\),\na likelihood function, \\(L_j\\), and\na log-likelihood function, \\(\\ell_j\\).\n\n\n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_1(t_i) = 100 + a_1t_i\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{1i} = y_i -(100 + a_1t_i) \\Leftrightarrow r_{1i} = y_i -100 - a_1t_i \\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_1(t_i, y_i;a_1) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_1(\\overrightarrow{t}, \\overrightarrow{y}; a_1) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_1\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_1(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_1(a_1;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\) \\[\n\\begin{align*}\n\\mathscr{}\n&= \\ln(L_1(a_1;\\overrightarrow{t}, \\overrightarrow{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i - 100 - a_1t_i\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i - 100 - a_1t_i\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_1(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i - 100 - a_1t_i\\right)^2\\)\n\n\n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_2(t_i; a_1,a_2) = 100 + a_1t_i + a_2t_i^2\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{2i} = y_i -(100 + a_1t_i + a_2t_i^2) \\Leftrightarrow r_{2i} = y_i -100 - a_1t_i - a_2t_i^2 \\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_2(t_i, y_i;a_1,a_2) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_2(\\overrightarrow{t}, \\overrightarrow{y}; a_1, a_2) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i - 100 - a_1t_i\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_2\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_2(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_2(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\) \\[\n\\begin{align*}\n&= \\ln(L_2(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_2(a_1,a_2;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2t_i^2\\right)^2\\) \n\n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_4(t; a_1,a_2) = 100 + a_1t + a_2\\ln(0.005t+1)\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{4i} = y_i -(100 + a_1t_i + a_2\\ln(0.005t_i+1)) \\Leftrightarrow r_{4i} = y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_4(t_i, y_i;a_1,a_2) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_4(\\overrightarrow{t}, \\overrightarrow{y}; a_1, a_2) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_2\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_4(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_4(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\)\n\n\\[\n\\begin{align*}\n&= \\ln(L_4(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_4(a_1,a_2;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2\\ln(0.005t_i+1)\\right)^2\\) \n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_5(t; a_1) = 100e^{-0.00005t} + a_1te^{-0.00005t}\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{5i} = y_i -(100e^{-0.00005t_i} + a_1t_ie^{-0.00005t_i}) \\Leftrightarrow r_{5i} = y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\\\\\)\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_5(t_i, y_i;a_1) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_5(\\overrightarrow{t}, \\overrightarrow{y}; a_1) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_2\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_5(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_5(a_1;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\)\n\n\\[\n\\begin{align*}\n&= \\ln(L_5(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_5(a_1;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100e^{-0.00005t_i} - a_1t_ie^{-0.00005t_i}\\right)^2\\) \n\n\n\nFor points \\((t_i, y_i)\\) where \\(i = 1,2,3,...,44\\),\n\n\\(f_j\\) is a model to predict \\(y_i\\) given an \\(x_i\\) where \\(f_6(t; a_1,a_2) = 100 + a_1t + a_2(1-e^{-0.0003t})\\).\n\\(r_j\\), is a set of residuals calculated for each point where \\(r_{ji} = y_i-f(x_i)\\). This means that the following is true: \\(r_{6i} = y_i -(100 + a_1t_i + a_2(1-e^{-0.0003t_i})) \\Leftrightarrow r_{6i} = y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\\\\\).\n\\(p_j\\) will be used to denote a probability model of a single residual, \\(r_{ji}\\), where \\(p_6(t_i, y_i;a_1,a_2) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2}\\).\n\\(J_j\\) will be used to calculate the joint probability of every residual \\(r_{ji}\\), where \\(J_6(\\overrightarrow{t}, \\overrightarrow{y}; a_1, a_2) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2}\\right)\\).\n\\(L_j\\) will be used to calculate the likelihood of the model \\(f_6\\) given the residuals calculated using the points \\((t_i,y_i)\\) where \\(L_6(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y}) = \\prod_{i=1}^{44}\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2}\\right)\\).\n\\(\\ell_j\\) will be used to calculate the log-likelihood function from \\(L\\) where \\(\\ell_6(a_1, a_2;\\overrightarrow{t}, \\overrightarrow{y})\\\\\\) \\[\n\\begin{align*}\n&= \\ln(L_6(a_1;\\textbf{t},\\textbf{y})) &\\text{(definition)}\\\\\n&= \\ln\\left(\\prod_{i=1}^{44}\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right) &\\text{(substitution)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)  &\\text{(log product/sum rule)}\\\\\n&= \\sum_{i=1}^{44}\\left(\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)\\right)&\\text{(another product to sum)}\\\\\n&= \\sum_{i=1}^{44}\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)&\\text{(separate the sum)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}\\ln\\left(e^{-\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2/2}\\right)&\\text{(pull out constant)}\\\\\n&= \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)\\sum_{i=1}^{44}1+\\sum_{i=1}^{44}-\\frac{1}{2}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2\\ln\\left(e\\right)&\\text{(bring power down)}\\\\\n&= 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2&\\text{(simplify)}\n\\end{align*}\n\\] thus, \\(\\ell_6(a_1,a_2;\\overrightarrow{t}, \\overrightarrow{y}) = 44\\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\frac{1}{2}\\sum_{i=1}^{44}\\left(y_i -100 - a_1t_i - a_2(1-e^{-0.0003t_i})\\right)^2\\)"
  },
  {
    "objectID": "posts/usa_christian/index.html",
    "href": "posts/usa_christian/index.html",
    "title": "American Christianity",
    "section": "",
    "text": "The General Social Survey\nThe General Social Survey (GSS) is a nationally representative survey of American adults that has been conducted since 1972. The GSS is conducted by the National Opinion Research Center (NORC) at the University of Chicago, and it is widely considered to be one of the most important sources of data on trends in American attitudes, beliefs, and behaviors.\n\n\nShow the code\ndf_raw <- readxl::read_excel(\"GSS.xlsx\")\n\n\nOne of the key findings of the GSS is that Christianity in the United States is in decline. In the early 1970s, around 90% of Americans identified as Christian, but by 2020, that number had dropped to around 70%. This decline is particularly pronounced among young adults, with only around half of adults under the age of 30 identifying as Christian.\nOne of the main reasons for this decline is the rise of the “Nones” - a term used to describe people who do not identify with any particular religion. The number of Nones in the United States has been steadily increasing over the past few decades, and by 2020, around one in four Americans identified as Nones.\n\n\nShow the code\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(across(everything(), ~replace(., . %in% c(\"Protestant\",\"Catholic\",\"Orthodox-christian\") , \"Christian\"))) %>%\n  mutate(across(everything(), ~replace(., . %in% c(\"Buddhism\",\"Hinduism\",\"Other eastern religions\",\"Inter-nondenominational\",\"Native american\",\"Other\",\"Muslim/islam\") , \"Other\"))) %>%\n  group_by(year, relig) %>% \n  count() %>%\n  filter(!relig %in% c(\".n:  No answer\", \".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Religion\",\n    title = \"Percent Religon Over Time\"\n  ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw()\n\n\n\n\n\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Catholicism has remained relatively stable, with the proportion of Catholics in the U.S. population staying around 30% since the 1970s. On the other hand, the proportion of Protestants has seen a slight decline, dropping from around 55% in the 1970s to around 45% in 2020.\nOther branches of Christianity, such as Mormonism, Jehovah’s Witnesses, and Eastern Orthodoxy, have seen an increase in recent years. By 2020, these denominations made up around 5% of the U.S. population, a significant increase from their representation in the 1970s.\n\n\nShow the code\nchristian_vector <- c('PROTESTANT','CATHOLIC','CHRISTIAN','ORTHODOX-CHRISTIAN') %>% tolower()\n\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  filter(tolower(relig) %in% christian_vector) %>% \n  mutate(across(everything(), ~replace(., . ==  \"Orthodox-christian\" , \"Christian\"))) %>% \n  mutate(across(everything(), ~replace(., . ==  \"Christian\" , \"Other\"))) %>% \n  group_by(year, relig) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Denomination\",\n    title = \"Percent Christian Denomination Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw()"
  },
  {
    "objectID": "posts/usa_christian/index.html#rise-of-none",
    "href": "posts/usa_christian/index.html#rise-of-none",
    "title": "American Christianity",
    "section": "Rise of ‘None’",
    "text": "Rise of ‘None’\nOne of the key findings of the GSS is that Christianity in the United States is in decline. In the early 1970s, around 90% of Americans identified as Christian, but by 2020, that number had dropped to around 70%. This decline is particularly pronounced among young adults, with only around half of adults under the age of 30 identifying as Christian.\nOne of the main reasons for this decline is the rise of the “Nones” - a term used to describe people who do not identify with any particular religion. The number of Nones in the United States has been steadily increasing over the past few decades, and by 2020, around one in four Americans identified as Nones.\n\n\nShow the code\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(across(everything(), ~replace(., . %in% c(\"Protestant\",\"Catholic\",\"Orthodox-christian\") , \"Christian\"))) %>%\n  mutate(across(everything(), ~replace(., . %in% c(\"Buddhism\",\"Hinduism\",\"Other eastern religions\",\"Inter-nondenominational\",\"Native american\",\"Other\",\"Muslim/islam\") , \"Other\"))) %>%\n  group_by(year, relig) %>% \n  count() %>%\n  filter(!relig %in% c(\".n:  No answer\", \".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Religion\",\n    title = \"Percent Religon Over Time\"\n  ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_bw()"
  },
  {
    "objectID": "posts/usa_christian/index.html#new-christians",
    "href": "posts/usa_christian/index.html#new-christians",
    "title": "American Christianity",
    "section": "New Christians",
    "text": "New Christians\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Catholicism has remained relatively stable, with the proportion of Catholics in the U.S. population staying around 30% since the 1970s. On the other hand, the proportion of Protestants has seen a slight decline, dropping from around 55% in the 1970s to around 45% in 2020.\nOther branches of Christianity, such as Mormonism, Jehovah’s Witnesses, and Eastern Orthodoxy, have seen an increase in recent years. By 2020, these denominations made up around 5% of the U.S. population, a significant increase from their representation in the 1970s.\n\n\nShow the code\nchristian_vector <- c('PROTESTANT','CATHOLIC','CHRISTIAN','ORTHODOX-CHRISTIAN') %>% tolower()\n\ndf_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  filter(tolower(relig) %in% christian_vector) %>% \n  mutate(across(everything(), ~replace(., . ==  \"Orthodox-christian\" , \"Christian\"))) %>%\n  mutate(across(everything(), ~replace(., . ==  \"Christian\" , \"Other\"))) %>% \n  group_by(year, relig) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x=as.numeric(year), y = percent, color = relig)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"Percent\",\n    color = \"Denomination\",\n    title = \"Percent Christian Denomination Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_bw()"
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-christians",
    "href": "posts/usa_christian/index.html#strength-of-christians",
    "title": "State of American Christianity",
    "section": "Strength of Christians",
    "text": "Strength of Christians\nAnother aspect of the decline of Christianity in the United States is the weakening of its strength within the population. This can be seen in the decreasing participation in religious practices such as church attendance.\nAccording to the GSS, the proportion of general Christians who do not attend church has increased from 15% in 1970 to 23% in 2020. Additionally, the proportion of Christians who attend church weekly has declined from 41% in 1970 to 36% in 2020. However, the proportion of Christians who attend church either monthly or less frequently has remained relatively stable.\nThis decline in church attendance is particularly pronounced among young Christians. The proportion of young Christians who attend church weekly or more frequently has decreased in all categories, except for the “seldom” category in which young Christians increased from 23% in 1970 to 45% in 2020.\nTechnical Note: This code is using the tidyverse and patchwork library to analyze and create a plot of church attendance among Christians over time. The code first filters the dataframe to only include rows where the “age” column contains numeric values, and then converts the “age” and “year” columns to numeric format. Next, it combines all Christian denominations as a single variable called “Christian”, and then filters to only Christians. The code replaces the several different categories of church attendance to Weekly, Monthly, Yearly, and Seldom. The code then groups and counts how many people in each category and uses “ggplot” to make a visualization.\n\n\nShow the code\nchristian_attend <- df_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(age = as.numeric(age)) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(age = plyr::round_any(as.numeric(age), 10)) %>% \n  mutate(across(everything(), ~replace(., . %in% c(\"Protestant\",\"Catholic\",\"Orthodox-christian\") , \"Christian\"))) %>% \n  filter(relig == \"Christian\") %>% \n  filter(!attend %in% c(\".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>%\n  mutate(attend = ifelse(attend %in% c(\"Every week\",\"Nearly every week\",\"Several times a week\"), \"Weekly\", attend),\n         attend = ifelse(attend %in% c(\"2-3 times a month\",\"About once a month\",\"At least monthly\"), \"Monthly\", attend),\n         attend = ifelse(attend %in% c(\"About once or twice a year\",\"Several times a year\"), \"Yearly\", attend),\n         attend = ifelse(attend %in% c(\"Less than once a year\",\"Never\"), \"Seldom\", attend)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(attend = factor(attend, levels = c(\"Weekly\", \"Monthly\", \"Yearly\",\"Seldom\"))) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nChristians Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n  \n\ndf_christian <- df_raw %>%\n  filter(str_detect(as.character(age), \"^[0-9]+$\")) %>% \n  mutate(age = as.numeric(age)) %>% \n  mutate(year = plyr::round_any(as.numeric(year), 10)) %>% \n  mutate(age = plyr::round_any(as.numeric(age), 10)) %>% \n  filter(!attend %in% c(\".d:  Do not Know/Cannot Choose\",\".s:  Skipped on Web\")) %>%\n  mutate(attend = ifelse(attend %in% c(\"Every week\",\"Nearly every week\",\"Several times a week\"), \"Weekly\", attend),\n         attend = ifelse(attend %in% c(\"2-3 times a month\",\"About once a month\",\"At least monthly\"), \"Monthly\", attend),\n         attend = ifelse(attend %in% c(\"About once or twice a year\",\"Several times a year\"), \"Yearly\", attend),\n         attend = ifelse(attend %in% c(\"Less than once a year\",\"Never\"), \"Seldom\", attend)) %>% \n  filter(attend != \".n:  No answer\") %>% \n  mutate(attend = factor(attend, levels = c(\"Weekly\", \"Monthly\", \"Yearly\",\"Seldom\"))) \n\nchristian_attend_youth <- df_christian %>%\n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nChristian Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n\n\nchristian_attend + christian_attend_youth & scale_y_continuous(limits = c(.1, .46),labels = scales::percent)\n\n\n\n\n\nStrength of Protestants\nWhile overall Christianity in the United States is in decline, the decline has not been evenly distributed across different denominations. In particular, the decline of Protestant Christianity appears to be less pronounced than that of Christianity as a whole.\nWhen it comes to church attendance, the data shows that weekly attendance among Protestants has remained relatively stable. According to the GSS, the proportion of Protestants who attend church weekly has remained around 35% since 1970. Furthermore, the weekly attendance for both general Protestants and young Protestants actually increased slightly.\n\n\nShow the code\nprot_attend <- df_christian %>% \n  filter(relig == \"Protestant\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nProtestants Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() +  theme(legend.position = 'none')\n\nprot_attend_youth <- df_christian %>%\n  filter(relig == \"Protestant\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"\",\n    title = \"Church Attendance Among\\nProtestant Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n\n\n\nprot_attend + prot_attend_youth & scale_y_continuous(limits = c(.1, .42),labels = scales::percent)\n\n\n\n\n\nStrength of Catholics\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Among all Christian denominations, the Catholic Church appears to be the one that has been most affected by the decline in attendance.\nAccording to the GSS, weekly attendance among Catholics has decreased significantly since 1970. Among general Catholics, weekly attendance dropped from 53% to 27% during this time period. Similarly, weekly attendance among Catholic youth has also decreased, declining from 40% in 1970 to 15% in 2020.\nHowever, the decline in attendance has not been limited to weekly attendance. The proportion of Catholics who attend church seldom or less frequently has also increased. Seldom attendance increased from 11% in 1970 to 26% in 2020. Furthermore, the youth seldom attendance doubled from 12% to 24% in 2020.\n\n\nShow the code\ncath_attend <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholics Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n\ncath_attend_youth <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1) +\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholic Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n  \n\ncath_attend + cath_attend_youth & scale_y_continuous(limits = c(.1, .54),labels = scales::percent)\n\n\n\n\n\nCharting the decline\nIn conclusion, this blog post has discussed the decline of Christianity in the United States over the past few decades, as reported by the GSS. The data shows that the proportion of Christians in the U.S. population has dropped from around 90% in the early 1970s to around 70% in 2020.\nThe decline of Christianity is also reflected in the weakening of its strength within the population, as seen in the decreasing participation in religious practices such as church attendance. The proportion of general Christians who do not attend church has increased from 15% in 1970 to 23% in 2020 and the proportion of Christians who attend church weekly has declined from 41% in 1970 to 36% in 2020. However, the proportion of Christians who attend church either monthly or less frequently has remained relatively stable.\nWhen looking at different Christian denominations, the decline of Protestant Christianity appears to be less pronounced than that of Christianity as a whole. The proportion of Protestants who attend church weekly has remained around 35% since 1970, and weekly attendance for both general Protestants and young Protestants actually increased slightly. On the other hand, the Catholic Church appears to be the one that has been most affected by the decline in attendance. Among general Catholics, weekly attendance dropped from 53% to 27% during this time period, and the youth seldom attendance doubled from 12% to 24% in 2020.\nThe term “Chreaster” is defined as such: “A Christian who does not frequently attend church, attending only on the major holidays of Christmas and Easter.” (Wiktionary) Chreasters have been on the increase especially among younger Christians and Catholics.\nWhile this blog post does not try to explain the ‘why’ in this phenomenon, there are several factors that readers can probably think of that have led to this decline. Some of the possible factors include the increasing secularization of American society, the rise of social media and the internet, the growing diversity of American society, the sexual abuse scandals that have plagued the Catholic Church, and the loss of trust and credibility among Catholics."
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-protestants",
    "href": "posts/usa_christian/index.html#strength-of-protestants",
    "title": "American Christianity",
    "section": "Strength of Protestants",
    "text": "Strength of Protestants\nWhile overall Christianity in the United States is in decline, the decline has not been evenly distributed across different denominations. In particular, the decline of Protestant Christianity appears to be less pronounced than that of Christianity as a whole.\nWhen it comes to church attendance, the data shows that weekly attendance among Protestants has remained relatively stable. According to the General Social Survey (GSS), the proportion of Protestants who attend church weekly has remained around 35% since 1970. Furthermore, the weekly attendance for both general Protestants and young Protestants actually increased slightly.\n\n\nShow the code\nprot_attend <- df_christian %>% \n  filter(relig == \"Protestant\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nProtestants Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() +  theme(legend.position = 'none')\n\nprot_attend_youth <- df_christian %>%\n  filter(relig == \"Protestant\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"\",\n    title = \"Church Attendance Among\\nProtestant Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n\n\n\nprot_attend + prot_attend_youth & scale_y_continuous(limits = c(.1, .42),labels = scales::percent)"
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-protestants-1",
    "href": "posts/usa_christian/index.html#strength-of-protestants-1",
    "title": "American Christianity",
    "section": "Strength of Protestants",
    "text": "Strength of Protestants\n\n\nShow the code\ncath_attend <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among Catholics Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n\ncath_attend_youth <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among Protestant Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw()\n\ncath_attend + cath_attend_youth & scale_y_continuous(limits = c(.1, .5),labels = scales::percent)\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\nWarning: Removed 1 rows containing missing values (geom_text_repel)."
  },
  {
    "objectID": "posts/usa_christian/index.html#strength-of-catholics",
    "href": "posts/usa_christian/index.html#strength-of-catholics",
    "title": "American Christianity",
    "section": "Strength of Catholics",
    "text": "Strength of Catholics\nWhile Christianity as a whole is in decline, the decline has not been evenly distributed across different denominations. Among all Christian denominations, the Catholic Church appears to be the one that has been most affected by the decline in attendance.\nAccording to the General Social Survey (GSS), weekly attendance among Catholics has decreased significantly since 1970. Among general Catholics, weekly attendance dropped from 53% to 27% during this time period. Similarly, weekly attendance among Catholic youth has also decreased, declining from 40% in 1970 to 15% in 2020.\nHowever, the decline in attendance has not been limited to weekly attendance. The proportion of Catholics who attend church seldom or less frequently has also increased. Seldom attendance increased from 11% in 1970 to 26% in 2020. Furthermore, the youth seldom attendance doubled from 12% to 24% in 2020.\n\n\nShow the code\ncath_attend <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1)+\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholics Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + theme(legend.position = 'none')\n\ncath_attend_youth <- df_christian %>%\n  filter(relig == \"Catholic\") %>% \n  filter(age %in% c(20,30)) %>% \n  group_by(year,attend) %>% \n  count() %>% \n  group_by(year) %>% \n  mutate(percent = round(n/sum(n),5)) %>% \n  mutate(percent_label = (round(n/sum(n),2))*100) %>% \n  mutate(percent_label = percent_label %>%  as.character() %>% paste0(\"%\")) %>% \n  ggplot(aes(x = year, y = percent, group = attend, color = attend)) + \n  geom_line(size = 1) +\n  geom_point(size = 3.5) +\n  labs(\n    x = \"Year\",\n    y = \"\",\n    color = \"Attendance\",\n    title = \"Church Attendance Among\\nCatholic Youth Over Time\"\n    ) +\n  ggrepel::geom_text_repel(aes(label = percent_label)) +\n  theme_bw() + \n  theme(\n    # legend.position = 'bottom',\n    legend.direction = \"vertical\"\n    )\n  \n\ncath_attend + cath_attend_youth & scale_y_continuous(limits = c(.1, .54),labels = scales::percent)"
  }
]