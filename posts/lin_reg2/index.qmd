---
title: "Linear Regression Guide"
author: "Aj Averett"
date: "2023-4-18"
image: 'reg.png'
categories: [statistics]
editor: 
  markdown: 
    wrap: 72
---

# Simple Linear Regression

## **True** $Y$**, Observed** $Y$**, and Estimated** $Y$

Simple linear regression means that there is only a single variable (not
multiple $X$'s)

-   $X$ - A single quantitative explanatory variable (independent)

-   $Y$ - A single quantitative response variable (dependent)

The **True Regression Line**

$$
E\{Y\} = \beta_0 + \beta_1 X_i 
$$

-   $E\{Y\}$ - True mean y-value, also $\mu_{Y|X}$ or $E\{Y|X\}$

-   $\beta_0$ - True $y$-intercept

-   $\beta_1$ - True slope

The **Observed Points**

$$
Y_i = \beta_0 + \beta_1 X_i+ \epsilon_i = b_0 + b_1X_i + r_i
$$

-   $Y_i$ - Response or dependent variable for the $i^{\text{th}}$
    observation.

-   $\epsilon_i$ - Error, distance of dot to true line.
    $\epsilon_i = Y_i - E\{Y\}$

-   $r_i$ - Residual, distance of dot to estimated line.
    $r_i = Y_i - \hat{Y}_i$

The **Estimated Regression Line** obtained from a regression analysis
from the observed points

$$
\hat{Y}_i = b_0 + b_1 X_i
$$

-   $\hat{Y}_i$ - The fitted line

-   $b_0$ - Estimated $y$-intercept, also $\hat{\beta}_0$

-   $b_1$ - Estimated slope, also $\hat{\beta}_1$

::: {.callout-note collapse="true"}
# R code for calling your data

```{r}
#| warning: false
library(tidyverse)

ggplot(mtcars, aes(x=wt, y=mpg)) + 
  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +
  labs(title="Miles per Gallon vs. Weight of Car", 
       x="Weight of Car", 
       y="Miles per Gallon") +
  theme_classic()  

```

```{r}
```

```{r}
Y <- mtcars$mpg 
X <- mtcars$wt

i <- 3 #random number
```

```{r}
X[i]
```

```{r}
Y[i]
```

```{r}
#| warning: false
library(tidyverse)

ggplot(mtcars, aes(x=wt, y=mpg)) + 
  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +
  labs(title="Miles per Gallon vs. Weight of Car", 
       x="Weight of Car", y="Miles per Gallon") +
  theme_classic() +
  geom_vline(aes(xintercept = wt[i] ), color = "red", linetype = "dashed") +
  geom_hline(aes(yintercept = mpg[i]), color = "red", linetype = "dashed")


```
:::

## Residuals and the Sum of Squared Errors

The estimated regression line or line of best fit from the observed
points is represented by the equation $\hat{Y}_i=b_0+b_1X_i$ , where
$b_1$ is the estimated slope of the line and $b_0$ is the estimated
y-intercept.

### Minimizing Residuals

A single residual, $r_i$, is the distance between an observed point and
the estimated regression line. Given any line, we can calculate how
different our predicted value, $\hat{Y_i}$ is from our actual data
point, $Y_i$. This vertical distance from the real y output and the
predicted y output is called the residual. The formula for any given
residual is represented as such

$$
\begin{align*}
r_i &= Y_i-\hat{Y}_i\\
r_i &= Y_i - (b_0+b_1X_i)
\end{align*}
$$

The goal of simple linear regression is to minimize the sum of the
squared errors ($\text{SSE}$). This is represented by the equations:

$$
\begin{align*}
\text{SSE} &= \sum_{i=1}^n \left(r_i\right)^2 \\
\text{SSE} &= \sum_{i=1}^n\left(Y_i - \hat{Y_i}\right)^2\\
\text{SSE} &= \sum_{i=1}^n\left(Y_i - (b_0+b_1X_i)\right)^2
\end{align*}
$$

The closer any given line is to the best fit line, the lower the
$\text{SSE}$ will be. In fact, the best fit line represents the line
with the lowest possible $\text{SSE}$. This line is found by
manipulating the values for $b_0$ and $b_1$ since the $X$ values must
stay the same.

## Calculating the Estimated Regression Line

There are multiple ways to calculate the estimated regression line like
with calculus or linear algebra.

::: {.callout-tip collapse="true"}
## The Calculus Behind Best Fit

# The Calculus Behind Best Fit

There exists a combination of $b_0$ and $b_1$ that is the lowest.
Geometrically, we can imagine a shape in three dimensions where two
inputs ($b_0$ and $b_1$) produce an output ($\text{SSE}$). Roughly
speaking, where the three dimensional slope is equal to 0 is where the
combination of $b_0$ and $b_1$ will produce the lowest possible
$\text{SSE}$. Since the derivative of an equation can tell us the slope
at any given point, we can set both equations' derivatives to zero to
find where slope is zero for both equations.

**Step One: Take the partial derivatives with respect to** $b_0$ and
$b_1$

The partial derivative with respect to $b_0$ is calculated as so:

$$\frac{\partial}{\partial b_0}\sum_{i=1}^n\left(Y_i - (b_0+b_1X_i)\right)^2$$
$$
\begin{align*}
\mathscr{}
&= \sum_{i=1}^n\frac{\partial}{\partial b_0}\left(Y_i - (b_0+b_1X_i)\right)^2  &\text{Sum Rule}\\
&= \sum_{i=1}^n2\left(Y_i - (b_0+b_1X_i)\right)\frac{\partial}{\partial b_0}\left(Y_i - (b_0+b_1X_i)\right) &\text{Power Rule}\\
&= \sum_{i=1}^n 2\left(Y_i - (b_0+b_1X_i)\right)(-1) &\text{Simplify}\\
&= -2\sum_{i=1}^n\left(Y_i - (b_0+b_1X_i)\right) &\text{Constant Multiple Rule for Sums}
\end{align*}
$$

The partial derivative with respect to $b_1$ is calculated as so:

$$\frac{\partial}{\partial b_1}\sum_{i=1}^n\left(Y_i - (b_0+b_1X_i)\right)^2$$
$$
\begin{align*}
\mathscr{}
&= \sum_{i=1}^n\frac{\partial}{\partial b_1}\left(Y_i - (b_0+b_1X_i)\right)^2  &\text{Sum Rule}\\
&= \sum_{i=1}^n2\left(Y_i - (b_0+b_1X_i)\right)\frac{\partial}{\partial b_1}\left(Y_i - (b_0+b_1X_i)\right) &\text{Power Rule}\\
&= \sum_{i=1}^n 2\left(Y_i - (b_0+b_1X_i)\right)(-X_i) &\text{Simplify}\\
&= -2\sum_{i=1}^nX_i\left(Y_i - (b_0+b_1X_i)\right) &\text{Constant Multiple Rule for Sums}
\end{align*}
$$

**Step Two: Set the simplified partial derivatives to zero and solve
for** $b_0$ **and** $b_1$ **respectively**

Solving for $b_0$: $$0 = -2\sum_{i=1}^n\left(Y_i - (b_0+b_1X_i)\right)$$

$$
\begin{align*}
\mathscr{}
&0 = \sum_{i=1}^n\left(Y_i - b_0-b_1x_i\right) &\text{Simplify}\\
&0 = \sum_{i=1}^nY_i - \sum_{i=1}^nb_0-\sum_{i=1}^nb_1X_i &\text{Sum Rule}\\
&0 = \sum_{i=1}^nY_i - nb_0-b_1\sum_{i=1}^nX_i &\text{Constant Multiple Rule for Sums}\\
& nb_0 = \sum_{i=1}^nY_i -b_1\sum_{i=1}^nX_i &\text{Simplify}\\
& b_0 = \frac{\sum_{i=1}^nY_i}{n} -\frac{b_1\sum_{i=1}^nX_i}{n} &\text{Simplify}\\
& b_0 = \bar{Y} - b_1\bar{X} &\text{Simplify to average}\\
\end{align*}
$$

Solving for $b_1$ while substituting $b_0$:

$$
0 = -2\sum_{i=1}^nX_i\left(Y_i - (b_0+b_1X_i)\right)
$$

$$
\begin{align*}
\mathscr{}
&0 = \sum_{i=1}^nX_i\left(Y_i - ((\bar{Y} - b_1\bar{X})+b_1X_i)\right) &\text{Substitute }b_0\\
&0 = \sum_{i=1}^nX_i(Y_i-\bar{Y}) - \sum_{i=1}^nb_1X_i(X_i-\bar{X})&\text{Sum Rule}\\
&0 = \sum_{i=1}^nX_i(Y_i-\bar{Y}) - b_1\sum_{i=1}^nX_i(X_i-\bar{X})&\text{Constant Multiple Rule for Sums}\\
& b_1= \frac{\sum_{i=1}^nX_i(Y_i-\bar{Y})}{\sum_{i=1}^nX_i(X_i-\bar{X})} &\text{Simplify}\\
& b_1= \frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2} &\text{Simplify (steps skipped)}\\
\end{align*}
$$

**Step Three: Construct the formula**

Once you have found $b_0$ and $b_1$ as so:

-   $b_0 = \bar{Y} - b_1\bar{X}$

-   $b_1= \frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}$

($\bar{Y}$ represents the mean of the $Y$ values and $\bar{X}$
represents the mean of the $X$ values)

We can construct the formula for $\hat{Y}_i$ as the following:

$$
\hat{Y}_i = b_0 + b_1X_i
$$

Which means $b_0$ is the change in the mean of $Y$ for a $1$ unit
increase in $X$.
:::

::: {.callout-tip collapse="true"}
## Least Squares in Linear Algebra

# Least Squares in Linear Algebra

__Step One: Organize the data into vectors__
Let's assume you have $n$ observations of the independent variable $\mathbf{X}$ and dependent variable $\mathbf{Y}$. Organize the data into two vectors:

$$
\mathbf{X} = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix}, \quad \mathbf{Y} = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}
$$

__Step Two: Create a design matrix__

Create a design matrix $\mathbf{A}$ by adding a column of ones to represent the intercept term

$$
\mathbf{A} = \begin{bmatrix} 1 & X_1 \\ 1 & X_2 \\ \vdots & \vdots \\ 1 & X_n \end{bmatrix}
$$

__Step Three: Make a Projection:__ 

The goal of simple linear regression is to find the best-fitting line, which can be viewed as projecting the dependent variable $\mathbf{Y}$ onto the column space of the design matrix $\mathbf{A}$. This projection, denoted by $\mathbf{\hat{Y}}$, can be computed as:

$$
\mathbf{\hat{Y}} = \mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{Y}
$$

This formula comes from the orthogonal projection of $\mathbf{Y}$ onto the column space of $\mathbf{A}$, which minimizes the squared residuals.

__Step Four: Compute the coefficients:__

The coefficients of the best-fitting line can be found by solving the following linear system:

$$
\mathbf{A}^T\mathbf{A}\mathbf{\beta} = \mathbf{A}^T\mathbf{Y}
$$

Here, $\mathbf{\beta}$ is a vector containing the coefficients $\beta_0$ and $\beta_1$:


$$
\mathbf{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}
$$

To solve for $\mathbf{\beta}$, you can use the following formula:


$$
\mathbf{\beta} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{Y}
$$

__Step Five: Construct the formula:__ 

Once you have computed the coefficients $\beta_0$ and $\beta_1$, you can write the estimated regression line as:


$$
\hat{Y}_i = \beta_0 + \beta_1 X_i
$$

The estimated slope, $\beta_1$, indicates the average change in the dependent variable ($Y$) for a one-unit increase in the independent variable ($X$). The estimated intercept, $\beta_0$, represents the predicted value of $Y$ when $X = 0$.



:::


The formula from calculus to find the coefficients for the estimated
regression line is as so:

-   $b_0 = \bar{Y} - b_1\bar{X}$

-   $b_1= \frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}$

($\bar{Y}$ represents the mean of the $Y$ values and $\bar{X}$
represents the mean of the $X$ values)

::: {.callout-note collapse="true"}
# R code for calculating the coefficients for the estimated regression line

```{r}
mylm <- lm(mpg ~ wt, data = mtcars)
pander::pander(summary(mylm))
```

```{r}
b_0 <- mylm$coefficients[1]
b_1 <- mylm$coefficients[2]
```

```{r}
b_0
```

```{r}
b_1
```

```{r}
ggplot(mtcars, aes(x = wt, y = mpg)) +
geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +
stat_function(fun = function(x) b_0 + b_1 * x) + 
labs(title="Miles per Gallon vs. Weight of Car", 
     x="Weight of Car", 
     y="Miles per Gallon") +
theme_classic() 
```
:::

## Assumption for Simple Linear Regression

If we are to make a model off the points displayed as such:

$$
Y_i = \beta_0 + \beta_1 X_i+ \epsilon_i
$$

We need to confirm the following assumptions

-   **1) Linear relation**: Linear relationship between $X$ and $Y$

-   **2) Normal errors**: $\epsilon \sim \mathcal{N}(0,\sigma^2)$, the
    error terms are normally distributed with a mean of 0

-   **3) Constant variance**: The variance $σ^2$ of the error terms is
    constant (the same) across all values of $X_i$

-   **4) Fixed** $X$: the $X$ values can be considered fixed and
    measured without error

-   **5) Independent errors**: $\epsilon$ is independent

$Y_i = \beta_0 + \beta_1 X_i+ \epsilon_i :\epsilon \sim \mathcal{N}(0,\sigma^2)$

Given $X$, $Y \sim \mathcal{N}(\beta_0 +\beta_1X,\sigma^2)$

::: {.callout-note collapse="true"}
# R code for testing the assumptions for simple linear regression

Three plots in R can be used to test the assumptions for simple linear
regression:

**Residuals versus Fitted-values Plot** The linear relationship and
constant variance assumptions can be diagnosed using a residuals versus
fitted-values plot. The fitted values are the $Y^i$

-   The residuals are the $r_i$

-   This plot compares the residual to the magnitude of the
    fitted-value. No discernable pattern in this plot is desirable.

**Q-Q Plot of the Residuals**

The normality of the error terms can be assessed by considering a normal
probability plot (Q-Q Plot) of the residuals. If the residuals appear to
be normal, then the error terms are also considered to be normal. If the
residuals do not appear to be normal, then the error terms are also
assumed to violate the normality assumption.

**Residuals versus Order Plot**

When the data is collected in a specific order, or has some other
important ordering to it, then the independence of the error terms can
be assessed. This is typically done by plotting the residuals against
their order of occurrance. If any dramatic trends are visible in the
plot, then the independence assumption is violated.

```{r}
par(mfrow=c(1,3)) #Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3

plot(mylm,which=1:2) #Q-Q Plot of the Residuals: Checks Assumption #2

plot(mylm$residuals) #Residuals versus Order Plot: Checks Assumption #5

```
:::

## Assessing the Estimated Regression Line

### Terms

**Sum of Squared Errors**

Measures how much the residuals deviate from the line.

$\text{SSE} = \sum_{i=1}^n \left(r_i\right)^2 = \sum_{i=1}^n\left(Y_i - \hat{Y_i}\right)^2$

**Sum of Squares Regression**

Measures how much the regression line deviates from the average y-value.

$\text{SSR} = \sum_{i=1}^n \left(r_i\right)^2 = \sum_{i=1}^n\left(\hat{Y}_i - \bar{Y}\right)^2$

**Total Sum of Squares**

Measures how much the y-values deviate from the average y-value.

$\text{SSTO} = \sum_{i=1}^n \left(Y_i - \bar{Y}\right)^2$

![](images/paste-1.png){width="544"}

$\text{SSTO}=\text{SSE}+\text{SSR}$

::: {.callout-note collapse="true"}
# R code for calculating the above terms

```{r}
residuals <- mylm$residuals
fitted_values <- mylm$fitted.values
mean_mpg <- mean(mtcars$mpg)

SSE <- sum(residuals^2)
SSR <- sum((fitted_values - mean_mpg)^2)
TSS <- sum((mtcars$mpg - mean_mpg)^2)

pander::pander(cat("Sum of Squared Errors (SSE):", SSE, "\n"))
pander::pander(cat("Sum of Squares Regression (SSR):", SSR, "\n"))
pander::pander(cat("Total Sum of Squares (TSS):", TSS, "\n"))
```

:::

## Calculating the Coefficient of Determination

Using the terms above, we can calculate a ratio of the variance of $Y$
explained by the estimated regression line ($\text{SSE}$) and the total
variance of $Y$ from the average $Y$ value ($\text{SSTO}$.

$$
R^2 = 1-\frac{\text{SSE}}{\text{SSTO}}
$$

::: {.callout-note collapse="true"}
# R code for calculating the coefficient of determination

```{r}
lm_summary <- summary(mylm)

pander::pander(lm_summary$r.squared)
```
:::