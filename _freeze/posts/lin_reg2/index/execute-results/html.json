{
  "hash": "e0fd06637f49af53e0167ca5d7c52896",
  "result": {
    "markdown": "---\ntitle: \"Linear Regression Guide\"\nauthor: \"Aj Averett\"\ndate: \"2023-4-18\"\nimage: 'reg.png'\ncategories: [statistics]\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# Simple Linear Regression\n\n## **True** $Y$**, Observed** $Y$**, and Estimated** $Y$\n\nSimple linear regression means that there is only a single variable (not\nmultiple $X$'s)\n\n-   $X$ - A single quantitative explanatory variable (independent)\n\n-   $Y$ - A single quantitative response variable (dependent)\n\nThe **True Regression Line**\n\n$$\nE\\{Y\\} = \\beta_0 + \\beta_1 X_i \n$$\n\n-   $E\\{Y\\}$ - True mean y-value, also $\\mu_{Y|X}$ or $E\\{Y|X\\}$\n\n-   $\\beta_0$ - True $y$-intercept\n\n-   $\\beta_1$ - True slope\n\nThe **Observed Points**\n\n$$\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i = b_0 + b_1X_i + r_i\n$$\n\n-   $Y_i$ - Response or dependent variable for the $i^{\\text{th}}$\n    observation.\n\n-   $\\epsilon_i$ - Error, distance of dot to true line.\n    $\\epsilon_i = Y_i - E\\{Y\\}$\n\n-   $r_i$ - Residual, distance of dot to estimated line.\n    $r_i = Y_i - \\hat{Y}_i$\n\nThe **Estimated Regression Line** obtained from a regression analysis\nfrom the observed points\n\n$$\n\\hat{Y}_i = b_0 + b_1 X_i\n$$\n\n-   $\\hat{Y}_i$ - The fitted line\n\n-   $b_0$ - Estimated $y$-intercept, also $\\hat{\\beta}_0$\n\n-   $b_1$ - Estimated slope, also $\\hat{\\beta}_1$\n\n::: {.callout-note collapse=\"true\"}\n# R code for calling your data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", \n       y=\"Miles per Gallon\") +\n  theme_classic()  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nY <- mtcars$mpg \nX <- mtcars$wt\n\ni <- 3 #random number\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX[i]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.32\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nY[i]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 22.8\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\n  labs(title=\"Miles per Gallon vs. Weight of Car\", \n       x=\"Weight of Car\", y=\"Miles per Gallon\") +\n  theme_classic() +\n  geom_vline(aes(xintercept = wt[i] ), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = mpg[i]), color = \"red\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\n## Residuals and the Sum of Squared Errors\n\nThe estimated regression line or line of best fit from the observed\npoints is represented by the equation $\\hat{Y}_i=b_0+b_1X_i$ , where\n$b_1$ is the estimated slope of the line and $b_0$ is the estimated\ny-intercept.\n\n### Minimizing Residuals\n\nA single residual, $r_i$, is the distance between an observed point and\nthe estimated regression line. Given any line, we can calculate how\ndifferent our predicted value, $\\hat{Y_i}$ is from our actual data\npoint, $Y_i$. This vertical distance from the real y output and the\npredicted y output is called the residual. The formula for any given\nresidual is represented as such\n\n$$\n\\begin{align*}\nr_i &= Y_i-\\hat{Y}_i\\\\\nr_i &= Y_i - (b_0+b_1X_i)\n\\end{align*}\n$$\n\nThe goal of simple linear regression is to minimize the sum of the\nsquared errors ($\\text{SSE}$). This is represented by the equations:\n\n$$\n\\begin{align*}\n\\text{SSE} &= \\sum_{i=1}^n \\left(r_i\\right)^2 \\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2\\\\\n\\text{SSE} &= \\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2\n\\end{align*}\n$$\n\nThe closer any given line is to the best fit line, the lower the\n$\\text{SSE}$ will be. In fact, the best fit line represents the line\nwith the lowest possible $\\text{SSE}$. This line is found by\nmanipulating the values for $b_0$ and $b_1$ since the $X$ values must\nstay the same.\n\n## Calculating the Estimated Regression Line\n\nThere are multiple ways to calculate the estimated regression line like\nwith calculus or linear algebra.\n\n::: {.callout-tip collapse=\"true\"}\n## The Calculus Behind Best Fit\n\n# The Calculus Behind Best Fit\n\nThere exists a combination of $b_0$ and $b_1$ that is the lowest.\nGeometrically, we can imagine a shape in three dimensions where two\ninputs ($b_0$ and $b_1$) produce an output ($\\text{SSE}$). Roughly\nspeaking, where the three dimensional slope is equal to 0 is where the\ncombination of $b_0$ and $b_1$ will produce the lowest possible\n$\\text{SSE}$. Since the derivative of an equation can tell us the slope\nat any given point, we can set both equations' derivatives to zero to\nfind where slope is zero for both equations.\n\n**Step One: Take the partial derivatives with respect to** $b_0$ and\n$b_1$\n\nThe partial derivative with respect to $b_0$ is calculated as so:\n\n$$\\frac{\\partial}{\\partial b_0}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2$$\n$$\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_0}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-1) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n$$\n\nThe partial derivative with respect to $b_1$ is calculated as so:\n\n$$\\frac{\\partial}{\\partial b_1}\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)^2$$\n$$\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(Y_i - (b_0+b_1X_i)\\right)\\frac{\\partial}{\\partial b_1}\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(Y_i - (b_0+b_1X_i)\\right)(-X_i) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n$$\n\n**Step Two: Set the simplified partial derivatives to zero and solve\nfor** $b_0$ **and** $b_1$ **respectively**\n\nSolving for $b_0$: $$0 = -2\\sum_{i=1}^n\\left(Y_i - (b_0+b_1X_i)\\right)$$\n\n$$\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^n\\left(Y_i - b_0-b_1x_i\\right) &\\text{Simplify}\\\\\n&0 = \\sum_{i=1}^nY_i - \\sum_{i=1}^nb_0-\\sum_{i=1}^nb_1X_i &\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nY_i - nb_0-b_1\\sum_{i=1}^nX_i &\\text{Constant Multiple Rule for Sums}\\\\\n& nb_0 = \\sum_{i=1}^nY_i -b_1\\sum_{i=1}^nX_i &\\text{Simplify}\\\\\n& b_0 = \\frac{\\sum_{i=1}^nY_i}{n} -\\frac{b_1\\sum_{i=1}^nX_i}{n} &\\text{Simplify}\\\\\n& b_0 = \\bar{Y} - b_1\\bar{X} &\\text{Simplify to average}\\\\\n\\end{align*}\n$$\n\nSolving for $b_1$ while substituting $b_0$:\n\n$$\n0 = -2\\sum_{i=1}^nX_i\\left(Y_i - (b_0+b_1X_i)\\right)\n$$\n\n$$\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^nX_i\\left(Y_i - ((\\bar{Y} - b_1\\bar{X})+b_1X_i)\\right) &\\text{Substitute }b_0\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - \\sum_{i=1}^nb_1X_i(X_i-\\bar{X})&\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nX_i(Y_i-\\bar{Y}) - b_1\\sum_{i=1}^nX_i(X_i-\\bar{X})&\\text{Constant Multiple Rule for Sums}\\\\\n& b_1= \\frac{\\sum_{i=1}^nX_i(Y_i-\\bar{Y})}{\\sum_{i=1}^nX_i(X_i-\\bar{X})} &\\text{Simplify}\\\\\n& b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2} &\\text{Simplify (steps skipped)}\\\\\n\\end{align*}\n$$\n\n**Step Three: Construct the formula**\n\nOnce you have found $b_0$ and $b_1$ as so:\n\n-   $b_0 = \\bar{Y} - b_1\\bar{X}$\n\n-   $b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}$\n\n($\\bar{Y}$ represents the mean of the $Y$ values and $\\bar{X}$\nrepresents the mean of the $X$ values)\n\nWe can construct the formula for $\\hat{Y}_i$ as the following:\n\n$$\n\\hat{Y}_i = b_0 + b_1X_i\n$$\n\nWhich means $b_0$ is the change in the mean of $Y$ for a $1$ unit\nincrease in $X$.\n:::\n\nThe formula from calculus to find the coefficients for the estimated\nregression line is as so:\n\n-   $b_0 = \\bar{Y} - b_1\\bar{X}$\n\n-   $b_1= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(X_i-\\bar{X})^2}$\n\n($\\bar{Y}$ represents the mean of the $Y$ values and $\\bar{X}$\nrepresents the mean of the $X$ values)\n\n::: {.callout-note collapse=\"true\"}\n# R code for calculating the coefficients for the estimated regression line\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmylm <- lm(mpg ~ wt, data = mtcars)\npander::pander(summary(mylm))\n```\n\n::: {.cell-output-display}\n---------------------------------------------------------------\n     &nbsp;        Estimate   Std. Error   t value   Pr(>|t|)  \n----------------- ---------- ------------ --------- -----------\n **(Intercept)**    37.29       1.878       19.86    8.242e-19 \n\n     **wt**         -5.344      0.5591     -9.559    1.294e-10 \n---------------------------------------------------------------\n\n\n--------------------------------------------------------------\n Observations   Residual Std. Error   $R^2$    Adjusted $R^2$ \n-------------- --------------------- -------- ----------------\n      32               3.046          0.7528       0.7446     \n--------------------------------------------------------------\n\nTable: Fitting linear model: mpg ~ wt\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb_0 <- mylm$coefficients[1]\nb_1 <- mylm$coefficients[2]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb_0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   37.28513 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       wt \n-5.344472 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(x = wt, y = mpg)) +\ngeom_point(shape = 21, color = 'skyblue4', fill = 'skyblue', size = 3) +\nstat_function(fun = function(x) b_0 + b_1 * x) + \nlabs(title=\"Miles per Gallon vs. Weight of Car\", \n     x=\"Weight of Car\", \n     y=\"Miles per Gallon\") +\ntheme_classic() \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n:::\n\n## Assumption for Simple Linear Regression\n\nIf we are to make a model off the points displayed as such:\n\n$$\nY_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i\n$$\n\nWe need to confirm the following assumptions\n\n-   **1) Linear relation**: Linear relationship between $X$ and $Y$\n\n-   **2) Normal errors**: $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, the\n    error terms are normally distributed with a mean of 0\n\n-   **3) Constant variance**: The variance $σ^2$ of the error terms is\n    constant (the same) across all values of $X_i$\n\n-   **4) Fixed** $X$: the $X$ values can be considered fixed and\n    measured without error\n\n-   **5) Independent errors**: $\\epsilon$ is independent\n\n$Y_i = \\beta_0 + \\beta_1 X_i+ \\epsilon_i :\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$\n\nGiven $X$, $Y \\sim \\mathcal{N}(\\beta_0 +\\beta_1X,\\sigma^2)$\n\n::: {.callout-note collapse=\"true\"}\n# R code for testing the assumptions for simple linear regression\n\nThree plots in R can be used to test the assumptions for simple linear\nregression:\n\n**Residuals versus Fitted-values Plot** The linear relationship and\nconstant variance assumptions can be diagnosed using a residuals versus\nfitted-values plot. The fitted values are the $Y^i$\n\n-   The residuals are the $r_i$\n\n-   This plot compares the residual to the magnitude of the\n    fitted-value. No discernable pattern in this plot is desirable.\n\n**Q-Q Plot of the Residuals**\n\nThe normality of the error terms can be assessed by considering a normal\nprobability plot (Q-Q Plot) of the residuals. If the residuals appear to\nbe normal, then the error terms are also considered to be normal. If the\nresiduals do not appear to be normal, then the error terms are also\nassumed to violate the normality assumption.\n\n**Residuals versus Order Plot**\n\nWhen the data is collected in a specific order, or has some other\nimportant ordering to it, then the independence of the error terms can\nbe assessed. This is typically done by plotting the residuals against\ntheir order of occurrance. If any dramatic trends are visible in the\nplot, then the independence assumption is violated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,3)) #Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3\n\nplot(mylm,which=1:2) #Q-Q Plot of the Residuals: Checks Assumption #2\n\nplot(mylm$residuals) #Residuals versus Order Plot: Checks Assumption #5\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n:::\n\n## Assessing the Estimated Regression Line\n\n### Terms\n\n**Sum of Squared Errors**\n\nMeasures how much the residuals deviate from the line.\n\n$\\text{SSE} = \\sum_{i=1}^n \\left(r_i\\right)^2 = \\sum_{i=1}^n\\left(Y_i - \\hat{Y_i}\\right)^2$\n\n**Sum of Squares Regression**\n\nMeasures how much the regression line deviates from the average y-value.\n\n$\\text{SSR} = \\sum_{i=1}^n \\left(r_i\\right)^2 = \\sum_{i=1}^n\\left(\\hat{Y}_i - \\bar{Y}\\right)^2$\n\n**Total Sum of Squares**\n\nMeasures how much the y-values deviate from the average y-value.\n\n$\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2$\n\n![](images/paste-1.png){width=\"544\"}\n\n$\\text{SSTO}=\\text{SSE}+\\text{SSR}$\n\n::: {.callout-note collapse=\"true\"}\n# R code for calculating the above terms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals <- mylm$residuals\nfitted_values <- mylm$fitted.values\nmean_mpg <- mean(mtcars$mpg)\n\nSSE <- sum(residuals^2)\nSSR <- sum((fitted_values - mean_mpg)^2)\nTSS <- sum((mtcars$mpg - mean_mpg)^2)\n\npander::pander(cat(\"Sum of Squared Errors (SSE):\", SSE, \"\\n\"))\n```\n\n::: {.cell-output-display}\nSum of Squared Errors (SSE): 278.3219\n:::\n\n```{.r .cell-code}\npander::pander(cat(\"Sum of Squares Regression (SSR):\", SSR, \"\\n\"))\n```\n\n::: {.cell-output-display}\nSum of Squares Regression (SSR): 847.7252\n:::\n\n```{.r .cell-code}\npander::pander(cat(\"Total Sum of Squares (TSS):\", TSS, \"\\n\"))\n```\n\n::: {.cell-output-display}\nTotal Sum of Squares (TSS): 1126.047\n:::\n:::\n\n\n:::\n\n## Calculating the Coefficient of Determination\n\nUsing the terms above, we can calculate a ratio of the variance of $Y$\nexplained by the estimated regression line ($\\text{SSE}$) and the total\nvariance of $Y$ from the average $Y$ value ($\\text{SSTO}$.\n\n$$\nR^2 = 1-\\frac{\\text{SSE}}{\\text{SSTO}}\n$$\n\n::: {.callout-note collapse=\"true\"}\n# R code for calculating the coefficient of determination\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_summary <- summary(mylm)\n\npander::pander(lm_summary$r.squared)\n```\n\n::: {.cell-output-display}\n_0.7528_\n:::\n:::\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}