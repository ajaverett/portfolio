{
  "hash": "a3c9533431847d8e2e6766427d1ebb62",
  "result": {
    "markdown": "---\ntitle: \"Regression (by hand)\"\nauthor: \"Aj Averett\"\ndate: \"2023-1-15\"\ncategories: [statistics]\neditor: \n  markdown: \n    wrap: 72\n---\n\n# Defining terms\n\n## The Data\n\nYou have a dataset where each row represents an individual sample and\neach column represents a variable about each sample.\n\n-   $n$ refers to the total number of samples you have\n\n-   $\\vec{x}$ refers to the independent variable column in your data\n\n-   $\\vec{y}$ refers to the dependent variable column in your data\n\n-   We will use the notation $x_i$ and $y_i$ to denote the$i$th\n    observations of $x$ and $y$, respectively.This would mean, for\n    example: $x_1$ would refer to the first x value in your data while\n    $y_n$ would refer to the last y value in your data\n\n## The Line\n\nThe goal of simple linear regression is to find the line of best fit,\nwhich is represented by the equation $f(x)=\\beta_0+\\beta_1x$ , where\n$\\beta_1$ is the slope of the line and $\\beta_0$ is the y-intercept. The\nfunction $f(x)$ will find any $\\hat{y_i}$ which is the predicted y value\nof $x_i$ with a best fit line.\n\n## Minimizing Residuals\n\nGiven any line, we can calculate how different our predicted value,\n$\\hat{y_i}$ is from our actual data point, $y_i$. This vertical distance\nfrom the real y output and the predicted y output is called the\nresidual. The residual for a given point is calculated as\n$e_i = y_i - \\hat{y_i}$.\n\nThe goal of simple linear regression is to minimize the sum of the\nsquared residuals, also known as the residual sum of squares ($RSS$).\nThis is represented by the equations:\n\n$$\\sum_{i=1}^n \\left(e_i\\right)^2 \\text{ or } \\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2 \\text{ or } \\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2$$.\n\nThe closer any given line is to the best fit line, the lower the $RSS$\nwill be. In fact, the best fit line represents the line with the lowest\npossible $RSS$. This line is found by manipulating the values for\n$\\beta_0$ and $\\beta_1$ since the x values must stay the same.\n\n## The Calculus Behind Best Fit\n\nThere exists a combination of $\\beta_0$ and $\\beta_1$ that is the\nlowest. Geometrically, we can imagine a shape in three dimensions where\ntwo inputs ($\\beta_0$ and $\\beta_1$) produce an output ($RSS$). Roughly\nspeaking, where the three dimensional slope is equal to 0 is where the\ncombination of $\\beta_0$ and $\\beta_1$ will produce the lowest possible\n$RSS$. Since the derivative of an equation can tell us the slope at any\ngiven point, we can set both equations' derivatives to zero to find\nwhere slope is zero for both equations.\n\n**Step One: Take the partial derivatives with respect to** $\\beta_0$ and\n$\\beta_1$\n\nThe partial derivative with respect to $\\beta_0$ is calculated as so:\n\n$$\\frac{\\partial}{\\partial\\beta_0}\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2$$\n$$\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial\\beta_0}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\frac{\\partial}{\\partial\\beta_0}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)(-1) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n$$\n\nThe partial derivative with respect to $\\beta_1$ is calculated as so:\n\n$$\\frac{\\partial}{\\partial\\beta_1}\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2$$\n$$\n\\begin{align*}\n\\mathscr{}\n&= \\sum_{i=1}^n\\frac{\\partial}{\\partial\\beta_1}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)^2  &\\text{Sum Rule}\\\\\n&= \\sum_{i=1}^n2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)\\frac{\\partial}{\\partial\\beta_1}\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Power Rule}\\\\\n&= \\sum_{i=1}^n 2\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)(-x_i) &\\text{Simplify}\\\\\n&= -2\\sum_{i=1}^nx_i\\left(y_i - (\\beta_0+\\beta_1x_i)\\right) &\\text{Constant Multiple Rule for Sums}\n\\end{align*}\n$$\n\n**Step Two: Set the simplified partial derivatives to zero and solve\nfor** $\\beta_0$ **and** $\\beta_1$ **respectively**\n\nSolving for $\\beta_0$:\n$$0 = -2\\sum_{i=1}^n\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)$$\n\n$$\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^n\\left(y_i - \\beta_0-\\beta_1x_i\\right) &\\text{Simplify}\\\\\n&0 = \\sum_{i=1}^ny_i - \\sum_{i=1}^n\\beta_0-\\sum_{i=1}^n\\beta_1x_i &\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^ny_i - n\\beta_0-\\beta_1\\sum_{i=1}^nx_i &\\text{Constant Multiple Rule for Sums}\\\\\n& n\\beta_0 = \\sum_{i=1}^ny_i -\\beta_1\\sum_{i=1}^nx_i &\\text{Simplify}\\\\\n& \\beta_0 = \\frac{\\sum_{i=1}^ny_i}{n} -\\frac{\\beta_1\\sum_{i=1}^nx_i}{n} &\\text{Simplify}\\\\\n& \\beta_0 = \\bar{y} - \\beta_1\\bar{x} &\\text{Simplify to 'Average'}\\\\\n\\end{align*}\n$$\n\nSolving for $\\beta_1$ while substituting $\\beta_0$:\n\n$$0 = -2\\sum_{i=1}^nx_i\\left(y_i - (\\beta_0+\\beta_1x_i)\\right)$$\n$$\n\\begin{align*}\n\\mathscr{}\n&0 = \\sum_{i=1}^nx_i\\left(y_i - ((\\bar{y} - \\beta_1\\bar{x})+\\beta_1x_i)\\right) &\\text{Substitute }\\beta_0\\\\\n&0 = \\sum_{i=1}^nx_i(y_i-\\bar{y}) - \\sum_{i=1}^n\\beta_1x_i(x_i-\\bar{x})&\\text{Sum Rule}\\\\\n&0 = \\sum_{i=1}^nx_i(y_i-\\bar{y}) - \\beta_1\\sum_{i=1}^nx_i(x_i-\\bar{x})&\\text{Constant Multiple Rule for Sums}\\\\\n& \\beta_1= \\frac{\\sum_{i=1}^nx_i(y_i-\\bar{y})}{\\sum_{i=1}^nx_i(x_i-\\bar{x})} &\\text{Simplify}\\\\\n& \\beta_1= \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} &\\text{Simplify (steps skipped)}\\\\\n\\end{align*}\n$$\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# import pandas as pd\n# import numpy as np\n\n# df = pd.read_csv('https://gist.githubusercontent.com/ericbusboom/b2ac1d366c005cd2ed8c/raw/c92c66e43d144fa9c29dbd602d5af6988e8db533/anscombes.csv').query('dataset == \"I\"')\n\n\n# # Assume we have a dataset with x and y values\n# x = df['x']\n# y = df['y']\n\n# # Step 1: initialize beta_0 and beta_1\n# beta_0 = 0\n# beta_1 = 0\n\n# # Step 2: calculate the residual sum of squares\n# rss = sum((y[i] - (beta_0 + beta_1 * x[i])) ** 2 for i in range(len(x)))\n\n# # Step 3: take the partial derivative with respect to beta_0\n# partial_beta_0 = -2 * sum(y[i] - (beta_0 + beta_1 * x[i]) for i in range(len(x)))\n\n# # Step 4: take the partial derivative with respect to beta_1\n# partial_beta_1 = -2 * sum(x[i] * (y[i] - (beta_0 + beta_1 * x[i])) for i in range(len(x)))\n\n# # Step 5: set the partial derivatives to zero\n# beta_0 = sum(y) / len(x) - beta_1 * sum(x) / len(x)\n# beta_1 = (np.dot(x, y) - (sum(x) * sum(y) / len(x))) / (np.dot(x, x) - (sum(x) ** 2 / len(x)))\n\n# ###\n\n# # Step 1: calculate the mean of x and y\n# x_mean = sum(x) / len(x)\n# y_mean = sum(y) / len(y)\n\n# # Step 2: calculate the standard deviation of x and y\n# x_std = np.std(x)\n# y_std = np.std(y)\n\n# # Step 3: calculate the covariance\n# cov = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(len(x)))\n\n# # Step 4: calculate the correlation coefficient\n# correlation = cov / (x_std * y_std)\n\n# print(correlation)\n\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# import sympy as sym\n\n# # Define the variables\n# x, y, beta_0, beta_1, n = sym.symbols('x y beta_0 beta_1 n')\n\n# # Define the equation for the residual sum of squares\n# rss = sym.Sum((y - (beta_0 + beta_1*x))**2, (x, 1, n))\n\n# # Take the partial derivative with respect to beta_0\n# partial_beta_0 = sym.diff(rss, beta_0)\n\n# # Take the partial derivative with respect to beta_1\n# partial_beta_1 = sym.diff(rss, beta_1)\n\n# # Set the partial derivatives to zero\n# eq1 = sym.Eq(partial_beta_0, 0)\n# eq2 = sym.Eq(partial_beta_1, 0)\n\n# # Solve for beta_0 and beta_1\n# beta_0_sol, beta_1_sol = sym.solve((eq1, eq2), (beta_0, beta_1))\n\n# print(\"beta_0: \", beta_0_sol)\n# print(\"beta_1: \", beta_1_sol)\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}